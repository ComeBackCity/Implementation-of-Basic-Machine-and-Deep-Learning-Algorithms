{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0) * 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * np.sqrt(2/input_shape[1] * self.filter_dimension ** 2)\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.output_channel_count)\n",
    "\n",
    "        # print('Convolution layer')\n",
    "        # print(self.filters[0, 0, :, :])\n",
    "        # print(self.bias)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 10e-4) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dz_prime_dim = (dz.shape[2] - 1) * self.stride + 1\n",
    "        dz_prime = np.zeros((dz.shape[0], dz.shape[1], dz_prime_dim, dz_prime_dim))\n",
    "        dz_prime[:, :, ::self.stride, ::self.stride] = dz\n",
    "\n",
    "        for i in range(padded_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + dz_prime_dim <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + dz_prime_dim <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+dz_prime_dim, image_y:image_y+dz_prime_dim]\n",
    "                    dz_slice = dz_prime[i, :, :, :]\n",
    "                    dz_slice_shape = dz_slice.shape\n",
    "                    dz_slice = np.broadcast_to(dz_slice, (image_slice.shape[0], dz_slice_shape[0], dz_slice_shape[1], dz_slice_shape[2]))\n",
    "                    dz_slice = np.transpose(dz_slice, (1, 0, 2, 3))\n",
    "                    dw[:, :, out_x, out_y] += np.sum(image_slice * dz_slice, axis=(2, 3))\n",
    "                    # for f in range(self.output_channel_count):\n",
    "                    #     dw[f, :, out_x, out_y] += np.sum(image_slice * dz_prime[i, f, :, :], axis=(1, 2))\n",
    "                    image_x += 1\n",
    "                    out_x += 1\n",
    "                image_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        # for i in range(batch_size):\n",
    "        #     tmp_y = out_y = 0\n",
    "        #     while tmp_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "        #         tmp_x = out_x = 0\n",
    "        #         while tmp_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "        #             image_slice = padded_image[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension]\n",
    "        #             for f in range(self.output_channel_count):\n",
    "        #                 dw[f, :, :, :] += np.sum(dz[i, f, out_x, out_y] * image_slice, axis=0)\n",
    "        #                 dout_padded[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension] += dz[i, f, out_x, out_y] * self.filters[f, :, :, :]\n",
    "        #             tmp_x += self.stride\n",
    "        #             out_x += 1\n",
    "        #         tmp_y += self.stride\n",
    "        #         out_y += 1\n",
    "        #\n",
    "        self.filters -= learning_rate * dw / batch_size\n",
    "        # return dout_padded[:, :, self.padding:padded_dimensions[2]-self.padding, self.padding:padded_dimensions[3]-self.padding]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.       , 0.       , 0.       ],\n       [0.       , 0.4777692, 0.       ],\n       [0.       , 0.       , 0.       ]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(input_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.       , 0.       , 0.       ],\n       [0.       , 0.4777692, 0.       ],\n       [0.       , 0.       , 0.       ]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    max_val = np.max(image_slice, axis=(1, 2))\n",
    "                    output[i, :, out_x, out_y] = max_val\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == max_val.reshape(input_dimensions[1], 1, 1)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.4777692, 0.4777692],\n       [0.4777692, 0.4777692]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 ,\n        0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 ,\n        0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 ,\n        0.4777692 ],\n       [0.95247325, 0.95247325, 0.95247325, 0.95247325, 0.95247325,\n        0.95247325, 0.95247325, 0.95247325, 0.95247325, 0.95247325,\n        0.95247325, 0.95247325, 0.95247325, 0.95247325, 0.95247325,\n        0.95247325],\n       [1.90110531, 1.90110531, 1.90110531, 1.90110531, 1.90110531,\n        1.90110531, 1.90110531, 1.90110531, 1.90110531, 1.90110531,\n        1.90110531, 1.90110531, 1.90110531, 1.90110531, 1.90110531,\n        1.90110531],\n       [1.42937845, 1.42937845, 1.42937845, 1.42937845, 1.42937845,\n        1.42937845, 1.42937845, 1.42937845, 1.42937845, 1.42937845,\n        1.42937845, 1.42937845, 1.42937845, 1.42937845, 1.42937845,\n        1.42937845],\n       [1.90503461, 1.90503461, 1.90503461, 1.90503461, 1.90503461,\n        1.90503461, 1.90503461, 1.90503461, 1.90503461, 1.90503461,\n        1.90503461, 1.90503461, 1.90503461, 1.90503461, 1.90503461,\n        1.90503461],\n       [0.95251388, 0.95251388, 0.95251388, 0.95251388, 0.95251388,\n        0.95251388, 0.95251388, 0.95251388, 0.95251388, 0.95251388,\n        0.95251388, 0.95251388, 0.95251388, 0.95251388, 0.95251388,\n        0.95251388],\n       [0.47103298, 0.47103298, 0.47103298, 0.47103298, 0.47103298,\n        0.47103298, 0.47103298, 0.47103298, 0.47103298, 0.47103298,\n        0.47103298, 0.47103298, 0.47103298, 0.47103298, 0.47103298,\n        0.47103298],\n       [1.43388051, 1.43388051, 1.43388051, 1.43388051, 1.43388051,\n        1.43388051, 1.43388051, 1.43388051, 1.43388051, 1.43388051,\n        1.43388051, 1.43388051, 1.43388051, 1.43388051, 1.43388051,\n        1.43388051],\n       [0.47472288, 0.47472288, 0.47472288, 0.47472288, 0.47472288,\n        0.47472288, 0.47472288, 0.47472288, 0.47472288, 0.47472288,\n        0.47472288, 0.47472288, 0.47472288, 0.47472288, 0.47472288,\n        0.47472288],\n       [1.41343535, 1.41343535, 1.41343535, 1.41343535, 1.41343535,\n        1.41343535, 1.41343535, 1.41343535, 1.41343535, 1.41343535,\n        1.41343535, 1.41343535, 1.41343535, 1.41343535, 1.41343535,\n        1.41343535],\n       [1.42481284, 1.42481284, 1.42481284, 1.42481284, 1.42481284,\n        1.42481284, 1.42481284, 1.42481284, 1.42481284, 1.42481284,\n        1.42481284, 1.42481284, 1.42481284, 1.42481284, 1.42481284,\n        1.42481284],\n       [1.45001068, 1.45001068, 1.45001068, 1.45001068, 1.45001068,\n        1.45001068, 1.45001068, 1.45001068, 1.45001068, 1.45001068,\n        1.45001068, 1.45001068, 1.45001068, 1.45001068, 1.45001068,\n        1.45001068],\n       [1.44937555, 1.44937555, 1.44937555, 1.44937555, 1.44937555,\n        1.44937555, 1.44937555, 1.44937555, 1.44937555, 1.44937555,\n        1.44937555, 1.44937555, 1.44937555, 1.44937555, 1.44937555,\n        1.44937555],\n       [0.92475104, 0.92475104, 0.92475104, 0.92475104, 0.92475104,\n        0.92475104, 0.92475104, 0.92475104, 0.92475104, 0.92475104,\n        0.92475104, 0.92475104, 0.92475104, 0.92475104, 0.92475104,\n        0.92475104],\n       [0.44660692, 0.44660692, 0.44660692, 0.44660692, 0.44660692,\n        0.44660692, 0.44660692, 0.44660692, 0.44660692, 0.44660692,\n        0.44660692, 0.44660692, 0.44660692, 0.44660692, 0.44660692,\n        0.44660692],\n       [0.96526399, 0.96526399, 0.96526399, 0.96526399, 0.96526399,\n        0.96526399, 0.96526399, 0.96526399, 0.96526399, 0.96526399,\n        0.96526399, 0.96526399, 0.96526399, 0.96526399, 0.96526399,\n        0.96526399],\n       [0.49517698, 0.49517698, 0.49517698, 0.49517698, 0.49517698,\n        0.49517698, 0.49517698, 0.49517698, 0.49517698, 0.49517698,\n        0.49517698, 0.49517698, 0.49517698, 0.49517698, 0.49517698,\n        0.49517698],\n       [1.43460985, 1.43460985, 1.43460985, 1.43460985, 1.43460985,\n        1.43460985, 1.43460985, 1.43460985, 1.43460985, 1.43460985,\n        1.43460985, 1.43460985, 1.43460985, 1.43460985, 1.43460985,\n        1.43460985],\n       [1.90846466, 1.90846466, 1.90846466, 1.90846466, 1.90846466,\n        1.90846466, 1.90846466, 1.90846466, 1.90846466, 1.90846466,\n        1.90846466, 1.90846466, 1.90846466, 1.90846466, 1.90846466,\n        1.90846466],\n       [0.46293936, 0.46293936, 0.46293936, 0.46293936, 0.46293936,\n        0.46293936, 0.46293936, 0.46293936, 0.46293936, 0.46293936,\n        0.46293936, 0.46293936, 0.46293936, 0.46293936, 0.46293936,\n        0.46293936],\n       [0.48006658, 0.48006658, 0.48006658, 0.48006658, 0.48006658,\n        0.48006658, 0.48006658, 0.48006658, 0.48006658, 0.48006658,\n        0.48006658, 0.48006658, 0.48006658, 0.48006658, 0.48006658,\n        0.48006658],\n       [1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 ,\n        1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 ,\n        1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 ,\n        1.9112458 ],\n       [0.48268792, 0.48268792, 0.48268792, 0.48268792, 0.48268792,\n        0.48268792, 0.48268792, 0.48268792, 0.48268792, 0.48268792,\n        0.48268792, 0.48268792, 0.48268792, 0.48268792, 0.48268792,\n        0.48268792],\n       [0.48050306, 0.48050306, 0.48050306, 0.48050306, 0.48050306,\n        0.48050306, 0.48050306, 0.48050306, 0.48050306, 0.48050306,\n        0.48050306, 0.48050306, 0.48050306, 0.48050306, 0.48050306,\n        0.48050306],\n       [1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 ,\n        1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 ,\n        1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 ,\n        1.8935999 ],\n       [0.49026492, 0.49026492, 0.49026492, 0.49026492, 0.49026492,\n        0.49026492, 0.49026492, 0.49026492, 0.49026492, 0.49026492,\n        0.49026492, 0.49026492, 0.49026492, 0.49026492, 0.49026492,\n        0.49026492],\n       [1.90941193, 1.90941193, 1.90941193, 1.90941193, 1.90941193,\n        1.90941193, 1.90941193, 1.90941193, 1.90941193, 1.90941193,\n        1.90941193, 1.90941193, 1.90941193, 1.90941193, 1.90941193,\n        1.90941193],\n       [0.46894939, 0.46894939, 0.46894939, 0.46894939, 0.46894939,\n        0.46894939, 0.46894939, 0.46894939, 0.46894939, 0.46894939,\n        0.46894939, 0.46894939, 0.46894939, 0.46894939, 0.46894939,\n        0.46894939],\n       [0.95093824, 0.95093824, 0.95093824, 0.95093824, 0.95093824,\n        0.95093824, 0.95093824, 0.95093824, 0.95093824, 0.95093824,\n        0.95093824, 0.95093824, 0.95093824, 0.95093824, 0.95093824,\n        0.95093824],\n       [1.44856704, 1.44856704, 1.44856704, 1.44856704, 1.44856704,\n        1.44856704, 1.44856704, 1.44856704, 1.44856704, 1.44856704,\n        1.44856704, 1.44856704, 1.44856704, 1.44856704, 1.44856704,\n        1.44856704],\n       [1.44390473, 1.44390473, 1.44390473, 1.44390473, 1.44390473,\n        1.44390473, 1.44390473, 1.44390473, 1.44390473, 1.44390473,\n        1.44390473, 1.44390473, 1.44390473, 1.44390473, 1.44390473,\n        1.44390473],\n       [1.44106375, 1.44106375, 1.44106375, 1.44106375, 1.44106375,\n        1.44106375, 1.44106375, 1.44106375, 1.44106375, 1.44106375,\n        1.44106375, 1.44106375, 1.44106375, 1.44106375, 1.44106375,\n        1.44106375],\n       [1.44036166, 1.44036166, 1.44036166, 1.44036166, 1.44036166,\n        1.44036166, 1.44036166, 1.44036166, 1.44036166, 1.44036166,\n        1.44036166, 1.44036166, 1.44036166, 1.44036166, 1.44036166,\n        1.44036166],\n       [0.96931311, 0.96931311, 0.96931311, 0.96931311, 0.96931311,\n        0.96931311, 0.96931311, 0.96931311, 0.96931311, 0.96931311,\n        0.96931311, 0.96931311, 0.96931311, 0.96931311, 0.96931311,\n        0.96931311],\n       [0.46325287, 0.46325287, 0.46325287, 0.46325287, 0.46325287,\n        0.46325287, 0.46325287, 0.46325287, 0.46325287, 0.46325287,\n        0.46325287, 0.46325287, 0.46325287, 0.46325287, 0.46325287,\n        0.46325287],\n       [1.92101125, 1.92101125, 1.92101125, 1.92101125, 1.92101125,\n        1.92101125, 1.92101125, 1.92101125, 1.92101125, 1.92101125,\n        1.92101125, 1.92101125, 1.92101125, 1.92101125, 1.92101125,\n        1.92101125],\n       [0.47607298, 0.47607298, 0.47607298, 0.47607298, 0.47607298,\n        0.47607298, 0.47607298, 0.47607298, 0.47607298, 0.47607298,\n        0.47607298, 0.47607298, 0.47607298, 0.47607298, 0.47607298,\n        0.47607298],\n       [1.44236624, 1.44236624, 1.44236624, 1.44236624, 1.44236624,\n        1.44236624, 1.44236624, 1.44236624, 1.44236624, 1.44236624,\n        1.44236624, 1.44236624, 1.44236624, 1.44236624, 1.44236624,\n        1.44236624],\n       [1.93275733, 1.93275733, 1.93275733, 1.93275733, 1.93275733,\n        1.93275733, 1.93275733, 1.93275733, 1.93275733, 1.93275733,\n        1.93275733, 1.93275733, 1.93275733, 1.93275733, 1.93275733,\n        1.93275733],\n       [0.49435448, 0.49435448, 0.49435448, 0.49435448, 0.49435448,\n        0.49435448, 0.49435448, 0.49435448, 0.49435448, 0.49435448,\n        0.49435448, 0.49435448, 0.49435448, 0.49435448, 0.49435448,\n        0.49435448],\n       [1.91410395, 1.91410395, 1.91410395, 1.91410395, 1.91410395,\n        1.91410395, 1.91410395, 1.91410395, 1.91410395, 1.91410395,\n        1.91410395, 1.91410395, 1.91410395, 1.91410395, 1.91410395,\n        1.91410395],\n       [0.48472684, 0.48472684, 0.48472684, 0.48472684, 0.48472684,\n        0.48472684, 0.48472684, 0.48472684, 0.48472684, 0.48472684,\n        0.48472684, 0.48472684, 0.48472684, 0.48472684, 0.48472684,\n        0.48472684],\n       [1.89572016, 1.89572016, 1.89572016, 1.89572016, 1.89572016,\n        1.89572016, 1.89572016, 1.89572016, 1.89572016, 1.89572016,\n        1.89572016, 1.89572016, 1.89572016, 1.89572016, 1.89572016,\n        1.89572016],\n       [1.91816156, 1.91816156, 1.91816156, 1.91816156, 1.91816156,\n        1.91816156, 1.91816156, 1.91816156, 1.91816156, 1.91816156,\n        1.91816156, 1.91816156, 1.91816156, 1.91816156, 1.91816156,\n        1.91816156],\n       [0.49236632, 0.49236632, 0.49236632, 0.49236632, 0.49236632,\n        0.49236632, 0.49236632, 0.49236632, 0.49236632, 0.49236632,\n        0.49236632, 0.49236632, 0.49236632, 0.49236632, 0.49236632,\n        0.49236632],\n       [1.43420526, 1.43420526, 1.43420526, 1.43420526, 1.43420526,\n        1.43420526, 1.43420526, 1.43420526, 1.43420526, 1.43420526,\n        1.43420526, 1.43420526, 1.43420526, 1.43420526, 1.43420526,\n        1.43420526],\n       [1.91439439, 1.91439439, 1.91439439, 1.91439439, 1.91439439,\n        1.91439439, 1.91439439, 1.91439439, 1.91439439, 1.91439439,\n        1.91439439, 1.91439439, 1.91439439, 1.91439439, 1.91439439,\n        1.91439439],\n       [0.94925996, 0.94925996, 0.94925996, 0.94925996, 0.94925996,\n        0.94925996, 0.94925996, 0.94925996, 0.94925996, 0.94925996,\n        0.94925996, 0.94925996, 0.94925996, 0.94925996, 0.94925996,\n        0.94925996],\n       [1.90495423, 1.90495423, 1.90495423, 1.90495423, 1.90495423,\n        1.90495423, 1.90495423, 1.90495423, 1.90495423, 1.90495423,\n        1.90495423, 1.90495423, 1.90495423, 1.90495423, 1.90495423,\n        1.90495423],\n       [1.42175942, 1.42175942, 1.42175942, 1.42175942, 1.42175942,\n        1.42175942, 1.42175942, 1.42175942, 1.42175942, 1.42175942,\n        1.42175942, 1.42175942, 1.42175942, 1.42175942, 1.42175942,\n        1.42175942]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * np.sqrt(2/flattened_input.shape[1])\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros((1, self.output_dimension))\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        # print('Fully connected layer')\n",
    "        # print(self.weights)\n",
    "        # print(self.bias)\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 10e-4) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.31304803,  0.2086442 ,  0.90147642,  0.26763196],\n       [-0.62408769,  0.41594982,  1.79716937,  0.53354691],\n       [-1.24565852,  0.83022218,  3.58709102,  1.0649421 ],\n       [-0.9365696 ,  0.62421671,  2.69701556,  0.80069488],\n       [-1.24823311,  0.83193812,  3.59450499,  1.06714318],\n       [-0.62411431,  0.41596757,  1.79724603,  0.53356967],\n       [-0.30863427,  0.20570245,  0.88876621,  0.26385853],\n       [-0.93951948,  0.62618278,  2.70551025,  0.8032168 ],\n       [-0.31105199,  0.20731385,  0.89572848,  0.2659255 ],\n       [-0.92612323,  0.61725427,  2.6669334 ,  0.79176404],\n       [-0.93357809,  0.62222288,  2.68840097,  0.79813737],\n       [-0.95008843,  0.63322691,  2.73594539,  0.81225244],\n       [-0.94967228,  0.63294954,  2.73474699,  0.81189666],\n       [-0.60592331,  0.4038434 ,  1.74486187,  0.51801776],\n       [-0.29262962,  0.19503547,  0.84267803,  0.25017579],\n       [-0.63246855,  0.4215356 ,  1.82130352,  0.5407119 ],\n       [-0.3244541 ,  0.21624626,  0.9343222 ,  0.27738327],\n       [-0.93999737,  0.62650128,  2.7068864 ,  0.80362536],\n       [-1.25048058,  0.83343605,  3.60097698,  1.06906459],\n       [-0.3033311 ,  0.20216793,  0.87349482,  0.25932473],\n       [-0.31455334,  0.20964747,  0.90581122,  0.26891888],\n       [-1.25230286,  0.83465058,  3.60622454,  1.0706225 ],\n       [-0.31627092,  0.21079223,  0.91075729,  0.27038728],\n       [-0.31483933,  0.20983808,  0.90663479,  0.26916338],\n       [-1.24074076,  0.82694453,  3.57292947,  1.0607378 ],\n       [-0.32123558,  0.21410114,  0.9250539 ,  0.27463168],\n       [-1.25110126,  0.83384972,  3.60276433,  1.06959523],\n       [-0.30726904,  0.20479254,  0.8848348 ,  0.26269136],\n       [-0.62308191,  0.41527948,  1.79427304,  0.53268704],\n       [-0.94914252,  0.63259646,  2.73322145,  0.81144376],\n       [-0.94608764,  0.6305604 ,  2.72442439,  0.80883207],\n       [-0.94422615,  0.62931974,  2.71906391,  0.80724064],\n       [-0.94376612,  0.62901313,  2.71773917,  0.80684735],\n       [-0.63512165,  0.42330387,  1.82894357,  0.54298009],\n       [-0.30353652,  0.20230484,  0.87408636,  0.25950035],\n       [-1.25870146,  0.8389152 ,  3.62465044,  1.07609281],\n       [-0.31193662,  0.20790345,  0.89827591,  0.26668178],\n       [-0.94507957,  0.62988854,  2.7215215 ,  0.80797025],\n       [-1.26639783,  0.84404477,  3.64681347,  1.08267261],\n       [-0.32391518,  0.21588707,  0.93277027,  0.27692253],\n       [-1.25417561,  0.83589875,  3.61161744,  1.07222356],\n       [-0.31760688,  0.21168263,  0.91460441,  0.27152942],\n       [-1.24213002,  0.82787046,  3.57693008,  1.06192551],\n       [-1.25683427,  0.83767073,  3.61927352,  1.07449651],\n       [-0.32261248,  0.21501883,  0.92901892,  0.27580883],\n       [-0.93973227,  0.6263246 ,  2.706123  ,  0.80339872],\n       [-1.25436591,  0.83602559,  3.61216545,  1.07238625],\n       [-0.62198225,  0.41454656,  1.79110639,  0.53174692],\n       [-1.24818044,  0.83190302,  3.59435333,  1.06709815],\n       [-0.9315774 ,  0.62088944,  2.68263962,  0.79642693]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1).reshape(-1, 1)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.12753766, 0.21488519, 0.42963505, 0.2279421 ],\n       [0.05472937, 0.15484708, 0.61625298, 0.17417057],\n       [0.00691526, 0.05512543, 0.8682502 , 0.06970911],\n       [0.02028702, 0.09661823, 0.7678284 , 0.11526636],\n       [0.00685163, 0.05485305, 0.868897  , 0.06939833],\n       [0.05472504, 0.15484169, 0.61626787, 0.17416539],\n       [0.12892069, 0.21562357, 0.42692054, 0.2285352 ],\n       [0.02008722, 0.09613813, 0.76901729, 0.11475736],\n       [0.12816184, 0.21521976, 0.42840725, 0.22821115],\n       [0.02100924, 0.09833103, 0.76358067, 0.11707906],\n       [0.02049149, 0.09710671, 0.76661796, 0.11578384],\n       [0.01938605, 0.09443099, 0.77323866, 0.1129443 ],\n       [0.01941323, 0.09449782, 0.77307357, 0.11301538],\n       [0.05775098, 0.15852416, 0.60602758, 0.17769727],\n       [0.13402189, 0.21825589, 0.41709364, 0.23062858],\n       [0.05338027, 0.15315421, 0.62092682, 0.17253871],\n       [0.12401118, 0.21295313, 0.43665702, 0.22637867],\n       [0.02005502, 0.0960605 , 0.76920946, 0.11467503],\n       [0.00679654, 0.0546162 , 0.8694593 , 0.06912795],\n       [0.13059601, 0.21650371, 0.42366142, 0.22923886],\n       [0.12706832, 0.21463217, 0.4305612 , 0.2277383 ],\n       [0.00675219, 0.0544248 , 0.86991366, 0.06890935],\n       [0.12653427, 0.21434274, 0.43161819, 0.22750481],\n       [0.12697929, 0.21458403, 0.43073719, 0.22769949],\n       [0.00703837, 0.05564888, 0.86700688, 0.07030587],\n       [0.12499932, 0.21350175, 0.43467465, 0.22682427],\n       [0.0067814 , 0.05455095, 0.86961422, 0.06905343],\n       [0.12935056, 0.21585089, 0.42608127, 0.22871728],\n       [0.05489317, 0.15505041, 0.61569018, 0.17436623],\n       [0.01944788, 0.09458294, 0.77286328, 0.11310589],\n       [0.0196488 , 0.09507482, 0.77164772, 0.11362866],\n       [0.01977215, 0.09537537, 0.77090457, 0.1139479 ],\n       [0.01980275, 0.09544975, 0.77072063, 0.11402687],\n       [0.05295902, 0.1526189 , 0.62240045, 0.17202163],\n       [0.13053084, 0.21646977, 0.42378761, 0.22921178],\n       [0.00659863, 0.05375723, 0.87149791, 0.06814623],\n       [0.12788495, 0.21507162, 0.42895136, 0.22809208],\n       [0.01971551, 0.0952375 , 0.77124551, 0.11380148],\n       [0.00641834, 0.05296349, 0.87338069, 0.06723747],\n       [0.12417626, 0.21304518, 0.43632504, 0.22645352],\n       [0.00670689, 0.0542287 , 0.87037913, 0.06868529],\n       [0.12611995, 0.21411707, 0.43244048, 0.2273225 ],\n       [0.00700338, 0.05550058, 0.86735917, 0.07013687],\n       [0.00664309, 0.05395132, 0.87103739, 0.06836821],\n       [0.12457592, 0.21326738, 0.43552263, 0.22663407],\n       [0.02007288, 0.09610356, 0.76910287, 0.1147207 ],\n       [0.00670231, 0.0542088 , 0.87042634, 0.06866255],\n       [0.05507272, 0.15527277, 0.61507439, 0.17458011],\n       [0.00685293, 0.05485861, 0.86888379, 0.06940467],\n       [0.02062928, 0.09743431, 0.76580576, 0.11613065]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "1.688466162077336"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.87246234,  0.21488519,  0.42963505,  0.2279421 ],\n       [ 0.05472937, -0.84515292,  0.61625298,  0.17417057],\n       [ 0.00691526,  0.05512543,  0.8682502 , -0.93029089],\n       [ 0.02028702,  0.09661823, -0.2321716 ,  0.11526636],\n       [ 0.00685163,  0.05485305,  0.868897  , -0.93060167],\n       [ 0.05472504, -0.84515831,  0.61626787,  0.17416539],\n       [-0.87107931,  0.21562357,  0.42692054,  0.2285352 ],\n       [ 0.02008722,  0.09613813, -0.23098271,  0.11475736],\n       [-0.87183816,  0.21521976,  0.42840725,  0.22821115],\n       [ 0.02100924,  0.09833103, -0.23641933,  0.11707906],\n       [ 0.02049149,  0.09710671, -0.23338204,  0.11578384],\n       [ 0.01938605,  0.09443099, -0.22676134,  0.1129443 ],\n       [ 0.01941323,  0.09449782, -0.22692643,  0.11301538],\n       [ 0.05775098, -0.84147584,  0.60602758,  0.17769727],\n       [-0.86597811,  0.21825589,  0.41709364,  0.23062858],\n       [ 0.05338027, -0.84684579,  0.62092682,  0.17253871],\n       [-0.87598882,  0.21295313,  0.43665702,  0.22637867],\n       [ 0.02005502,  0.0960605 , -0.23079054,  0.11467503],\n       [ 0.00679654,  0.0546162 ,  0.8694593 , -0.93087205],\n       [-0.86940399,  0.21650371,  0.42366142,  0.22923886],\n       [-0.87293168,  0.21463217,  0.4305612 ,  0.2277383 ],\n       [ 0.00675219,  0.0544248 ,  0.86991366, -0.93109065],\n       [-0.87346573,  0.21434274,  0.43161819,  0.22750481],\n       [-0.87302071,  0.21458403,  0.43073719,  0.22769949],\n       [ 0.00703837,  0.05564888,  0.86700688, -0.92969413],\n       [-0.87500068,  0.21350175,  0.43467465,  0.22682427],\n       [ 0.0067814 ,  0.05455095,  0.86961422, -0.93094657],\n       [-0.87064944,  0.21585089,  0.42608127,  0.22871728],\n       [ 0.05489317, -0.84494959,  0.61569018,  0.17436623],\n       [ 0.01944788,  0.09458294, -0.22713672,  0.11310589],\n       [ 0.0196488 ,  0.09507482, -0.22835228,  0.11362866],\n       [ 0.01977215,  0.09537537, -0.22909543,  0.1139479 ],\n       [ 0.01980275,  0.09544975, -0.22927937,  0.11402687],\n       [ 0.05295902, -0.8473811 ,  0.62240045,  0.17202163],\n       [-0.86946916,  0.21646977,  0.42378761,  0.22921178],\n       [ 0.00659863,  0.05375723,  0.87149791, -0.93185377],\n       [-0.87211505,  0.21507162,  0.42895136,  0.22809208],\n       [ 0.01971551,  0.0952375 , -0.22875449,  0.11380148],\n       [ 0.00641834,  0.05296349,  0.87338069, -0.93276253],\n       [-0.87582374,  0.21304518,  0.43632504,  0.22645352],\n       [ 0.00670689,  0.0542287 ,  0.87037913, -0.93131471],\n       [-0.87388005,  0.21411707,  0.43244048,  0.2273225 ],\n       [ 0.00700338,  0.05550058,  0.86735917, -0.92986313],\n       [ 0.00664309,  0.05395132,  0.87103739, -0.93163179],\n       [-0.87542408,  0.21326738,  0.43552263,  0.22663407],\n       [ 0.02007288,  0.09610356, -0.23089713,  0.1147207 ],\n       [ 0.00670231,  0.0542088 ,  0.87042634, -0.93133745],\n       [ 0.05507272, -0.84472723,  0.61507439,  0.17458011],\n       [ 0.00685293,  0.05485861,  0.86888379, -0.93059533],\n       [ 0.02062928,  0.09743431, -0.23419424,  0.11613065]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.49934384,  0.71022374, -0.09181652,  0.04538306,  0.13736213,\n        -0.01723307, -0.41244775,  0.20566909, -0.16616513,  0.36467306,\n        -0.71521475,  0.14872631,  0.02940374,  0.18820957,  0.3883842 ,\n         0.28934322],\n       [ 0.1945828 ,  0.6874966 , -0.37035842, -0.28054639,  0.44193886,\n         0.33731519, -0.11719669, -0.00668149,  0.06714353, -0.02601073,\n        -0.27457023,  0.34839494,  0.2971754 ,  0.12737456, -0.07918443,\n        -0.49147727],\n       [ 0.8107233 , -0.74157065, -0.39402901,  0.10990078,  0.32856888,\n         0.62085117, -0.13265802, -0.21289445, -0.10452892,  0.20406277,\n         0.10839519,  0.56567504,  0.7772092 , -0.05682288, -0.69625887,\n        -0.04994852],\n       [-0.16939948,  0.00161532,  0.11402961,  0.01855819, -0.11477176,\n        -0.14163031,  0.04992143,  0.0275235 ,  0.01292559, -0.04051935,\n         0.04297226, -0.13989836, -0.16234857, -0.0141956 ,  0.10696579,\n         0.06364969],\n       [ 0.81119398, -0.74155948, -0.3943463 ,  0.10984706,  0.32888953,\n         0.62124243, -0.13280014, -0.21296772, -0.10456513,  0.20417694,\n         0.10826708,  0.56606338,  0.7776571 , -0.0567806 , -0.69654843,\n        -0.05012567],\n       [ 0.19459408,  0.68749997, -0.37036537, -0.28054754,  0.4419461 ,\n         0.33732318, -0.11720108, -0.00668221,  0.06714228, -0.02600719,\n        -0.27457557,  0.34840347,  0.29718464,  0.12737619, -0.07918884,\n        -0.49148018],\n       [ 0.49717218,  0.70904631, -0.09063243,  0.04555187,  0.13608784,\n        -0.01848487, -0.41141752,  0.20561108, -0.16584178,  0.36382832,\n        -0.71377248,  0.14725216,  0.0279244 ,  0.18779064,  0.38875225,\n         0.2896034 ],\n       [-0.16852381,  0.00173359,  0.11345642,  0.01845976, -0.11418541,\n        -0.1409414 ,  0.04962753,  0.0274177 ,  0.01284778, -0.04028403,\n         0.04266535, -0.13919675, -0.16155797, -0.01409801,  0.10650138,\n         0.0633552 ],\n       [ 0.49836187,  0.70969244, -0.09128076,  0.04545952,  0.13678565,\n        -0.01779974, -0.4119823 ,  0.20564329, -0.1660191 ,  0.36429144,\n        -0.71456347,  0.14805936,  0.02873416,  0.18802036,  0.38855167,\n         0.28946155],\n       [-0.17252938,  0.0011835 ,  0.11607648,  0.01890968, -0.11686629,\n        -0.14408865,  0.05097477,  0.02789863,  0.01320481, -0.04136274,\n         0.04407581, -0.1424039 , -0.1651701 , -0.01454621,  0.10861843,\n         0.06469851],\n       [-0.17029118,  0.00149374,  0.11461306,  0.01865839, -0.11536869,\n        -0.14233132,  0.05022107,  0.02763085,  0.01300497, -0.04075926,\n         0.04328561, -0.14061253, -0.1631531 , -0.01429521,  0.10743779,\n         0.0639491 ],\n       [-0.1654158 ,  0.00214442,  0.11142017,  0.01811004, -0.11210306,\n        -0.13849237,  0.04858722,  0.02703927,  0.01257268, -0.03945111,\n         0.04158256, -0.13670437, -0.15874768, -0.01375338,  0.10484587,\n         0.06230623],\n       [-0.16553731,  0.00212862,  0.11149983,  0.01812372, -0.11218451,\n        -0.13858824,  0.04862781,  0.02705416,  0.0125834 , -0.03948361,\n         0.0416247 , -0.13680188, -0.15885767, -0.0137668 ,  0.10491081,\n         0.06234735],\n       [ 0.18683093,  0.68512918, -0.36559881, -0.27975748,  0.43697436,\n         0.33184869, -0.11416139, -0.00620225,  0.06800975, -0.02845784,\n        -0.27086494,  0.34254373,  0.29084967,  0.12624897, -0.07619016,\n        -0.48949843],\n       [ 0.48929107,  0.70469764, -0.08635894,  0.04615554,  0.13148232,\n        -0.02298444, -0.40765157,  0.20537134, -0.16465574,  0.36073819,\n        -0.70847766,  0.14192823,  0.0226008 ,  0.18625498,  0.39001637,\n         0.29050116],\n       [ 0.19812017,  0.68854703, -0.37253823, -0.28090883,  0.44421022,\n         0.33982494, -0.11857164, -0.00691093,  0.06675259, -0.02490279,\n        -0.27623814,  0.35107384,  0.30007809,  0.12788223, -0.08057707,\n        -0.49239589],\n       [ 0.50495109,  0.71322269, -0.09488669,  0.04494236,  0.14066259,\n        -0.01397744, -0.41509304,  0.20580303, -0.16699317,  0.36684085,\n        -0.7189057 ,  0.15254661,  0.03324785,  0.18928288,  0.38739503,\n         0.28864628],\n       [-0.16838228,  0.00175261,  0.11336376,  0.01844384, -0.11409063,\n        -0.14083002,  0.04958007,  0.02740057,  0.01283522, -0.04024603,\n         0.04261583, -0.13908333, -0.16143015, -0.01408226,  0.10642624,\n         0.06330756],\n       [ 0.81160316, -0.74155007, -0.39462217,  0.10980036,  0.32916831,\n         0.62158268, -0.13292361, -0.2130315 , -0.10459658,  0.20427614,\n         0.10815592,  0.56640103,  0.77804659, -0.0567439 , -0.69680036,\n        -0.05027976],\n       [ 0.49456178,  0.70761912, -0.08921282,  0.04575339,  0.13455908,\n        -0.01998278, -0.41017489,  0.20553677, -0.16545114,  0.36280908,\n        -0.7120293 ,  0.14548423,  0.02615324,  0.18728467,  0.38918344,\n         0.28990887],\n       [ 0.50008427,  0.71062314, -0.09222088,  0.04532526,  0.1377971 ,\n        -0.0168051 , -0.41279827,  0.20568808, -0.16627503,  0.3649604 ,\n        -0.71570485,  0.14922962,  0.02990933,  0.18835198,  0.38825678,\n         0.28925326],\n       [ 0.81193377, -0.74154267, -0.3948451 ,  0.10976263,  0.32939357,\n         0.62185765, -0.13302331, -0.2130831 , -0.10462197,  0.20435624,\n         0.10806624,  0.56667388,  0.77836137, -0.05671429, -0.69700406,\n        -0.05040431],\n       [ 0.50092896,  0.71107752, -0.09268256,  0.04525918,  0.13829365,\n        -0.01631615, -0.4131977 ,  0.20570926, -0.1664002 ,  0.36528781,\n        -0.71626295,  0.14980424,  0.03048688,  0.1885142 ,  0.38811023,\n         0.28914987],\n       [ 0.50022493,  0.7106989 , -0.09229773,  0.04531427,  0.13787977,\n        -0.01672374, -0.41286481,  0.20569165, -0.16629589,  0.36501496,\n        -0.71579786,  0.14932528,  0.03000545,  0.18837902,  0.38823247,\n         0.2892361 ],\n       [ 0.80981844, -0.74159315, -0.39341918,  0.11000407,  0.3279525 ,\n         0.62009935, -0.1323845 , -0.21275389, -0.10445923,  0.20384305,\n         0.10864215,  0.56492866,  0.77634853, -0.05690436, -0.69570293,\n        -0.04960827],\n       [ 0.50336962,  0.71238285, -0.09401892,  0.04506735,  0.13973025,\n        -0.01489907, -0.4143491 ,  0.20576755, -0.16676062,  0.36623138,\n        -0.71786949,  0.1514671 ,  0.03216012,  0.18898138,  0.38767966,\n         0.2888465 ],\n       [ 0.81171588, -0.74154753, -0.39469818,  0.1097875 ,  0.32924511,\n         0.62167642, -0.13295761, -0.21304909, -0.10460524,  0.20430345,\n         0.10812533,  0.56649406,  0.77815391, -0.0567338 , -0.6968698 ,\n        -0.05032222],\n       [ 0.49650028,  0.7086802 , -0.09026664,  0.04560388,  0.13569404,\n        -0.01887113, -0.41109812,  0.20559243, -0.16574144,  0.36356638,\n        -0.71332479,  0.14679668,  0.02746777,  0.18766066,  0.38886441,\n         0.28968279],\n       [ 0.1941566 ,  0.68736877, -0.37009612, -0.28050282,  0.44166545,\n         0.33701344, -0.1170306 , -0.0066543 ,  0.06719081, -0.02614459,\n        -0.27436831,  0.34807255,  0.29682635,  0.12731314, -0.07901774,\n        -0.49136725],\n       [-0.1656921 ,  0.00210846,  0.1116013 ,  0.01814115, -0.11228826,\n        -0.13871033,  0.04867953,  0.0270731 ,  0.01259707, -0.03952501,\n         0.04167841, -0.13692608, -0.15899778, -0.01378391,  0.1049935 ,\n         0.06239972],\n       [-0.16658691,  0.00199126,  0.11218777,  0.01824188, -0.11288792,\n        -0.1394159 ,  0.0489787 ,  0.02718241,  0.01267614, -0.03976454,\n         0.04198937, -0.13764391, -0.15980739, -0.01388292,  0.105471  ,\n         0.06270217],\n       [-0.16713405,  0.00191903,  0.11254626,  0.01830344, -0.11325451,\n        -0.13984707,  0.04916181,  0.02724906,  0.01272456, -0.03991115,\n         0.04217993, -0.13808269, -0.16030215, -0.01394357,  0.1057625 ,\n         0.06288686],\n       [-0.16726948,  0.00190108,  0.11263498,  0.01831868, -0.11334525,\n        -0.13995376,  0.04920716,  0.02726553,  0.01273655, -0.03994746,\n         0.04222714, -0.13819129, -0.16042459, -0.0139586 ,  0.10583461,\n         0.06293256],\n       [ 0.19923473,  0.68887415, -0.37322606, -0.28102334,  0.44492664,\n         0.34061769, -0.11900356, -0.00698461,  0.06662998, -0.02455482,\n        -0.27676071,  0.35191906,  0.30099475,  0.12804142, -0.08101926,\n        -0.49268734],\n       [ 0.49466291,  0.70767466, -0.08926774,  0.04574561,  0.13461825,\n        -0.01992489, -0.41022312,  0.20553974, -0.16546631,  0.36284865,\n        -0.71209703,  0.14555265,  0.02622172,  0.18730433,  0.38916696,\n         0.28989718],\n       [ 0.81308643, -0.74151826, -0.39562253,  0.10963111,  0.33017903,\n         0.62281688, -0.13337051, -0.2132634 , -0.10471037,  0.20463523,\n         0.10775452,  0.56762543,  0.77945942, -0.05661131, -0.69771527,\n        -0.05083899],\n       [ 0.49879709,  0.70992815, -0.09151815,  0.04542566,  0.1370411 ,\n        -0.01754871, -0.41218867,  0.20565481, -0.16608386,  0.36446065,\n        -0.71485231,  0.14835488,  0.02903079,  0.18810427,  0.38847766,\n         0.28940924],\n       [-0.16688303,  0.00195222,  0.1123818 ,  0.0182752 , -0.11308633,\n        -0.13964928,  0.04907779,  0.0272185 ,  0.01270234, -0.03984387,\n         0.04209246, -0.1378814 , -0.16007519, -0.01391573,  0.10562881,\n         0.06280215],\n       [ 0.81445609, -0.74149213, -0.39654671,  0.10947486,  0.33111253,\n         0.62395768, -0.13378225, -0.21347848, -0.10481515,  0.20496614,\n         0.10738605,  0.56875663,  0.78076531, -0.05648949, -0.6985624 ,\n        -0.05135632],\n       [ 0.50468633,  0.71308241, -0.09474131,  0.04496332,  0.14050642,\n        -0.01413192, -0.41496861,  0.20579721, -0.16695429,  0.36673892,\n        -0.71873248,  0.15236577,  0.03306556,  0.18923247,  0.38744299,\n         0.28868   ],\n       [ 0.81227245, -0.74153527, -0.3950735 ,  0.10972399,  0.32962434,\n         0.62213942, -0.13312539, -0.21313601, -0.10464796,  0.20443826,\n         0.10797449,  0.56695343,  0.77868392, -0.05668399, -0.69721287,\n        -0.05053196],\n       [ 0.50158586,  0.71142995, -0.0930419 ,  0.04520768,  0.13868003,\n        -0.01593537, -0.41350799,  0.20572538, -0.16649739,  0.36554213,\n        -0.71669623,  0.15025142,  0.03093657,  0.18864017,  0.38799538,\n         0.28906889],\n       [ 0.81007484, -0.74158664, -0.39359196,  0.10997481,  0.32812714,\n         0.62031233, -0.13246205, -0.21279368, -0.10447899,  0.20390534,\n         0.10857208,  0.56514013,  0.77659235, -0.05688124, -0.69586036,\n        -0.04970464],\n       [ 0.81275138, -0.74152513, -0.39539652,  0.10966934,  0.3299507 ,\n         0.62253798, -0.13326965, -0.21321092, -0.10468469,  0.20455418,\n         0.10784497,  0.5673488 ,  0.77914016, -0.0566412 , -0.69750838,\n        -0.05071257],\n       [ 0.50404626,  0.71274275, -0.09439002,  0.04501394,  0.14012901,\n        -0.01450507, -0.4146676 ,  0.20578295, -0.16686021,  0.36649233,\n        -0.7183133 ,  0.15192878,  0.03262517,  0.18911049,  0.38755842,\n         0.28876118],\n       [-0.16846078,  0.00174206,  0.11341516,  0.01845267, -0.1141432 ,\n        -0.1408918 ,  0.04960639,  0.02741008,  0.01284219, -0.04026711,\n         0.0426433 , -0.13914624, -0.16150105, -0.01409099,  0.10646792,\n         0.06333398],\n       [ 0.8123068 , -0.74153453, -0.39509666,  0.10972007,  0.32964775,\n         0.622168  , -0.13313574, -0.21314138, -0.1046506 ,  0.20444658,\n         0.10796519,  0.56698179,  0.77871664, -0.05668091, -0.69723406,\n        -0.05054492],\n       [ 0.19369021,  0.68722859, -0.36980917, -0.28045518,  0.44136631,\n         0.3366834 , -0.11684874, -0.00662466,  0.0672426 , -0.02629116,\n        -0.27414711,  0.34771984,  0.29644455,  0.12724587, -0.07883561,\n        -0.49124701],\n       [ 0.81118438, -0.74155971, -0.39433982,  0.10984816,  0.32888299,\n         0.62123444, -0.13279724, -0.21296622, -0.10456439,  0.20417461,\n         0.1082697 ,  0.56605546,  0.77764796, -0.05678146, -0.69654251,\n        -0.05012205],\n       [-0.1708896 ,  0.0014115 ,  0.11500448,  0.0187256 , -0.11576919,\n        -0.14280148,  0.05042236,  0.02770268,  0.01305831, -0.04092043,\n         0.04349637, -0.14109166, -0.16369272, -0.01436218,  0.10775402,\n         0.06414977]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.49934384,  0.71022374],\n       [-0.09181652,  0.04538306]])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        ],\n       [0.        , 1.16313412, 0.        ],\n       [0.        , 0.        , 0.        ]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        ],\n       [0.        , 1.16313412, 0.        ],\n       [0.        , 0.        , 0.        ]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8 ms ± 313 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "# test_conv_back.shape\n",
    "# [[ 3.76629539  2.02311445]\n",
    "#  [-4.3710521  -0.02371311]]\n",
    "# [[ 1.75740352 -2.03670953]\n",
    "#  [ 0.75098017  0.30702159]]\n",
    "# [[ 3.73144254  1.97063061]\n",
    "#  [-4.44140316 -0.11088261]]\n",
    "# [[ 1.69251643 -2.13383967]\n",
    "#  [ 0.62075872  0.14498783]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f9e175d63a0>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 1, 28, 28)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parse_input_model()\n",
    "mnist_batch_1 = x_train[0:32].reshape(32, 1, 28, 28)\n",
    "mnist_labels_1 = y_train[0:32]\n",
    "mnist_batch_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# train\n",
    "mnist_subsample_x = x_train[:4992]\n",
    "mnist_subsample_y = y_train[:4992]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:2000]\n",
    "mnist_validation_y = y_test[:2000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:7001]\n",
    "mnist_test_y = y_test[5001:7001]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "validation_batch = mnist_validation_x.reshape(2000, 1, 28, 28)\n",
    "validation_labels = label_binarizer.transform(mnist_validation_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "test_batch = mnist_test_x.reshape(2000, 1, 28, 28)\n",
    "test_labels = label_binarizer.transform(mnist_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "def measure_accuracy(y_true, y_pred):\n",
    "    accurate = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total = y_true.shape[0]\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def predict_labels(a):\n",
    "    return (a == a.max(axis=1)[:,None]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# inp = mnist_batch_1\n",
    "# for layer in model:\n",
    "#     inp = layer.forward(inp)\n",
    "#\n",
    "# labels_true = label_binarizer.transform(mnist_labels_1)\n",
    "# l = loss_function(labels_true, inp)\n",
    "#\n",
    "# out = labels_true\n",
    "# for layer in reversed(model):\n",
    "#     out = layer.backward(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     losses = []\n",
    "#     index = [i for i in range(1,157)]\n",
    "#     for j in range(0, 4992, 32):\n",
    "#         batch_x = mnist_subsample_x[j:j+32].reshape(32, 1, 28, 28)\n",
    "#         batch_y = mnist_subsample_y[j:j+32]\n",
    "#         model_out = batch_x\n",
    "#         # train\n",
    "#         for layer in model:\n",
    "#             # print(layer)\n",
    "#             model_out = layer.forward(model_out)\n",
    "#             # print(model_out.shape)\n",
    "#\n",
    "#         true_labels = label_binarizer.transform(batch_y)\n",
    "#         l = loss_function(true_labels, model_out)\n",
    "#         losses.append(l)\n",
    "#         # print(\"Epoc {} batch {} loss = {}\".format(i, j//32, l))\n",
    "#\n",
    "#\n",
    "#\n",
    "#         model_back = true_labels\n",
    "#         for layer in reversed(model):\n",
    "#             # print(layer)\n",
    "#             model_back = layer.backward(model_back)\n",
    "#             # print(model_back.shape)\n",
    "#\n",
    "#     plt.plot(index, losses)\n",
    "#     plt.show()\n",
    "#\n",
    "#     #validation\n",
    "#     # validation_out = validation_batch\n",
    "#     # for layer in model:\n",
    "#     #     validation_out = layer.forward(validation_out)\n",
    "#     # validation_loss = loss_function(validation_labels, validation_out)\n",
    "#     # print('Validation loss after epoc {} is {}'.format(i, validation_loss))\n",
    "#     # validation_predictions = predict_labels(validation_out)\n",
    "#     # accuracy = measure_accuracy(validation_labels, validation_predictions)\n",
    "#     # print('Validation accuracy after epoc {} is {}'.format(i, accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}