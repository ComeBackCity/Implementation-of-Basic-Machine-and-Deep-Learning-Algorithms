{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(120)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6, -1],\n       [ 9,  7]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(-1, 10, (2, 2))\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0).astype(np.int32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        # padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * np.sqrt(2/(input_shape[1] * self.filter_dimension * self.filter_dimension))\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters, axis=(1, 2, 3)) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 1e-3) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dz_prime_dim = (dz.shape[2] - 1) * self.stride + 1\n",
    "        dz_prime = np.zeros((dz.shape[0], dz.shape[1], dz_prime_dim, dz_prime_dim))\n",
    "        dz_prime[:, :, ::self.stride, ::self.stride] = dz\n",
    "\n",
    "        # calculate dw\n",
    "        for i in range(padded_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + dz_prime_dim <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + dz_prime_dim <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+dz_prime_dim, image_y:image_y+dz_prime_dim]\n",
    "                    dz_slice = dz_prime[i, :, :, :]\n",
    "                    dz_slice_shape = dz_slice.shape\n",
    "                    dz_slice = np.broadcast_to(dz_slice, (image_slice.shape[0], dz_slice_shape[0], dz_slice_shape[1], dz_slice_shape[2])).transpose((1, 0, 2, 3))\n",
    "                    dw[:, :, out_x, out_y] += np.sum(image_slice * dz_slice, axis=(2, 3))\n",
    "                    image_x += 1\n",
    "                    out_x += 1\n",
    "                image_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        rotated_filter = np.rot90(self.filters, 2, axes=(2, 3))\n",
    "        dx = np.zeros(self.input_batch.shape)\n",
    "        padding = self.filter_dimension - 1 - self.padding\n",
    "        if padding < 0:\n",
    "            dz_prime_padded = dz_prime[:, :, -padding:padding, -padding:padding]\n",
    "        else:\n",
    "            dz_prime_padded = np.pad(dz_prime, [(0, 0), (0, 0), (padding, padding), (padding, padding)], mode='constant')\n",
    "\n",
    "        dz_padded_dimensions = dz_prime_padded.shape\n",
    "\n",
    "        # calculate dx\n",
    "        for i in range(dz_padded_dimensions[0]):\n",
    "            dz_y = out_y = 0\n",
    "            while dz_y + self.filter_dimension <= dz_padded_dimensions[3]:\n",
    "                dz_x = out_x = 0\n",
    "                while dz_x + self.filter_dimension <= dz_padded_dimensions[2]:\n",
    "                    dzp_slice = dz_prime_padded[i, :, dz_x:dz_x+self.filter_dimension, dz_y:dz_y+self.filter_dimension]\n",
    "                    dzp_slice = dzp_slice.reshape(dz_padded_dimensions[1], 1, self.filter_dimension, self.filter_dimension)\n",
    "                    dx[i, :, out_x, out_y] = np.sum(dzp_slice * rotated_filter, axis=(0, 2, 3))\n",
    "                    dz_x += 1\n",
    "                    out_x += 1\n",
    "                dz_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        self.filters -= learning_rate * dw / batch_size\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        , -6.33984923,  0.        ],\n       [ 0.        ,  0.        ,  0.        ]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_matrix\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    def backward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(self.input_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [ 0., -0.,  0.],\n       [ 0.,  0.,  0.]])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        # 32 x 3 x 32 x 32\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.max(image_slice, axis=(1, 2))\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == np.max(image_slice, axis=(1, 2), keepdims=True)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[25.97456031, 25.97456031],\n       [25.97456031, 25.97456031]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 1, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.97456031,  25.97456031,  25.97456031,  25.97456031,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.95987843,  38.95987843,  38.95987843,  38.95987843],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.48349051,  51.48349051,  51.48349051,  51.48349051,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         75.88419886,  75.88419886,  75.88419886,  75.88419886],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.12366923, 103.12366923, 103.12366923, 103.12366923,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.78603067, 151.78603067, 151.78603067, 151.78603067],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.24987482,  77.24987482,  77.24987482,  77.24987482,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        113.81456442, 113.81456442, 113.81456442, 113.81456442],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.69022593, 102.69022593, 102.69022593, 102.69022593,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.19914393, 151.19914393, 151.19914393, 151.19914393],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         50.63017963,  50.63017963,  50.63017963,  50.63017963,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         74.43870699,  74.43870699,  74.43870699,  74.43870699],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.33780319,  25.33780319,  25.33780319,  25.33780319,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.30451302,  37.30451302,  37.30451302,  37.30451302],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.82280721,  77.82280721,  77.82280721,  77.82280721,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.75905594, 114.75905594, 114.75905594, 114.75905594],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.55241593,  25.55241593,  25.55241593,  25.55241593,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.63542595,  37.63542595,  37.63542595,  37.63542595],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.57311056,  77.57311056,  77.57311056,  77.57311056,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        115.335647  , 115.335647  , 115.335647  , 115.335647  ],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.80133299,  76.80133299,  76.80133299,  76.80133299,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        113.14638816, 113.14638816, 113.14638816, 113.14638816],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.90878409,  76.90878409,  76.90878409,  76.90878409,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.38025919, 112.38025919, 112.38025919, 112.38025919],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.96540578,  76.96540578,  76.96540578,  76.96540578,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.78945909, 112.78945909, 112.78945909, 112.78945909],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.18594076,  51.18594076,  51.18594076,  51.18594076,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         76.16121195,  76.16121195,  76.16121195,  76.16121195],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.29357666,  25.29357666,  25.29357666,  25.29357666,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.33278144,  38.33278144,  38.33278144,  38.33278144],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         50.83522836,  50.83522836,  50.83522836,  50.83522836,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         74.07785337,  74.07785337,  74.07785337,  74.07785337],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.43382406,  25.43382406,  25.43382406,  25.43382406,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         36.37256885,  36.37256885,  36.37256885,  36.37256885],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.93017211,  77.93017211,  77.93017211,  77.93017211,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.89815057, 114.89815057, 114.89815057, 114.89815057],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        101.85742406, 101.85742406, 101.85742406, 101.85742406,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        148.99937623, 148.99937623, 148.99937623, 148.99937623],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.14186055,  25.14186055,  25.14186055,  25.14186055,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.44707834,  37.44707834,  37.44707834,  37.44707834],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.01282617,  26.01282617,  26.01282617,  26.01282617,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.72209436,  38.72209436,  38.72209436,  38.72209436],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.23340602, 103.23340602, 103.23340602, 103.23340602,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.69179934, 151.69179934, 151.69179934, 151.69179934],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.36870771,  26.36870771,  26.36870771,  26.36870771,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.82935721,  38.82935721,  38.82935721,  38.82935721],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.60410268,  26.60410268,  26.60410268,  26.60410268,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         39.08134678,  39.08134678,  39.08134678,  39.08134678],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.79306759, 103.79306759, 103.79306759, 103.79306759,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        153.27149018, 153.27149018, 153.27149018, 153.27149018],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         24.92205808,  24.92205808,  24.92205808,  24.92205808,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         36.03780327,  36.03780327,  36.03780327,  36.03780327],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.58776074, 103.58776074, 103.58776074, 103.58776074,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        152.56965228, 152.56965228, 152.56965228, 152.56965228],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.69304089,  25.69304089,  25.69304089,  25.69304089,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.78374544,  37.78374544,  37.78374544,  37.78374544],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.83347051,  51.83347051,  51.83347051,  51.83347051,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         76.1209857 ,  76.1209857 ,  76.1209857 ,  76.1209857 ],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.4610109 ,  77.4610109 ,  77.4610109 ,  77.4610109 ,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.89108624, 112.89108624, 112.89108624, 112.89108624],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.16054177,  77.16054177,  77.16054177,  77.16054177,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.65319492, 112.65319492, 112.65319492, 112.65319492],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.88300575,  77.88300575,  77.88300575,  77.88300575,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.54661232, 114.54661232, 114.54661232, 114.54661232],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.66378676,  77.66378676,  77.66378676,  77.66378676,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.01694212, 114.01694212, 114.01694212, 114.01694212],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         52.82206911,  52.82206911,  52.82206911,  52.82206911,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         78.02859262,  78.02859262,  78.02859262,  78.02859262],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.58698607,  25.58698607,  25.58698607,  25.58698607,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.80189349,  37.80189349,  37.80189349,  37.80189349],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.59501988, 102.59501988, 102.59501988, 102.59501988,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.33202437, 150.33202437, 150.33202437, 150.33202437],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.17406697,  26.17406697,  26.17406697,  26.17406697,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.86604643,  38.86604643,  38.86604643,  38.86604643],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.71149408,  77.71149408,  77.71149408,  77.71149408,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.17581935, 114.17581935, 114.17581935, 114.17581935],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.89250028, 102.89250028, 102.89250028, 102.89250028,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.12686045, 151.12686045, 151.12686045, 151.12686045],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.77252229,  26.77252229,  26.77252229,  26.77252229,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         39.26081147,  39.26081147,  39.26081147,  39.26081147],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.55661109, 102.55661109, 102.55661109, 102.55661109,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.43799466, 150.43799466, 150.43799466, 150.43799466],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.83942619,  25.83942619,  25.83942619,  25.83942619,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.23077925,  37.23077925,  37.23077925,  37.23077925],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.05835601, 103.05835601, 103.05835601, 103.05835601,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        152.4055621 , 152.4055621 , 152.4055621 , 152.4055621 ],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.76962261, 102.76962261, 102.76962261, 102.76962261,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.59642139, 150.59642139, 150.59642139, 150.59642139],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.435151  ,  26.435151  ,  26.435151  ,  26.435151  ,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.69273213,  38.69273213,  38.69273213,  38.69273213],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.51450566,  77.51450566,  77.51450566,  77.51450566,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        113.72373158, 113.72373158, 113.72373158, 113.72373158],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.12257936, 103.12257936, 103.12257936, 103.12257936,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.45159925, 151.45159925, 151.45159925, 151.45159925],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.76629409,  51.76629409,  51.76629409,  51.76629409,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         76.30137878,  76.30137878,  76.30137878,  76.30137878],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.51169287, 102.51169287, 102.51169287, 102.51169287,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.58005653, 150.58005653, 150.58005653, 150.58005653],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.5940859 ,  76.5940859 ,  76.5940859 ,  76.5940859 ,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.72757775, 112.72757775, 112.72757775, 112.72757775]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * np.sqrt(2/flattened_input.shape[1])\n",
    "            # self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * 0.01\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros((1, self.output_dimension))\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 1e-3) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6.46683713e+01,  1.23203857e+01, -3.75496823e-01,\n        -1.80154893e+01],\n       [ 1.26509648e+02,  2.37432235e+01, -2.96171882e-01,\n        -3.46999234e+01],\n       [ 2.53138252e+02,  4.74507553e+01, -5.21760363e-01,\n        -6.93446301e+01],\n       [ 1.89765083e+02,  3.56018814e+01, -4.28297350e-01,\n        -5.20302655e+01],\n       [ 2.52138002e+02,  4.72771688e+01, -5.36688497e-01,\n        -6.90916836e+01],\n       [ 1.24178636e+02,  2.32546770e+01, -2.28345419e-01,\n        -3.39832406e+01],\n       [ 6.22095928e+01,  1.16639676e+01, -1.31622183e-01,\n        -1.70458926e+01],\n       [ 1.91297693e+02,  3.59167224e+01, -4.65109771e-01,\n        -5.24918270e+01],\n       [ 6.27551483e+01,  1.17703235e+01, -1.37744039e-01,\n        -1.72015370e+01],\n       [ 1.91862356e+02,  3.62796149e+01, -7.80228091e-01,\n        -5.30357081e+01],\n       [ 1.88654098e+02,  3.53914560e+01, -4.23355461e-01,\n        -5.17226349e+01],\n       [ 1.87764999e+02,  3.49731472e+01, -1.14162979e-01,\n        -5.10980529e+01],\n       [ 1.88310433e+02,  3.51641078e+01, -2.23647897e-01,\n        -5.13817996e+01],\n       [ 1.26671035e+02,  2.39681349e+01, -5.34259922e-01,\n        -3.50388700e+01],\n       [ 6.34647832e+01,  1.21969343e+01, -4.97794878e-01,\n        -1.78404623e+01],\n       [ 1.23855432e+02,  2.30136752e+01, -7.31899378e-03,\n        -3.36215302e+01],\n       [ 6.11065994e+01,  1.11649997e+01,  2.27556449e-01,\n        -1.63012970e+01],\n       [ 1.91537626e+02,  3.59565428e+01, -4.59308035e-01,\n        -5.25497489e+01],\n       [ 2.48878874e+02,  4.64010677e+01, -2.06085214e-01,\n        -6.77973699e+01],\n       [ 6.22661601e+01,  1.17919078e+01, -2.75052785e-01,\n        -1.72390496e+01],\n       [ 6.43954632e+01,  1.21891550e+01, -2.77132512e-01,\n        -1.78194878e+01],\n       [ 2.53088625e+02,  4.73718216e+01, -4.36611064e-01,\n        -6.92256058e+01],\n       [ 6.47494808e+01,  1.21421041e+01, -1.39342306e-01,\n        -1.77447494e+01],\n       [ 6.52094558e+01,  1.22026019e+01, -1.08870719e-01,\n        -1.78318051e+01],\n       [ 2.55405306e+02,  4.80118952e+01, -6.92763125e-01,\n        -7.01718593e+01],\n       [ 6.03723524e+01,  1.11413051e+01,  8.99044952e-02,\n        -1.62726756e+01],\n       [ 2.54402855e+02,  4.77151763e+01, -5.57797973e-01,\n        -6.97324983e+01],\n       [ 6.30271695e+01,  1.18053419e+01, -1.18796674e-01,\n        -1.72518710e+01],\n       [ 1.27021569e+02,  2.37634009e+01, -2.04667371e-01,\n        -3.47254161e+01],\n       [ 1.88743886e+02,  3.50744201e+01, -1.57585820e-02,\n        -5.12417195e+01],\n       [ 1.88261226e+02,  3.50395861e+01, -8.27226582e-02,\n        -5.11937459e+01],\n       [ 1.91069964e+02,  3.57920706e+01, -3.64530140e-01,\n        -5.23053402e+01],\n       [ 1.90273648e+02,  3.55864416e+01, -2.94051602e-01,\n        -5.20018628e+01],\n       [ 1.30012696e+02,  2.44472875e+01, -3.61307759e-01,\n        -3.57313433e+01],\n       [ 6.29841761e+01,  1.18447239e+01, -1.76652304e-01,\n        -1.73119247e+01],\n       [ 2.50997528e+02,  4.68654562e+01, -2.92586721e-01,\n        -6.84795728e+01],\n       [ 6.46747945e+01,  1.22160930e+01, -2.46657307e-01,\n        -1.78575150e+01],\n       [ 1.90501338e+02,  3.56532602e+01, -3.24002953e-01,\n        -5.21007839e+01],\n       [ 2.52173016e+02,  4.71829931e+01, -4.13718947e-01,\n        -6.89487450e+01],\n       [ 6.55375244e+01,  1.22454676e+01, -8.67915463e-02,\n        -1.78934671e+01],\n       [ 2.51105937e+02,  4.69300208e+01, -3.46848807e-01,\n        -6.85762598e+01],\n       [ 6.24280420e+01,  1.14838219e+01,  1.37969994e-01,\n        -1.67709731e+01],\n       [ 2.53870576e+02,  4.77828775e+01, -7.61255719e-01,\n        -6.98402648e+01],\n       [ 2.51435359e+02,  4.69495429e+01, -2.95950661e-01,\n        -6.86025636e+01],\n       [ 6.46201852e+01,  1.20540435e+01, -6.11215827e-02,\n        -1.76126933e+01],\n       [ 1.89815548e+02,  3.54805632e+01, -2.68670430e-01,\n        -5.18460773e+01],\n       [ 2.52720441e+02,  4.72818206e+01, -4.10221407e-01,\n        -6.90929721e+01],\n       [ 1.27205002e+02,  2.38738198e+01, -2.97913050e-01,\n        -3.48907904e+01],\n       [ 2.51255334e+02,  4.70147038e+01, -4.16383302e-01,\n        -6.87030032e+01],\n       [ 1.88003466e+02,  3.52385208e+01, -3.84182581e-01,\n        -5.14975023e+01]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        # input_matrix -= np.max(input_matrix, axis=1).reshape(-1, 1)\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.00000000e+000, 1.84314163e-023, 5.64685974e-029,\n        1.23269487e-036],\n       [1.00000000e+000, 2.33942433e-045, 8.49047979e-056,\n        9.71790841e-071],\n       [1.00000000e+000, 4.68873005e-090, 6.86841274e-111,\n        8.86061433e-141],\n       [1.00000000e+000, 1.11627731e-067, 2.51225814e-083,\n        9.76369616e-106],\n       [1.00000000e+000, 1.07169304e-089, 1.83982295e-110,\n        3.10255826e-140],\n       [1.00000000e+000, 1.47666408e-044, 9.34834377e-055,\n        2.04724560e-069],\n       [1.00000000e+000, 1.11767075e-022, 8.42469773e-028,\n        3.79996882e-035],\n       [1.00000000e+000, 3.30295006e-068, 5.22965534e-084,\n        1.32909758e-106],\n       [1.00000000e+000, 7.20397052e-023, 4.85248163e-028,\n        1.88474584e-035],\n       [1.00000000e+000, 2.69944302e-068, 2.16964584e-084,\n        4.38654131e-107],\n       [1.00000000e+000, 2.74713674e-067, 7.66840907e-083,\n        4.03377146e-105],\n       [1.00000000e+000, 4.39887701e-067, 2.54165507e-082,\n        1.83271335e-104],\n       [1.00000000e+000, 3.08600073e-067, 1.32034977e-082,\n        7.99808492e-105],\n       [1.00000000e+000, 2.49285680e-045, 5.69432796e-056,\n        5.89224381e-071],\n       [1.00000000e+000, 5.42820377e-023, 1.66496574e-028,\n        4.89305981e-036],\n       [1.00000000e+000, 1.60317646e-044, 1.61098159e-054,\n        4.06091730e-069],\n       [1.00000000e+000, 2.04474352e-022, 3.63552891e-027,\n        2.41089207e-034],\n       [1.00000000e+000, 2.70392270e-068, 4.13800693e-084,\n        9.86734229e-107],\n       [1.00000000e+000, 1.16146518e-088, 6.66464269e-109,\n        2.94615045e-138],\n       [1.00000000e+000, 1.20035782e-022, 6.89757343e-028,\n        2.96023468e-035],\n       [1.00000000e+000, 2.12367683e-023, 8.18556297e-029,\n        1.97015531e-036],\n       [1.00000000e+000, 4.55331146e-090, 7.85939072e-111,\n        1.04883713e-140],\n       [1.00000000e+000, 1.42202288e-023, 6.59389753e-029,\n        1.49008500e-036],\n       [1.00000000e+000, 9.53708932e-024, 4.29152096e-029,\n        8.62260823e-037],\n       [1.00000000e+000, 8.51502825e-091, 5.99821088e-112,\n        4.01452017e-142],\n       [1.00000000e+000, 4.16128847e-022, 6.60184814e-027,\n        5.16996139e-034],\n       [1.00000000e+000, 1.72457391e-090, 1.87066252e-111,\n        1.69748059e-141],\n       [1.00000000e+000, 5.68385394e-023, 3.76751793e-028,\n        1.36538677e-035],\n       [1.00000000e+000, 1.43069665e-045, 5.57632087e-056,\n        5.67776040e-071],\n       [1.00000000e+000, 1.82893820e-067, 1.05372686e-082,\n        5.96452559e-105],\n       [1.00000000e+000, 2.86211649e-067, 1.59684371e-082,\n        1.01397459e-104],\n       [1.00000000e+000, 3.66157258e-068, 7.26198685e-084,\n        2.01116753e-106],\n       [1.00000000e+000, 6.60997279e-068, 1.72782385e-083,\n        6.04063211e-106],\n       [1.00000000e+000, 1.42405059e-046, 2.39491115e-057,\n        1.04298520e-072],\n       [1.00000000e+000, 6.17188912e-023, 3.71193828e-028,\n        1.34229047e-035],\n       [1.00000000e+000, 2.22109650e-089, 7.34665437e-110,\n        1.79001968e-139],\n       [1.00000000e+000, 1.64996761e-023, 6.38221912e-029,\n        1.43441023e-036],\n       [1.00000000e+000, 5.62774072e-068, 1.33538728e-083,\n        4.35749569e-106],\n       [1.00000000e+000, 9.41812169e-090, 2.00897666e-110,\n        3.45613939e-140],\n       [1.00000000e+000, 7.17057217e-024, 3.16025188e-029,\n        5.83956851e-037],\n       [1.00000000e+000, 2.12581927e-089, 6.24371013e-110,\n        1.45809566e-139],\n       [1.00000000e+000, 7.50250558e-023, 8.86677046e-028,\n        4.02072890e-035],\n       [1.00000000e+000, 3.14231606e-090, 2.59896641e-111,\n        2.59518422e-141],\n       [1.00000000e+000, 1.55933076e-089, 4.72586256e-110,\n        1.02163562e-139],\n       [1.00000000e+000, 1.48188581e-023, 8.11456769e-029,\n        1.93515005e-036],\n       [1.00000000e+000, 9.40086390e-068, 2.80203046e-083,\n        1.11606540e-105],\n       [1.00000000e+000, 6.01368507e-090, 1.16613813e-110,\n        1.73065449e-140],\n       [1.00000000e+000, 1.32995704e-045, 4.22851052e-056,\n        4.00582240e-071],\n       [1.00000000e+000, 1.99260535e-089, 5.01605066e-110,\n        1.10626634e-139],\n       [1.00000000e+000, 4.51884229e-067, 1.52856424e-082,\n        9.68391572e-105]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "151.16192651773153"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.00000000e+000,  1.84314163e-023,  5.64685974e-029,\n         1.23269487e-036],\n       [ 1.00000000e+000, -1.00000000e+000,  8.49047979e-056,\n         9.71790841e-071],\n       [ 1.00000000e+000,  4.68873005e-090,  6.86841274e-111,\n        -1.00000000e+000],\n       [ 1.00000000e+000,  1.11627731e-067, -1.00000000e+000,\n         9.76369616e-106],\n       [ 1.00000000e+000,  1.07169304e-089,  1.83982295e-110,\n        -1.00000000e+000],\n       [ 1.00000000e+000, -1.00000000e+000,  9.34834377e-055,\n         2.04724560e-069],\n       [ 0.00000000e+000,  1.11767075e-022,  8.42469773e-028,\n         3.79996882e-035],\n       [ 1.00000000e+000,  3.30295006e-068, -1.00000000e+000,\n         1.32909758e-106],\n       [ 0.00000000e+000,  7.20397052e-023,  4.85248163e-028,\n         1.88474584e-035],\n       [ 1.00000000e+000,  2.69944302e-068, -1.00000000e+000,\n         4.38654131e-107],\n       [ 1.00000000e+000,  2.74713674e-067, -1.00000000e+000,\n         4.03377146e-105],\n       [ 1.00000000e+000,  4.39887701e-067, -1.00000000e+000,\n         1.83271335e-104],\n       [ 1.00000000e+000,  3.08600073e-067, -1.00000000e+000,\n         7.99808492e-105],\n       [ 1.00000000e+000, -1.00000000e+000,  5.69432796e-056,\n         5.89224381e-071],\n       [ 0.00000000e+000,  5.42820377e-023,  1.66496574e-028,\n         4.89305981e-036],\n       [ 1.00000000e+000, -1.00000000e+000,  1.61098159e-054,\n         4.06091730e-069],\n       [ 0.00000000e+000,  2.04474352e-022,  3.63552891e-027,\n         2.41089207e-034],\n       [ 1.00000000e+000,  2.70392270e-068, -1.00000000e+000,\n         9.86734229e-107],\n       [ 1.00000000e+000,  1.16146518e-088,  6.66464269e-109,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.20035782e-022,  6.89757343e-028,\n         2.96023468e-035],\n       [ 0.00000000e+000,  2.12367683e-023,  8.18556297e-029,\n         1.97015531e-036],\n       [ 1.00000000e+000,  4.55331146e-090,  7.85939072e-111,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.42202288e-023,  6.59389753e-029,\n         1.49008500e-036],\n       [ 0.00000000e+000,  9.53708932e-024,  4.29152096e-029,\n         8.62260823e-037],\n       [ 1.00000000e+000,  8.51502825e-091,  5.99821088e-112,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  4.16128847e-022,  6.60184814e-027,\n         5.16996139e-034],\n       [ 1.00000000e+000,  1.72457391e-090,  1.87066252e-111,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  5.68385394e-023,  3.76751793e-028,\n         1.36538677e-035],\n       [ 1.00000000e+000, -1.00000000e+000,  5.57632087e-056,\n         5.67776040e-071],\n       [ 1.00000000e+000,  1.82893820e-067, -1.00000000e+000,\n         5.96452559e-105],\n       [ 1.00000000e+000,  2.86211649e-067, -1.00000000e+000,\n         1.01397459e-104],\n       [ 1.00000000e+000,  3.66157258e-068, -1.00000000e+000,\n         2.01116753e-106],\n       [ 1.00000000e+000,  6.60997279e-068, -1.00000000e+000,\n         6.04063211e-106],\n       [ 1.00000000e+000, -1.00000000e+000,  2.39491115e-057,\n         1.04298520e-072],\n       [ 0.00000000e+000,  6.17188912e-023,  3.71193828e-028,\n         1.34229047e-035],\n       [ 1.00000000e+000,  2.22109650e-089,  7.34665437e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.64996761e-023,  6.38221912e-029,\n         1.43441023e-036],\n       [ 1.00000000e+000,  5.62774072e-068, -1.00000000e+000,\n         4.35749569e-106],\n       [ 1.00000000e+000,  9.41812169e-090,  2.00897666e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  7.17057217e-024,  3.16025188e-029,\n         5.83956851e-037],\n       [ 1.00000000e+000,  2.12581927e-089,  6.24371013e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  7.50250558e-023,  8.86677046e-028,\n         4.02072890e-035],\n       [ 1.00000000e+000,  3.14231606e-090,  2.59896641e-111,\n        -1.00000000e+000],\n       [ 1.00000000e+000,  1.55933076e-089,  4.72586256e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.48188581e-023,  8.11456769e-029,\n         1.93515005e-036],\n       [ 1.00000000e+000,  9.40086390e-068, -1.00000000e+000,\n         1.11606540e-105],\n       [ 1.00000000e+000,  6.01368507e-090,  1.16613813e-110,\n        -1.00000000e+000],\n       [ 1.00000000e+000, -1.00000000e+000,  4.22851052e-056,\n         4.00582240e-071],\n       [ 1.00000000e+000,  1.99260535e-089,  5.01605066e-110,\n        -1.00000000e+000],\n       [ 1.00000000e+000,  4.51884229e-067, -1.00000000e+000,\n         9.68391572e-105]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 1.20044931e-24,  1.52950073e-24, -5.56178809e-24,\n        -5.87148094e-24,  4.47679910e-24, -2.96856391e-24,\n        -3.97999500e-24, -2.77634886e-24, -5.66841387e-24,\n        -1.52833991e-24,  3.24762342e-24, -2.84228294e-24,\n        -7.53772731e-25,  8.62944213e-24, -2.24583386e-25,\n         1.67644442e-24],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 7.27973479e-24,  9.27460654e-24, -3.37264250e-23,\n        -3.56043633e-23,  2.71470474e-23, -1.80009992e-23,\n        -2.41342182e-23, -1.68358227e-23, -3.43728711e-23,\n        -9.26765133e-24,  1.96932268e-23, -1.72352955e-23,\n        -4.57097408e-24,  5.23286139e-23, -1.36184660e-24,\n         1.01656609e-23],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [ 4.69213642e-24,  5.97799041e-24, -2.17384324e-23,\n        -2.29488641e-23,  1.74976882e-23, -1.16026059e-23,\n        -1.55557901e-23, -1.08515422e-23, -2.21551153e-23,\n        -5.97349771e-24,  1.26933318e-23, -1.11090634e-23,\n        -2.94621502e-24,  3.37285003e-23, -8.77782783e-25,\n         6.55232184e-24],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 3.53542211e-24,  4.50450542e-24, -1.63799236e-23,\n        -1.72919945e-23,  1.31845417e-23, -8.74266492e-24,\n        -1.17214127e-23, -8.17657592e-24, -1.66939453e-23,\n        -4.50108674e-24,  9.56451814e-24, -8.37075708e-24,\n        -2.21992277e-24,  2.54144172e-23, -6.61416547e-25,\n         4.93726667e-24],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 1.33192081e-23,  1.69667629e-23, -6.17016595e-23,\n        -6.51371977e-23,  4.96646223e-23, -3.29313995e-23,\n        -4.41517533e-23, -3.08014415e-23, -6.28837819e-23,\n        -1.69543902e-23,  3.60273874e-23, -3.15308076e-23,\n        -8.36303341e-24,  9.57342266e-23, -2.49139500e-24,\n         1.85969025e-23],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 7.81818183e-24,  9.96084276e-24, -3.62215365e-23,\n        -3.82384170e-23,  2.91554341e-23, -1.93328303e-23,\n        -2.59198129e-23, -1.80812782e-23, -3.69158621e-23,\n        -9.95333689e-24,  2.11502403e-23, -1.85104541e-23,\n        -4.90908246e-24,  5.61998990e-23, -1.46260440e-24,\n         1.09178204e-23],\n       [ 1.38317290e-24,  1.76229152e-24, -6.40832132e-24,\n        -6.76515022e-24,  5.15818950e-24, -3.42038698e-24,\n        -4.58576142e-24, -3.19892945e-24, -6.53117182e-24,\n        -1.76095683e-24,  3.74192160e-24, -3.27488734e-24,\n        -8.68505289e-25,  9.94289093e-24, -2.58765593e-25,\n         1.93160063e-24],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 9.26184528e-25,  1.18003331e-24, -4.29103993e-24,\n        -4.52997356e-24,  3.45394499e-24, -2.29030083e-24,\n        -3.07063948e-24, -2.14202095e-24, -4.37329831e-24,\n        -1.17914147e-24,  2.50560234e-24, -2.19287493e-24,\n        -5.81557852e-25,  6.65780488e-24, -1.73270201e-25,\n         1.29340331e-24],\n       [ 6.21164004e-25,  7.91414168e-25, -2.87787417e-24,\n        -3.03811999e-24,  2.31645938e-24, -1.53603796e-24,\n        -2.05938826e-24, -1.43658995e-24, -2.93304278e-24,\n        -7.90815816e-25,  1.68043429e-24, -1.47069713e-24,\n        -3.90033389e-25,  4.46519356e-24, -1.16207267e-25,\n         8.67448103e-25],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 2.71056797e-23,  3.45296349e-23, -1.25569895e-22,\n        -1.32561646e-22,  1.01073242e-22, -6.70195176e-23,\n        -8.98542971e-23, -6.26841626e-23, -1.27975852e-22,\n        -3.45043212e-23,  7.33201674e-23, -6.41690610e-23,\n        -1.70195151e-23,  1.94829917e-22, -5.07029869e-24,\n         3.78471629e-23],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 3.70204093e-24,  4.71657149e-24, -1.71513848e-23,\n        -1.81064024e-23,  1.38054848e-23, -9.15433169e-24,\n        -1.22733513e-23, -8.56174569e-24, -1.74801445e-23,\n        -4.71302578e-24,  1.00149021e-23, -8.76493134e-24,\n        -2.32452974e-24,  2.66114159e-23, -6.92561191e-25,\n         5.16971276e-24],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 4.01988982e-24,  5.12156765e-24, -1.86240574e-23,\n        -1.96610777e-23,  1.49908719e-23, -9.94036959e-24,\n        -1.33272021e-23, -9.29687144e-24, -1.89810551e-23,\n        -5.11771113e-24,  1.08748296e-23, -9.51752967e-24,\n        -2.52411133e-24,  2.88963511e-23, -7.52027965e-25,\n         5.61361748e-24],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 1.07464126e-24,  1.36919313e-24, -4.97887556e-24,\n        -5.25610987e-24,  4.00759921e-24, -2.65743236e-24,\n        -3.56285733e-24, -2.48537352e-24, -5.07432286e-24,\n        -1.36815620e-24,  2.90724520e-24, -2.54438802e-24,\n        -6.74775798e-25,  7.72502101e-24, -2.01045106e-25,\n         1.50073600e-24],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 4.67029055e-25,  5.95034266e-25, -2.16376329e-24,\n        -2.28424601e-24,  1.74165710e-24, -1.15488839e-24,\n        -1.54837549e-24, -1.08011670e-24, -2.20524262e-24,\n        -5.94584277e-25,  1.26345442e-24, -1.10576104e-24,\n        -2.93250983e-25,  3.35720780e-24, -8.73718121e-26,\n         6.52201275e-25],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 4.88679179e-24,  6.22556781e-24, -2.26393217e-23,\n        -2.38998975e-23,  1.82227899e-23, -1.20832649e-23,\n        -1.62002381e-23, -1.13013856e-23, -2.30731754e-23,\n        -6.22095296e-24,  1.32192110e-23, -1.15693133e-23,\n        -3.06841282e-24,  3.51263436e-23, -9.14147382e-25,\n         6.82369676e-24],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 9.65181037e-25,  1.22970422e-24, -4.47168127e-24,\n        -4.72067276e-24,  3.59934551e-24, -2.38671056e-24,\n        -3.19989809e-24, -2.23219860e-24, -4.55739934e-24,\n        -1.22877693e-24,  2.61107628e-24, -2.28518474e-24,\n        -6.06043149e-25,  6.93808264e-24, -1.80564001e-25,\n         1.34784666e-24],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.20044931e-24,  1.52950073e-24],\n       [-5.56178809e-24, -5.87148094e-24]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.20044931e-24,  2.72995004e-24,  1.52950073e-24],\n       [-4.36133878e-24, -8.70331900e-24, -4.34198022e-24],\n       [-5.56178809e-24, -1.14332690e-23, -5.87148094e-24]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [-0., -0., -0.],\n       [-0., -0., -0.]])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 1, 2, 2)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "test_conv_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.60687076e-24, -2.44012914e-24],\n       [ 1.62296586e-23, -7.61966575e-27]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predictiona and Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "def measure_accuracy(y_true, y_pred):\n",
    "    accurate = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total = y_true.shape[0]\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def predict_labels(a):\n",
    "    return (a == a.max(axis=1)[:,None]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f1b07459c40>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mnist Dataset Train-test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "model = parse_input_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# train\n",
    "random_index = random.sample(range(0, 60000), 6400)\n",
    "mnist_subsample_x = x_train[random_index]\n",
    "mnist_subsample_y = y_train[random_index]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:2000]\n",
    "mnist_validation_y = y_test[:2000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:7001]\n",
    "mnist_test_y = y_test[5001:7001]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "validation_batch = mnist_validation_x.reshape(2000, 1, 28, 28) / 255.0\n",
    "validation_labels = label_binarizer.transform(mnist_validation_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "test_batch = mnist_test_x.reshape(2000, 1, 28, 28) / 255.0\n",
    "test_labels = label_binarizer.transform(mnist_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after epoc 0 is 2.079260420798463\n",
      "Validation accuracy after epoc 0 is 0.215\n",
      "Validation loss after epoc 1 is 1.909625092312869\n",
      "Validation accuracy after epoc 1 is 0.27\n",
      "Validation loss after epoc 2 is 1.7640865599198259\n",
      "Validation accuracy after epoc 2 is 0.365\n",
      "Validation loss after epoc 3 is 1.634557228765213\n",
      "Validation accuracy after epoc 3 is 0.42\n",
      "Validation loss after epoc 4 is 1.5171469195430445\n",
      "Validation accuracy after epoc 4 is 0.52\n",
      "Validation loss after epoc 5 is 1.4101169272650833\n",
      "Validation accuracy after epoc 5 is 0.585\n",
      "Validation loss after epoc 6 is 1.312536235953399\n",
      "Validation accuracy after epoc 6 is 0.635\n",
      "Validation loss after epoc 7 is 1.2256804453228003\n",
      "Validation accuracy after epoc 7 is 0.65\n",
      "Validation loss after epoc 8 is 1.1477091911948916\n",
      "Validation accuracy after epoc 8 is 0.675\n",
      "Validation loss after epoc 9 is 1.0782539671163804\n",
      "Validation accuracy after epoc 9 is 0.7\n",
      "Validation loss after epoc 10 is 1.017509335513693\n",
      "Validation accuracy after epoc 10 is 0.73\n",
      "Validation loss after epoc 11 is 0.9632502143706287\n",
      "Validation accuracy after epoc 11 is 0.74\n",
      "Validation loss after epoc 12 is 0.915260833492482\n",
      "Validation accuracy after epoc 12 is 0.76\n",
      "Validation loss after epoc 13 is 0.8731181193339151\n",
      "Validation accuracy after epoc 13 is 0.77\n",
      "Validation loss after epoc 14 is 0.8352725305350199\n",
      "Validation accuracy after epoc 14 is 0.78\n",
      "Validation loss after epoc 15 is 0.8013094847372748\n",
      "Validation accuracy after epoc 15 is 0.785\n",
      "Validation loss after epoc 16 is 0.7708929392115408\n",
      "Validation accuracy after epoc 16 is 0.795\n",
      "Validation loss after epoc 17 is 0.743293925094298\n",
      "Validation accuracy after epoc 17 is 0.81\n",
      "Validation loss after epoc 18 is 0.7185266865186165\n",
      "Validation accuracy after epoc 18 is 0.82\n",
      "Validation loss after epoc 19 is 0.6959074091020807\n",
      "Validation accuracy after epoc 19 is 0.825\n",
      "Validation loss after epoc 20 is 0.67499070060354\n",
      "Validation accuracy after epoc 20 is 0.825\n",
      "Validation loss after epoc 21 is 0.6558463087609452\n",
      "Validation accuracy after epoc 21 is 0.83\n",
      "Validation loss after epoc 22 is 0.6382831659045746\n",
      "Validation accuracy after epoc 22 is 0.835\n",
      "Validation loss after epoc 23 is 0.622116747513736\n",
      "Validation accuracy after epoc 23 is 0.84\n",
      "Validation loss after epoc 24 is 0.6071960149389308\n",
      "Validation accuracy after epoc 24 is 0.84\n",
      "Validation loss after epoc 25 is 0.5934198616622859\n",
      "Validation accuracy after epoc 25 is 0.845\n",
      "Validation loss after epoc 26 is 0.5805426873849808\n",
      "Validation accuracy after epoc 26 is 0.845\n",
      "Validation loss after epoc 27 is 0.568348970914052\n",
      "Validation accuracy after epoc 27 is 0.845\n",
      "Validation loss after epoc 28 is 0.5570675293004118\n",
      "Validation accuracy after epoc 28 is 0.85\n",
      "Validation loss after epoc 29 is 0.5460851346405938\n",
      "Validation accuracy after epoc 29 is 0.85\n",
      "Validation loss after epoc 30 is 0.5358487833658206\n",
      "Validation accuracy after epoc 30 is 0.85\n",
      "Validation loss after epoc 31 is 0.5258937242926648\n",
      "Validation accuracy after epoc 31 is 0.855\n",
      "Validation loss after epoc 32 is 0.5166515154043239\n",
      "Validation accuracy after epoc 32 is 0.86\n",
      "Validation loss after epoc 33 is 0.5077314291306694\n",
      "Validation accuracy after epoc 33 is 0.86\n",
      "Validation loss after epoc 34 is 0.4995720264837186\n",
      "Validation accuracy after epoc 34 is 0.86\n",
      "Validation loss after epoc 35 is 0.4914153159111932\n",
      "Validation accuracy after epoc 35 is 0.865\n",
      "Validation loss after epoc 36 is 0.4838059270352264\n",
      "Validation accuracy after epoc 36 is 0.865\n",
      "Validation loss after epoc 37 is 0.47644038002793637\n",
      "Validation accuracy after epoc 37 is 0.865\n",
      "Validation loss after epoc 38 is 0.4694040021991304\n",
      "Validation accuracy after epoc 38 is 0.865\n",
      "Validation loss after epoc 39 is 0.4625353203097221\n",
      "Validation accuracy after epoc 39 is 0.865\n",
      "Validation loss after epoc 40 is 0.4561449645867732\n",
      "Validation accuracy after epoc 40 is 0.865\n",
      "Validation loss after epoc 41 is 0.44981076656905133\n",
      "Validation accuracy after epoc 41 is 0.87\n",
      "Validation loss after epoc 42 is 0.44379567264963554\n",
      "Validation accuracy after epoc 42 is 0.87\n",
      "Validation loss after epoc 43 is 0.4379039781667201\n",
      "Validation accuracy after epoc 43 is 0.875\n",
      "Validation loss after epoc 44 is 0.43225535480376026\n",
      "Validation accuracy after epoc 44 is 0.875\n",
      "Validation loss after epoc 45 is 0.4268679634648291\n",
      "Validation accuracy after epoc 45 is 0.875\n",
      "Validation loss after epoc 46 is 0.4217573943018448\n",
      "Validation accuracy after epoc 46 is 0.875\n",
      "Validation loss after epoc 47 is 0.41663151468971465\n",
      "Validation accuracy after epoc 47 is 0.875\n",
      "Validation loss after epoc 48 is 0.4115480585635209\n",
      "Validation accuracy after epoc 48 is 0.875\n",
      "Validation loss after epoc 49 is 0.40633784138622653\n",
      "Validation accuracy after epoc 49 is 0.875\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_losses = []\n",
    "validation_accuracy = []\n",
    "validation_index = [i for i in range(1,51)]\n",
    "for i in range(50):\n",
    "    for j in range(0, 6400, 32):\n",
    "        batch_x = mnist_subsample_x[j:j+32].reshape(32, 1, 28, 28).astype(np.float64)\n",
    "        batch_x /= 255.0\n",
    "        batch_y = mnist_subsample_y[j:j+32]\n",
    "        model_out = batch_x\n",
    "        # train\n",
    "        for layer in model:\n",
    "            model_out = layer.forward(model_out)\n",
    "\n",
    "        true_labels = label_binarizer.transform(batch_y)\n",
    "        l = loss_function(true_labels, model_out)\n",
    "\n",
    "        model_back = true_labels\n",
    "        for layer in reversed(model):\n",
    "            model_back = layer.backward(model_back)\n",
    "\n",
    "    #validation\n",
    "    validation_out = validation_batch\n",
    "    for layer in model:\n",
    "        validation_out = layer.forward(validation_out)\n",
    "    validation_loss = loss_function(validation_labels, validation_out)\n",
    "    validation_losses.append(validation_loss)\n",
    "    # print('Validation loss after epoc {} is {}'.format(i, validation_loss))\n",
    "    validation_predictions = predict_labels(validation_out)\n",
    "    accuracy = measure_accuracy(validation_labels, validation_predictions)\n",
    "    validation_accuracy.append(accuracy * 100)\n",
    "    # print('Validation accuracy after epoc {} is {}'.format(i, accuracy))\n",
    "\n",
    "plt.plot(validation_index, validation_losses)\n",
    "plt.show()\n",
    "plt.plot(validation_index, validation_accuracy)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# test_out = test_batch\n",
    "# for layer in model:\n",
    "#     test_out = layer.forward(test_out)\n",
    "# test_prediction = predict_labels(test_out)\n",
    "# accuracy = measure_accuracy(test_labels, test_prediction)\n",
    "# accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Toy Dataset Train-test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# label_binarizer_toy = LabelBinarizer()\n",
    "# label_binarizer_toy.fit(range(1,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# toy_model = [FullyConnectedLayerBatch(4), SoftmaxLayerBatch()]\n",
    "# x_train, y_train, x_test, y_test = process_toy_dataset()\n",
    "# x_validation, y_validation = x_test[:250], y_test[:250]\n",
    "# x_test, y_test = x_test[250:], y_test[250:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# y_validation = label_binarizer_toy.transform(y_validation)\n",
    "# y_test = label_binarizer_toy.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# validation_losses = []\n",
    "# validation_accuracy = []\n",
    "# validation_index = [i for i in range(1,30001)]\n",
    "# for i in range(30000):\n",
    "#     for j in range(0, 500, 25):\n",
    "#         batch_x = x_train[j:j+25]\n",
    "#         batch_y = y_train[j:j+25]\n",
    "#         model_out = batch_x\n",
    "#         # train\n",
    "#         for layer in toy_model:\n",
    "#             model_out = layer.forward(model_out)\n",
    "#\n",
    "#         true_labels = label_binarizer_toy.transform(batch_y)\n",
    "#         l = loss_function(true_labels, model_out)\n",
    "#\n",
    "#         model_back_toy = true_labels\n",
    "#         for layer in reversed(toy_model):\n",
    "#             model_back_toy = layer.backward(model_back_toy)\n",
    "#\n",
    "#     #validation\n",
    "#     validation_out = x_validation\n",
    "#     for layer in toy_model:\n",
    "#         validation_out = layer.forward(validation_out)\n",
    "#     validation_loss = loss_function(y_validation, validation_out)\n",
    "#     validation_losses.append(validation_loss)\n",
    "#     validation_predictions = predict_labels(validation_out)\n",
    "#     accuracy = measure_accuracy(y_validation, validation_predictions)\n",
    "#     validation_accuracy.append(accuracy*100)\n",
    "#\n",
    "# # print(validation_losses)\n",
    "# plt.plot(validation_index, validation_losses)\n",
    "# plt.show()\n",
    "# plt.plot(validation_index, validation_accuracy)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# test_out = x_test\n",
    "# for layer in toy_model:\n",
    "#     test_out = layer.forward(test_out)\n",
    "# test_prediction = predict_labels(test_out)\n",
    "# accuracy = measure_accuracy(y_test, test_prediction)\n",
    "# accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}