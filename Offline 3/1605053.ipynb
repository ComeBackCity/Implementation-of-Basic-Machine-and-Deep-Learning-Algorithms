{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0) * 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * np.sqrt(2/input_shape[1] * self.filter_dimension ** 2)\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.output_channel_count)\n",
    "\n",
    "        # print('Convolution layer')\n",
    "        # print(self.filters[0, 0, :, :])\n",
    "        # print(self.bias)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 10e-4) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dout_padded = np.zeros(padded_dimensions)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension]\n",
    "                    for f in range(self.output_channel_count):\n",
    "                        dw[f, :, :, :] += np.sum(dz[i, f, out_x, out_y] * image_slice, axis=0)\n",
    "                        dout_padded[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension] += dz[i, f, out_x, out_y] * self.filters[f, :, :, :]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        self.filters -= learning_rate * dw / batch_size\n",
    "        return dout_padded[:, :, self.padding:padded_dimensions[2]-self.padding, self.padding:padded_dimensions[3]-self.padding]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.84 ms ± 924 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.       , 0.       , 0.       ],\n       [0.       , 0.4777692, 0.       ],\n       [0.       , 0.       , 0.       ]])"
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(input_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.       , 0.       , 0.       ],\n       [0.       , 0.4777692, 0.       ],\n       [0.       , 0.       , 0.       ]])"
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    max_val = np.max(image_slice, axis=(1, 2))\n",
    "                    output[i, :, out_x, out_y] = max_val\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == max_val.reshape(input_dimensions[1], 1, 1)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.74 ms ± 13.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.4777692, 0.4777692],\n       [0.4777692, 0.4777692]])"
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 ,\n        0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 ,\n        0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 , 0.4777692 ,\n        0.4777692 ],\n       [0.95247325, 0.95247325, 0.95247325, 0.95247325, 0.95247325,\n        0.95247325, 0.95247325, 0.95247325, 0.95247325, 0.95247325,\n        0.95247325, 0.95247325, 0.95247325, 0.95247325, 0.95247325,\n        0.95247325],\n       [1.90110531, 1.90110531, 1.90110531, 1.90110531, 1.90110531,\n        1.90110531, 1.90110531, 1.90110531, 1.90110531, 1.90110531,\n        1.90110531, 1.90110531, 1.90110531, 1.90110531, 1.90110531,\n        1.90110531],\n       [1.42937845, 1.42937845, 1.42937845, 1.42937845, 1.42937845,\n        1.42937845, 1.42937845, 1.42937845, 1.42937845, 1.42937845,\n        1.42937845, 1.42937845, 1.42937845, 1.42937845, 1.42937845,\n        1.42937845],\n       [1.90503461, 1.90503461, 1.90503461, 1.90503461, 1.90503461,\n        1.90503461, 1.90503461, 1.90503461, 1.90503461, 1.90503461,\n        1.90503461, 1.90503461, 1.90503461, 1.90503461, 1.90503461,\n        1.90503461],\n       [0.95251388, 0.95251388, 0.95251388, 0.95251388, 0.95251388,\n        0.95251388, 0.95251388, 0.95251388, 0.95251388, 0.95251388,\n        0.95251388, 0.95251388, 0.95251388, 0.95251388, 0.95251388,\n        0.95251388],\n       [0.47103298, 0.47103298, 0.47103298, 0.47103298, 0.47103298,\n        0.47103298, 0.47103298, 0.47103298, 0.47103298, 0.47103298,\n        0.47103298, 0.47103298, 0.47103298, 0.47103298, 0.47103298,\n        0.47103298],\n       [1.43388051, 1.43388051, 1.43388051, 1.43388051, 1.43388051,\n        1.43388051, 1.43388051, 1.43388051, 1.43388051, 1.43388051,\n        1.43388051, 1.43388051, 1.43388051, 1.43388051, 1.43388051,\n        1.43388051],\n       [0.47472288, 0.47472288, 0.47472288, 0.47472288, 0.47472288,\n        0.47472288, 0.47472288, 0.47472288, 0.47472288, 0.47472288,\n        0.47472288, 0.47472288, 0.47472288, 0.47472288, 0.47472288,\n        0.47472288],\n       [1.41343535, 1.41343535, 1.41343535, 1.41343535, 1.41343535,\n        1.41343535, 1.41343535, 1.41343535, 1.41343535, 1.41343535,\n        1.41343535, 1.41343535, 1.41343535, 1.41343535, 1.41343535,\n        1.41343535],\n       [1.42481284, 1.42481284, 1.42481284, 1.42481284, 1.42481284,\n        1.42481284, 1.42481284, 1.42481284, 1.42481284, 1.42481284,\n        1.42481284, 1.42481284, 1.42481284, 1.42481284, 1.42481284,\n        1.42481284],\n       [1.45001068, 1.45001068, 1.45001068, 1.45001068, 1.45001068,\n        1.45001068, 1.45001068, 1.45001068, 1.45001068, 1.45001068,\n        1.45001068, 1.45001068, 1.45001068, 1.45001068, 1.45001068,\n        1.45001068],\n       [1.44937555, 1.44937555, 1.44937555, 1.44937555, 1.44937555,\n        1.44937555, 1.44937555, 1.44937555, 1.44937555, 1.44937555,\n        1.44937555, 1.44937555, 1.44937555, 1.44937555, 1.44937555,\n        1.44937555],\n       [0.92475104, 0.92475104, 0.92475104, 0.92475104, 0.92475104,\n        0.92475104, 0.92475104, 0.92475104, 0.92475104, 0.92475104,\n        0.92475104, 0.92475104, 0.92475104, 0.92475104, 0.92475104,\n        0.92475104],\n       [0.44660692, 0.44660692, 0.44660692, 0.44660692, 0.44660692,\n        0.44660692, 0.44660692, 0.44660692, 0.44660692, 0.44660692,\n        0.44660692, 0.44660692, 0.44660692, 0.44660692, 0.44660692,\n        0.44660692],\n       [0.96526399, 0.96526399, 0.96526399, 0.96526399, 0.96526399,\n        0.96526399, 0.96526399, 0.96526399, 0.96526399, 0.96526399,\n        0.96526399, 0.96526399, 0.96526399, 0.96526399, 0.96526399,\n        0.96526399],\n       [0.49517698, 0.49517698, 0.49517698, 0.49517698, 0.49517698,\n        0.49517698, 0.49517698, 0.49517698, 0.49517698, 0.49517698,\n        0.49517698, 0.49517698, 0.49517698, 0.49517698, 0.49517698,\n        0.49517698],\n       [1.43460985, 1.43460985, 1.43460985, 1.43460985, 1.43460985,\n        1.43460985, 1.43460985, 1.43460985, 1.43460985, 1.43460985,\n        1.43460985, 1.43460985, 1.43460985, 1.43460985, 1.43460985,\n        1.43460985],\n       [1.90846466, 1.90846466, 1.90846466, 1.90846466, 1.90846466,\n        1.90846466, 1.90846466, 1.90846466, 1.90846466, 1.90846466,\n        1.90846466, 1.90846466, 1.90846466, 1.90846466, 1.90846466,\n        1.90846466],\n       [0.46293936, 0.46293936, 0.46293936, 0.46293936, 0.46293936,\n        0.46293936, 0.46293936, 0.46293936, 0.46293936, 0.46293936,\n        0.46293936, 0.46293936, 0.46293936, 0.46293936, 0.46293936,\n        0.46293936],\n       [0.48006658, 0.48006658, 0.48006658, 0.48006658, 0.48006658,\n        0.48006658, 0.48006658, 0.48006658, 0.48006658, 0.48006658,\n        0.48006658, 0.48006658, 0.48006658, 0.48006658, 0.48006658,\n        0.48006658],\n       [1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 ,\n        1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 ,\n        1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 , 1.9112458 ,\n        1.9112458 ],\n       [0.48268792, 0.48268792, 0.48268792, 0.48268792, 0.48268792,\n        0.48268792, 0.48268792, 0.48268792, 0.48268792, 0.48268792,\n        0.48268792, 0.48268792, 0.48268792, 0.48268792, 0.48268792,\n        0.48268792],\n       [0.48050306, 0.48050306, 0.48050306, 0.48050306, 0.48050306,\n        0.48050306, 0.48050306, 0.48050306, 0.48050306, 0.48050306,\n        0.48050306, 0.48050306, 0.48050306, 0.48050306, 0.48050306,\n        0.48050306],\n       [1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 ,\n        1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 ,\n        1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 , 1.8935999 ,\n        1.8935999 ],\n       [0.49026492, 0.49026492, 0.49026492, 0.49026492, 0.49026492,\n        0.49026492, 0.49026492, 0.49026492, 0.49026492, 0.49026492,\n        0.49026492, 0.49026492, 0.49026492, 0.49026492, 0.49026492,\n        0.49026492],\n       [1.90941193, 1.90941193, 1.90941193, 1.90941193, 1.90941193,\n        1.90941193, 1.90941193, 1.90941193, 1.90941193, 1.90941193,\n        1.90941193, 1.90941193, 1.90941193, 1.90941193, 1.90941193,\n        1.90941193],\n       [0.46894939, 0.46894939, 0.46894939, 0.46894939, 0.46894939,\n        0.46894939, 0.46894939, 0.46894939, 0.46894939, 0.46894939,\n        0.46894939, 0.46894939, 0.46894939, 0.46894939, 0.46894939,\n        0.46894939],\n       [0.95093824, 0.95093824, 0.95093824, 0.95093824, 0.95093824,\n        0.95093824, 0.95093824, 0.95093824, 0.95093824, 0.95093824,\n        0.95093824, 0.95093824, 0.95093824, 0.95093824, 0.95093824,\n        0.95093824],\n       [1.44856704, 1.44856704, 1.44856704, 1.44856704, 1.44856704,\n        1.44856704, 1.44856704, 1.44856704, 1.44856704, 1.44856704,\n        1.44856704, 1.44856704, 1.44856704, 1.44856704, 1.44856704,\n        1.44856704],\n       [1.44390473, 1.44390473, 1.44390473, 1.44390473, 1.44390473,\n        1.44390473, 1.44390473, 1.44390473, 1.44390473, 1.44390473,\n        1.44390473, 1.44390473, 1.44390473, 1.44390473, 1.44390473,\n        1.44390473],\n       [1.44106375, 1.44106375, 1.44106375, 1.44106375, 1.44106375,\n        1.44106375, 1.44106375, 1.44106375, 1.44106375, 1.44106375,\n        1.44106375, 1.44106375, 1.44106375, 1.44106375, 1.44106375,\n        1.44106375],\n       [1.44036166, 1.44036166, 1.44036166, 1.44036166, 1.44036166,\n        1.44036166, 1.44036166, 1.44036166, 1.44036166, 1.44036166,\n        1.44036166, 1.44036166, 1.44036166, 1.44036166, 1.44036166,\n        1.44036166],\n       [0.96931311, 0.96931311, 0.96931311, 0.96931311, 0.96931311,\n        0.96931311, 0.96931311, 0.96931311, 0.96931311, 0.96931311,\n        0.96931311, 0.96931311, 0.96931311, 0.96931311, 0.96931311,\n        0.96931311],\n       [0.46325287, 0.46325287, 0.46325287, 0.46325287, 0.46325287,\n        0.46325287, 0.46325287, 0.46325287, 0.46325287, 0.46325287,\n        0.46325287, 0.46325287, 0.46325287, 0.46325287, 0.46325287,\n        0.46325287],\n       [1.92101125, 1.92101125, 1.92101125, 1.92101125, 1.92101125,\n        1.92101125, 1.92101125, 1.92101125, 1.92101125, 1.92101125,\n        1.92101125, 1.92101125, 1.92101125, 1.92101125, 1.92101125,\n        1.92101125],\n       [0.47607298, 0.47607298, 0.47607298, 0.47607298, 0.47607298,\n        0.47607298, 0.47607298, 0.47607298, 0.47607298, 0.47607298,\n        0.47607298, 0.47607298, 0.47607298, 0.47607298, 0.47607298,\n        0.47607298],\n       [1.44236624, 1.44236624, 1.44236624, 1.44236624, 1.44236624,\n        1.44236624, 1.44236624, 1.44236624, 1.44236624, 1.44236624,\n        1.44236624, 1.44236624, 1.44236624, 1.44236624, 1.44236624,\n        1.44236624],\n       [1.93275733, 1.93275733, 1.93275733, 1.93275733, 1.93275733,\n        1.93275733, 1.93275733, 1.93275733, 1.93275733, 1.93275733,\n        1.93275733, 1.93275733, 1.93275733, 1.93275733, 1.93275733,\n        1.93275733],\n       [0.49435448, 0.49435448, 0.49435448, 0.49435448, 0.49435448,\n        0.49435448, 0.49435448, 0.49435448, 0.49435448, 0.49435448,\n        0.49435448, 0.49435448, 0.49435448, 0.49435448, 0.49435448,\n        0.49435448],\n       [1.91410395, 1.91410395, 1.91410395, 1.91410395, 1.91410395,\n        1.91410395, 1.91410395, 1.91410395, 1.91410395, 1.91410395,\n        1.91410395, 1.91410395, 1.91410395, 1.91410395, 1.91410395,\n        1.91410395],\n       [0.48472684, 0.48472684, 0.48472684, 0.48472684, 0.48472684,\n        0.48472684, 0.48472684, 0.48472684, 0.48472684, 0.48472684,\n        0.48472684, 0.48472684, 0.48472684, 0.48472684, 0.48472684,\n        0.48472684],\n       [1.89572016, 1.89572016, 1.89572016, 1.89572016, 1.89572016,\n        1.89572016, 1.89572016, 1.89572016, 1.89572016, 1.89572016,\n        1.89572016, 1.89572016, 1.89572016, 1.89572016, 1.89572016,\n        1.89572016],\n       [1.91816156, 1.91816156, 1.91816156, 1.91816156, 1.91816156,\n        1.91816156, 1.91816156, 1.91816156, 1.91816156, 1.91816156,\n        1.91816156, 1.91816156, 1.91816156, 1.91816156, 1.91816156,\n        1.91816156],\n       [0.49236632, 0.49236632, 0.49236632, 0.49236632, 0.49236632,\n        0.49236632, 0.49236632, 0.49236632, 0.49236632, 0.49236632,\n        0.49236632, 0.49236632, 0.49236632, 0.49236632, 0.49236632,\n        0.49236632],\n       [1.43420526, 1.43420526, 1.43420526, 1.43420526, 1.43420526,\n        1.43420526, 1.43420526, 1.43420526, 1.43420526, 1.43420526,\n        1.43420526, 1.43420526, 1.43420526, 1.43420526, 1.43420526,\n        1.43420526],\n       [1.91439439, 1.91439439, 1.91439439, 1.91439439, 1.91439439,\n        1.91439439, 1.91439439, 1.91439439, 1.91439439, 1.91439439,\n        1.91439439, 1.91439439, 1.91439439, 1.91439439, 1.91439439,\n        1.91439439],\n       [0.94925996, 0.94925996, 0.94925996, 0.94925996, 0.94925996,\n        0.94925996, 0.94925996, 0.94925996, 0.94925996, 0.94925996,\n        0.94925996, 0.94925996, 0.94925996, 0.94925996, 0.94925996,\n        0.94925996],\n       [1.90495423, 1.90495423, 1.90495423, 1.90495423, 1.90495423,\n        1.90495423, 1.90495423, 1.90495423, 1.90495423, 1.90495423,\n        1.90495423, 1.90495423, 1.90495423, 1.90495423, 1.90495423,\n        1.90495423],\n       [1.42175942, 1.42175942, 1.42175942, 1.42175942, 1.42175942,\n        1.42175942, 1.42175942, 1.42175942, 1.42175942, 1.42175942,\n        1.42175942, 1.42175942, 1.42175942, 1.42175942, 1.42175942,\n        1.42175942]])"
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * np.sqrt(2/flattened_input.shape[1])\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros((1, self.output_dimension))\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        # print('Fully connected layer')\n",
    "        # print(self.weights)\n",
    "        # print(self.bias)\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 10e-4) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.05040316, -0.30242   ,  0.2740126 , -0.05719638],\n       [ 2.09406739, -0.60289981,  0.54626725, -0.11402582],\n       [ 4.17968971, -1.20336822,  1.09033149, -0.22759178],\n       [ 3.14257099, -0.90477292,  0.81978432, -0.17111876],\n       [ 4.18832849, -1.2058554 ,  1.09258504, -0.22806218],\n       [ 2.09415671, -0.60292553,  0.54629055, -0.11403068],\n       [ 1.03559318, -0.29815608,  0.2701492 , -0.05638995],\n       [ 3.15246903, -0.90762265,  0.82236637, -0.17165773],\n       [ 1.04370563, -0.30049173,  0.27226545, -0.05683169],\n       [ 3.10751917, -0.8946812 ,  0.81064056, -0.16921013],\n       [ 3.13253324, -0.90188297,  0.81716583, -0.17057219],\n       [ 3.18793215, -0.91783278,  0.83161743, -0.17358876],\n       [ 3.18653578, -0.91743076,  0.83125316, -0.17351273],\n       [ 2.03311852, -0.58535212,  0.53036787, -0.11070704],\n       [ 0.98189109, -0.2826948 ,  0.25614025, -0.05346577],\n       [ 2.12218857, -0.61099614,  0.55360306, -0.11555707],\n       [ 1.08867515, -0.31343884,  0.28399639, -0.05928036],\n       [ 3.15407253, -0.90808431,  0.82278466, -0.17174504],\n       [ 4.19586968, -1.20802657,  1.09455226, -0.22847281],\n       [ 1.0177989 , -0.29303296,  0.26550731, -0.05542102],\n       [ 1.05545407, -0.3038742 ,  0.2753302 , -0.05747141],\n       [ 4.20198415, -1.20978698,  1.09614731, -0.22880576],\n       [ 1.06121725, -0.30553347,  0.27683361, -0.05778523],\n       [ 1.0564137 , -0.30415049,  0.27558053, -0.05752367],\n       [ 4.16318863, -1.19861742,  1.08602694, -0.22669327],\n       [ 1.07787571, -0.31032958,  0.2811792 , -0.05869231],\n       [ 4.1979523 , -1.20862618,  1.09509554, -0.22858622],\n       [ 1.03101228, -0.29683721,  0.26895421, -0.05614051],\n       [ 2.09069258, -0.60192818,  0.54538688, -0.11384205],\n       [ 3.18475821, -0.91691898,  0.83078946, -0.17341594],\n       [ 3.17450785, -0.91396781,  0.82811551, -0.17285778],\n       [ 3.1682618 , -0.91216952,  0.82648614, -0.17251768],\n       [ 3.16671821, -0.91172511,  0.82608347, -0.17243362],\n       [ 2.13109079, -0.61355917,  0.55592533, -0.11604181],\n       [ 1.01848817, -0.29323141,  0.26568712, -0.05545855],\n       [ 4.22345407, -1.21596835,  1.10174804, -0.22997483],\n       [ 1.0466739 , -0.30134632,  0.27303977, -0.05699332],\n       [ 3.17112539, -0.91299397,  0.82723314, -0.1726736 ],\n       [ 4.2492785 , -1.22340342,  1.10848471, -0.23138102],\n       [ 1.08686685, -0.31291821,  0.28352467, -0.0591819 ],\n       [ 4.20826798, -1.21159615,  1.09778653, -0.22914792],\n       [ 1.06569992, -0.30682407,  0.27800298, -0.05802932],\n       [ 4.16785015, -1.19995951,  1.08724297, -0.2269471 ],\n       [ 4.21718887, -1.21416455,  1.10011367, -0.22963368],\n       [ 1.08249576, -0.31165974,  0.28238441, -0.05894388],\n       [ 3.15318301, -0.90782821,  0.82255262, -0.17169661],\n       [ 4.20890652, -1.21177999,  1.09795311, -0.22918269],\n       [ 2.08700279, -0.60086586,  0.54442435, -0.11364114],\n       [ 4.18815178, -1.20580452,  1.09253894, -0.22805256],\n       [ 3.12582011, -0.8999502 ,  0.81541462, -0.17020665]])"
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1).reshape(-1, 1)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.48806105, 0.12616838, 0.22453912, 0.16123146],\n       [0.71940631, 0.04849492, 0.1530288 , 0.07906996],\n       [0.94134198, 0.00432429, 0.0428604 , 0.01147333],\n       [0.86816822, 0.01516578, 0.08508058, 0.03158542],\n       [0.94174193, 0.00427826, 0.04260569, 0.01137412],\n       [0.71942295, 0.04849047, 0.15302224, 0.07906435],\n       [0.48445733, 0.12764846, 0.22533437, 0.16255985],\n       [0.86915925, 0.01499077, 0.08455682, 0.03129316],\n       [0.4864315 , 0.12683648, 0.22490001, 0.161832  ],\n       [0.86460375, 0.01580096, 0.08695517, 0.03264011],\n       [0.86715624, 0.0153452 , 0.08561425, 0.0318843 ],\n       [0.87265464, 0.01437909, 0.08270046, 0.03026582],\n       [0.87251863, 0.01440273, 0.08277296, 0.03030569],\n       [0.70789587, 0.05161572, 0.15751904, 0.08296936],\n       [0.47138307, 0.13309793, 0.22813105, 0.16738796],\n       [0.72461124, 0.04710836, 0.15096527, 0.07731513],\n       [0.49736581, 0.12238989, 0.22243769, 0.15780661],\n       [0.86931916, 0.0149626 , 0.0844722 , 0.03124604],\n       [0.94208886, 0.00423847, 0.04238448, 0.01128819],\n       [0.48012594, 0.12943989, 0.22627631, 0.16415786],\n       [0.48928976, 0.12566587, 0.22426559, 0.16077878],\n       [0.94236866, 0.00420648, 0.04220589, 0.01121898],\n       [0.49069151, 0.12509393, 0.22395205, 0.16026251],\n       [0.48952318, 0.12557053, 0.22421348, 0.1606928 ],\n       [0.94057052, 0.00441356, 0.04335079, 0.01166514],\n       [0.49474163, 0.12344928, 0.22303733, 0.15877175],\n       [0.94218431, 0.00422755, 0.04232357, 0.01126457],\n       [0.4833424 , 0.12810828, 0.22557828, 0.16297104],\n       [0.71877715, 0.04866357, 0.15327684, 0.07928245],\n       [0.8723453 , 0.01443287, 0.08286531, 0.03035652],\n       [0.87134161, 0.01460784, 0.08339945, 0.0306511 ],\n       [0.87072649, 0.01471543, 0.08372622, 0.03083186],\n       [0.87057406, 0.01474213, 0.08380713, 0.03087667],\n       [0.7262449 , 0.04667634, 0.15031331, 0.07676545],\n       [0.48029374, 0.12937023, 0.2262401 , 0.16409592],\n       [0.94334062, 0.00409598, 0.04158425, 0.01097916],\n       [0.48715374, 0.12654013, 0.22474032, 0.16156581],\n       [0.87100883, 0.01466601, 0.08357629, 0.03074887],\n       [0.94448831, 0.00396681, 0.04084766, 0.01069722],\n       [0.49692649, 0.12256691, 0.22253846, 0.15796814],\n       [0.94265482, 0.00417384, 0.04202307, 0.01114827],\n       [0.49178161, 0.12465012, 0.22370714, 0.15986113],\n       [0.94078946, 0.00438816, 0.04321174, 0.01161064],\n       [0.94305867, 0.00412793, 0.04176478, 0.01104862],\n       [0.49586442, 0.12299541, 0.22278144, 0.15835873],\n       [0.86923048, 0.01497822, 0.08451913, 0.03127217],\n       [0.94268382, 0.00417054, 0.04200453, 0.01114111],\n       [0.71808817, 0.0488485 , 0.15354811, 0.07951522],\n       [0.94173377, 0.0042792 , 0.04261089, 0.01137614],\n       [0.86647553, 0.0154663 , 0.08597258, 0.03208559]])"
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [
    {
     "data": {
      "text/plain": "2.5125415101218196"
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.51193895,  0.12616838,  0.22453912,  0.16123146],\n       [ 0.71940631, -0.95150508,  0.1530288 ,  0.07906996],\n       [ 0.94134198,  0.00432429,  0.0428604 , -0.98852667],\n       [ 0.86816822,  0.01516578, -0.91491942,  0.03158542],\n       [ 0.94174193,  0.00427826,  0.04260569, -0.98862588],\n       [ 0.71942295, -0.95150953,  0.15302224,  0.07906435],\n       [-0.51554267,  0.12764846,  0.22533437,  0.16255985],\n       [ 0.86915925,  0.01499077, -0.91544318,  0.03129316],\n       [-0.5135685 ,  0.12683648,  0.22490001,  0.161832  ],\n       [ 0.86460375,  0.01580096, -0.91304483,  0.03264011],\n       [ 0.86715624,  0.0153452 , -0.91438575,  0.0318843 ],\n       [ 0.87265464,  0.01437909, -0.91729954,  0.03026582],\n       [ 0.87251863,  0.01440273, -0.91722704,  0.03030569],\n       [ 0.70789587, -0.94838428,  0.15751904,  0.08296936],\n       [-0.52861693,  0.13309793,  0.22813105,  0.16738796],\n       [ 0.72461124, -0.95289164,  0.15096527,  0.07731513],\n       [-0.50263419,  0.12238989,  0.22243769,  0.15780661],\n       [ 0.86931916,  0.0149626 , -0.9155278 ,  0.03124604],\n       [ 0.94208886,  0.00423847,  0.04238448, -0.98871181],\n       [-0.51987406,  0.12943989,  0.22627631,  0.16415786],\n       [-0.51071024,  0.12566587,  0.22426559,  0.16077878],\n       [ 0.94236866,  0.00420648,  0.04220589, -0.98878102],\n       [-0.50930849,  0.12509393,  0.22395205,  0.16026251],\n       [-0.51047682,  0.12557053,  0.22421348,  0.1606928 ],\n       [ 0.94057052,  0.00441356,  0.04335079, -0.98833486],\n       [-0.50525837,  0.12344928,  0.22303733,  0.15877175],\n       [ 0.94218431,  0.00422755,  0.04232357, -0.98873543],\n       [-0.5166576 ,  0.12810828,  0.22557828,  0.16297104],\n       [ 0.71877715, -0.95133643,  0.15327684,  0.07928245],\n       [ 0.8723453 ,  0.01443287, -0.91713469,  0.03035652],\n       [ 0.87134161,  0.01460784, -0.91660055,  0.0306511 ],\n       [ 0.87072649,  0.01471543, -0.91627378,  0.03083186],\n       [ 0.87057406,  0.01474213, -0.91619287,  0.03087667],\n       [ 0.7262449 , -0.95332366,  0.15031331,  0.07676545],\n       [-0.51970626,  0.12937023,  0.2262401 ,  0.16409592],\n       [ 0.94334062,  0.00409598,  0.04158425, -0.98902084],\n       [-0.51284626,  0.12654013,  0.22474032,  0.16156581],\n       [ 0.87100883,  0.01466601, -0.91642371,  0.03074887],\n       [ 0.94448831,  0.00396681,  0.04084766, -0.98930278],\n       [-0.50307351,  0.12256691,  0.22253846,  0.15796814],\n       [ 0.94265482,  0.00417384,  0.04202307, -0.98885173],\n       [-0.50821839,  0.12465012,  0.22370714,  0.15986113],\n       [ 0.94078946,  0.00438816,  0.04321174, -0.98838936],\n       [ 0.94305867,  0.00412793,  0.04176478, -0.98895138],\n       [-0.50413558,  0.12299541,  0.22278144,  0.15835873],\n       [ 0.86923048,  0.01497822, -0.91548087,  0.03127217],\n       [ 0.94268382,  0.00417054,  0.04200453, -0.98885889],\n       [ 0.71808817, -0.9511515 ,  0.15354811,  0.07951522],\n       [ 0.94173377,  0.0042792 ,  0.04261089, -0.98862386],\n       [ 0.86647553,  0.0154663 , -0.91402742,  0.03208559]])"
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.54541468, -0.22530726,  0.11916643,  0.11937809,  0.02749272,\n        -0.10364757, -0.21077176, -0.30562165, -0.28307583,  0.16720129,\n         0.165416  ,  0.22858148,  0.0613064 , -0.12577052, -0.15981975,\n        -0.02502631],\n       [ 0.79795372, -0.38422446, -0.39925442, -0.12206387, -0.11103235,\n        -0.20732199,  0.44618197,  0.48036052,  0.9541719 , -0.17136207,\n         0.08747484,  0.19790003, -0.29055962,  0.58249758,  0.13442857,\n         0.26709286],\n       [ 1.11266132,  0.52425798,  0.36659352,  0.04973475, -0.56013883,\n        -0.56077786,  0.8627288 ,  0.8518102 ,  0.56945455, -0.4515193 ,\n         0.15519647, -0.50926064,  0.14700561, -0.24111078, -0.23808422,\n         0.13122937],\n       [ 0.83676597,  0.65816086, -0.4736025 , -0.40300869,  0.32596363,\n         0.8476444 , -0.03102601,  0.3015393 ,  0.17090051, -0.22003528,\n        -0.74309293, -0.58717792, -0.17380814,  0.32338918,  0.66722909,\n        -0.12923371],\n       [ 1.11307581,  0.5244898 ,  0.36647531,  0.04961592, -0.56011322,\n        -0.56060479,  0.86284196,  0.85202129,  0.56962059, -0.45164435,\n         0.15500135, -0.50947995,  0.14695593, -0.24101241, -0.23790887,\n         0.13122081],\n       [ 0.79797157, -0.38421755, -0.39925796, -0.12206748, -0.11103374,\n        -0.20731953,  0.44618934,  0.48037074,  0.95418154, -0.17136758,\n         0.0874701 ,  0.1978929 , -0.29056154,  0.58250155,  0.13443323,\n         0.26709392],\n       [-0.54936329, -0.22628328,  0.11981518,  0.11998149,  0.02812087,\n        -0.10349449, -0.21274174, -0.30802976, -0.28564995,  0.16841858,\n         0.16593848,  0.22974406,  0.06175348, -0.12671749, -0.16047676,\n        -0.02549109],\n       [ 0.83780958,  0.65866789, -0.47385305, -0.4032662 ,  0.32595879,\n         0.84794431, -0.03067246,  0.30210232,  0.17138151, -0.22035482,\n        -0.74348565, -0.58767231, -0.17392409,  0.32362384,  0.66759064,\n        -0.1292183 ],\n       [-0.54720002, -0.22574975,  0.11945995,  0.11965129,  0.02777613,\n        -0.10357973, -0.21166182, -0.30671018, -0.28423875,  0.1677517 ,\n         0.16565329,  0.22910803,  0.06150845, -0.12619846, -0.16011753,\n        -0.02523597],\n       [ 0.83301102,  0.65634342, -0.47270462, -0.4020856 ,  0.32598672,\n         0.84657657, -0.03230376,  0.29951097,  0.16916429, -0.21888527,\n        -0.74168813, -0.58540422, -0.17339148,  0.32254556,  0.66593479,\n        -0.12929237],\n       [ 0.83570014,  0.6576439 , -0.47334707, -0.40274614,  0.32596929,\n         0.84733951, -0.0313878 ,  0.30096397,  0.17040858, -0.2197089 ,\n        -0.74269287, -0.58667364, -0.17368979,  0.32314962,  0.66686067,\n        -0.12924986],\n       [ 0.84148905,  0.6604622 , -0.47473992, -0.40417742,  0.32594726,\n         0.8490128 , -0.02943149,  0.30408484,  0.1730719 , -0.22148112,\n        -0.74487838, -0.58942046, -0.17433346,  0.32445185,  0.66887174,\n        -0.12916709],\n       [ 0.84134592,  0.6603922 , -0.47470532, -0.40414187,  0.32594754,\n         0.84897091, -0.0294796 ,  0.30400779,  0.17300631, -0.22143731,\n        -0.74482397, -0.58935231, -0.17431752,  0.32441962,  0.66882172,\n        -0.12916899],\n       [ 0.78560201, -0.38896607, -0.39682237, -0.11957929, -0.11004453,\n        -0.20896851,  0.44105176,  0.47327319,  0.94746192, -0.1675456 ,\n         0.09071723,  0.20280416, -0.28923356,  0.5797458 ,  0.1312293 ,\n         0.26634441],\n       [-0.56370008, -0.22974509,  0.12215813,  0.1221469 ,  0.03044294,\n        -0.10284507, -0.21993925, -0.31679211, -0.29506082,  0.1728367 ,\n         0.16776369,  0.23390376,  0.06338327, -0.13017147, -0.16281352,\n        -0.02721165],\n       [ 0.80353444, -0.38205488, -0.40036243, -0.12319743, -0.11146059,\n        -0.20654014,  0.44848122,  0.48355447,  0.95718175, -0.17308609,\n         0.08598157,  0.19566379, -0.29115873,  0.58373957,  0.1358951 ,\n         0.26741919],\n       [-0.53522556, -0.22274437,  0.11748532,  0.11780715,  0.02589445,\n        -0.10399157, -0.20571281, -0.29941801, -0.2764685 ,  0.16405932,\n         0.16402865,  0.22554826,  0.06015616, -0.1233352 , -0.15809776,\n        -0.02384492],\n       [ 0.83797796,  0.65874977, -0.47389351, -0.40330778,  0.32595807,\n         0.84799283, -0.03061548,  0.30219313,  0.17145905, -0.22040638,\n        -0.74354911, -0.58775214, -0.17394281,  0.32366171,  0.66764904,\n        -0.12921585],\n       [ 1.11343532,  0.52469105,  0.36637264,  0.04951275, -0.56009083,\n        -0.56045434,  0.86293993,  0.8522043 ,  0.56976445, -0.4517528 ,\n         0.15483187, -0.50967031,  0.14691282, -0.24092705, -0.23775658,\n         0.13121329],\n       [-0.55411098, -0.22744397,  0.12059322,  0.120703  ,  0.02888263,\n        -0.10329574, -0.21511743, -0.31092817, -0.28875512,  0.16988195,\n         0.16655542,  0.23113229,  0.06229204, -0.12785853, -0.16125907,\n        -0.0260551 ],\n       [-0.54406868, -0.22497235,  0.11894494,  0.11917171,  0.02727972,\n        -0.10369722, -0.21010145, -0.30480129, -0.28220011,  0.1667863 ,\n         0.16523596,  0.22818352,  0.06115417, -0.12544813, -0.15959447,\n        -0.02486877],\n       [ 1.11372523,  0.52485347,  0.36628977,  0.04942948, -0.56007266,\n        -0.56033277,  0.86301883,  0.85235182,  0.56988037, -0.45184025,\n         0.15469502, -0.50982391,  0.14687803, -0.24085818, -0.23763362,\n         0.13120717],\n       [-0.54253332, -0.22458896,  0.11869207,  0.11893587,  0.02703745,\n        -0.10375229, -0.20933758, -0.30386583, -0.28120226,  0.1663129 ,\n         0.16502938,  0.22772856,  0.06098063, -0.12508064, -0.15933668,\n        -0.02468962],\n       [-0.54381299, -0.2249086 ,  0.11890285,  0.11913247,  0.02723932,\n        -0.10370651, -0.20997418, -0.30464548, -0.28203385,  0.16670747,\n         0.16520164,  0.22810783,  0.06112526, -0.12538691, -0.1595516 ,\n        -0.0248389 ],\n       [ 1.11186165,  0.52381135,  0.36682114,  0.04996363, -0.56018762,\n        -0.56111057,  0.86250987,  0.85140268,  0.56913369, -0.45127799,\n         0.15557206, -0.50883798,  0.14710135, -0.2413004 , -0.23842182,\n         0.13124558],\n       [-0.53809825, -0.22347338,  0.11796033,  0.11825208,  0.02634177,\n        -0.103902  , -0.20713556, -0.30116554, -0.27832627,  0.16494528,\n         0.16442548,  0.22640826,  0.06047997, -0.12402062, -0.15858713,\n        -0.02417539],\n       [ 1.11353422,  0.52474645,  0.36634438,  0.04948435, -0.56008465,\n        -0.56041289,  0.86296686,  0.85225463,  0.56980401, -0.45178264,\n         0.1547852 , -0.5097227 ,  0.14690095, -0.24090356, -0.23771464,\n         0.13121121],\n       [-0.55058519, -0.22658334,  0.12001563,  0.1201676 ,  0.02831624,\n        -0.10344488, -0.21335243, -0.30877541, -0.28644806,  0.16879523,\n         0.16609844,  0.23010235,  0.06189198, -0.1270109 , -0.1606789 ,\n        -0.0256357 ],\n       [ 0.79727894, -0.38448564, -0.39912084, -0.12192727, -0.1109798 ,\n        -0.20741492,  0.44590317,  0.47997399,  0.95380704, -0.1711536 ,\n         0.0876542 ,  0.19816956, -0.29048718,  0.58234735,  0.13425214,\n         0.2670529 ],\n       [ 0.84116351,  0.66030302, -0.47466123, -0.40409659,  0.32594792,\n         0.84891756, -0.02954093,  0.3039096 ,  0.17292269, -0.22138149,\n        -0.74475465, -0.58926547, -0.1742972 ,  0.32437854,  0.66875799,\n        -0.12917142],\n       [ 0.84010712,  0.65978707, -0.47440618, -0.40383457,  0.32595057,\n         0.84860946, -0.02989655,  0.30334071,  0.17243803, -0.22105816,\n        -0.74435382, -0.58876297, -0.17417961,  0.32414074,  0.66838941,\n        -0.12918575],\n       [ 0.83945963,  0.65947124, -0.47425008, -0.40367418,  0.32595254,\n         0.84842133, -0.03011487,  0.30299186,  0.17214061, -0.22085996,\n        -0.74410865, -0.58845529, -0.17410756,  0.32399502,  0.6681639 ,\n        -0.12919473],\n       [ 0.83929916,  0.65939303, -0.47421142, -0.40363446,  0.32595307,\n         0.84837479, -0.03016902,  0.30290539,  0.17206687, -0.22081084,\n        -0.74404796, -0.58837907, -0.17408971,  0.32395892,  0.66810806,\n        -0.12919698],\n       [ 0.80528545, -0.38137064, -0.40071128, -0.12355452, -0.11159261,\n        -0.20628991,  0.44920022,  0.48455554,  0.9581233 , -0.17362697,\n         0.08550938,  0.19495951, -0.29134672,  0.5841291 ,  0.13635797,\n         0.26752005],\n       [-0.55392702, -0.22739926,  0.12056312,  0.12067513,  0.02885298,\n        -0.10330374, -0.21502523, -0.3108158 , -0.28863459,  0.16982525,\n         0.16653175,  0.23107869,  0.06227115, -0.12781427, -0.16122891,\n        -0.02603314],\n       [ 1.11473213,  0.5254184 ,  0.36600133,  0.04913977, -0.56000869,\n        -0.55990895,  0.863292  ,  0.8528638 ,  0.57028224, -0.45214389,\n         0.15421858, -0.51035803,  0.14675707, -0.24061879, -0.23720562,\n         0.13118548],\n       [-0.5464087 , -0.22555387,  0.11932989,  0.11953027,  0.02765039,\n        -0.10361008, -0.21126718, -0.30622766, -0.28372312,  0.16750775,\n         0.16554833,  0.22887483,  0.06141888, -0.12600874, -0.15998569,\n        -0.02514295],\n       [ 0.83975684,  0.65961617, -0.47432171, -0.40374778,  0.3259516 ,\n         0.84850762, -0.03001463,  0.303152  ,  0.17227717, -0.22095094,\n        -0.74422114, -0.58859649, -0.17414063,  0.3240619 ,  0.66826737,\n        -0.12919059],\n       [ 1.11592066,  0.52608696,  0.36565962,  0.04879676, -0.55993147,\n        -0.55940537,  0.86361277,  0.85346736,  0.5707552 , -0.45250217,\n         0.15365382, -0.5109898 ,  0.146614  , -0.24033577, -0.23669844,\n         0.13115903],\n       [-0.53570643, -0.22286675,  0.11756489,  0.11788174,  0.02596915,\n        -0.10397698, -0.20595077, -0.29971045, -0.2767792 ,  0.16420763,\n         0.16409539,  0.22569248,  0.06021034, -0.12344987, -0.15817989,\n        -0.0239001 ],\n       [ 1.11402171,  0.52501967,  0.36620494,  0.04934426, -0.56005396,\n        -0.56020824,  0.8630994 ,  0.85250264,  0.56999882, -0.45192967,\n         0.15455492, -0.50998108,  0.14684243, -0.24078773, -0.23750775,\n         0.13120085],\n       [-0.54133944, -0.22428984,  0.11849528,  0.11875217,  0.02684957,\n        -0.10379396, -0.20874416, -0.30313866, -0.28042713,  0.16594477,\n         0.16486786,  0.22737404,  0.06084577, -0.12479507, -0.15913562,\n        -0.02455071],\n       [ 1.11208862,  0.52393803,  0.36675659,  0.04989871, -0.56017385,\n        -0.5610163 ,  0.86257209,  0.85151838,  0.56922483, -0.45134649,\n         0.15546557, -0.50895788,  0.14707419, -0.2412466 , -0.23832609,\n         0.13124102],\n       [ 1.11444008,  0.5252544 ,  0.36608509,  0.04922388, -0.56002738,\n        -0.56003214,  0.8632129 ,  0.85271536,  0.57016579, -0.45205583,\n         0.15435696, -0.51020301,  0.14679217, -0.24068826, -0.23732992,\n         0.13119184],\n       [-0.53686904, -0.22316206,  0.11775717,  0.11806189,  0.02615005,\n        -0.10394104, -0.20652643, -0.30041764, -0.27753086,  0.1645662 ,\n         0.16425623,  0.22604074,  0.06034137, -0.12372722, -0.1583781 ,\n        -0.02403374],\n       [ 0.83788458,  0.65870435, -0.47387107, -0.40328472,  0.32595847,\n         0.84796592, -0.03064708,  0.30214277,  0.17141605, -0.22037779,\n        -0.74351392, -0.58770787, -0.17393243,  0.32364071,  0.66761665,\n        -0.12921721],\n       [ 1.11405176,  0.52503652,  0.36619634,  0.04933562, -0.56005206,\n        -0.56019561,  0.86310755,  0.85251792,  0.57001081, -0.45193873,\n         0.15454072, -0.50999701,  0.14683883, -0.24078059, -0.23749499,\n         0.13120021],\n       [ 0.79653996, -0.38477138, -0.39897465, -0.1217778 , -0.11092206,\n        -0.20751629,  0.44559764,  0.47955058,  0.95340725, -0.17092529,\n         0.08785032,  0.19846451, -0.29040785,  0.58218282,  0.13405914,\n         0.26700902],\n       [ 1.11306736,  0.52448507,  0.36647772,  0.04961835, -0.56011375,\n        -0.56060832,  0.86283965,  0.85201699,  0.5696172 , -0.4516418 ,\n         0.15500533, -0.50947548,  0.14695695, -0.24101442, -0.23791245,\n         0.13122098],\n       [ 0.83498309,  0.6572966 , -0.47317549, -0.40256974,  0.3259735 ,\n         0.84713519, -0.0316316 ,  0.30057673,  0.17007723, -0.21948931,\n        -0.74242432, -0.58633475, -0.1736102 ,  0.3229885 ,  0.66661328,\n        -0.12926094]])"
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.54541468, -0.22530726],\n       [ 0.11916643,  0.11937809]])"
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 ms ± 3.57 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        , -0.53217741,  0.        ],\n       [ 0.        ,  0.        ,  0.        ]])"
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [ 0., -0.,  0.],\n       [ 0.,  0.,  0.]])"
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 1, 2, 2)"
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "test_conv_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.52829787, -2.4338864 ],\n       [-2.73313474, -2.88112973]])"
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f4f70059d90>"
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 1, 28, 28)"
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parse_input_model()\n",
    "mnist_batch_1 = x_train[0:32].reshape(32, 1, 28, 28)\n",
    "mnist_labels_1 = y_train[0:32]\n",
    "mnist_batch_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [],
   "source": [
    "# train\n",
    "mnist_subsample_x = x_train[:4992]\n",
    "mnist_subsample_y = y_train[:4992]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:2000]\n",
    "mnist_validation_y = y_test[:2000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:7001]\n",
    "mnist_test_y = y_test[5001:7001]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [],
   "source": [
    "validation_batch = mnist_validation_x.reshape(2000, 1, 28, 28)\n",
    "validation_labels = label_binarizer.transform(mnist_validation_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [],
   "source": [
    "test_batch = mnist_test_x.reshape(2000, 1, 28, 28)\n",
    "test_labels = label_binarizer.transform(mnist_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [],
   "source": [
    "def measure_accuracy(y_true, y_pred):\n",
    "    accurate = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total = y_true.shape[0]\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [],
   "source": [
    "def predict_labels(a):\n",
    "    return (a == a.max(axis=1)[:,None]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 s ± 28.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "inp = mnist_batch_1\n",
    "for layer in model:\n",
    "    inp = layer.forward(inp)\n",
    "\n",
    "labels_true = label_binarizer.transform(mnist_labels_1)\n",
    "l = loss_function(labels_true, inp)\n",
    "\n",
    "out = labels_true\n",
    "for layer in reversed(model):\n",
    "    out = layer.backward(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     losses = []\n",
    "#     index = [i for i in range(1,157)]\n",
    "#     for j in range(0, 4992, 32):\n",
    "#         batch_x = mnist_subsample_x[j:j+32].reshape(32, 1, 28, 28)\n",
    "#         batch_y = mnist_subsample_y[j:j+32]\n",
    "#         model_out = batch_x\n",
    "#         # train\n",
    "#         for layer in model:\n",
    "#             # print(layer)\n",
    "#             model_out = layer.forward(model_out)\n",
    "#             # print(model_out.shape)\n",
    "#\n",
    "#         true_labels = label_binarizer.transform(batch_y)\n",
    "#         l = loss_function(true_labels, model_out)\n",
    "#         losses.append(l)\n",
    "#         # print(\"Epoc {} batch {} loss = {}\".format(i, j//32, l))\n",
    "#\n",
    "#\n",
    "#\n",
    "#         model_back = true_labels\n",
    "#         for layer in reversed(model):\n",
    "#             # print(layer)\n",
    "#             model_back = layer.backward(model_back)\n",
    "#             # print(model_back.shape)\n",
    "#\n",
    "#     plt.plot(index, losses)\n",
    "#     plt.show()\n",
    "#\n",
    "#     #validation\n",
    "#     # validation_out = validation_batch\n",
    "#     # for layer in model:\n",
    "#     #     validation_out = layer.forward(validation_out)\n",
    "#     # validation_loss = loss_function(validation_labels, validation_out)\n",
    "#     # print('Validation loss after epoc {} is {}'.format(i, validation_loss))\n",
    "#     # validation_predictions = predict_labels(validation_out)\n",
    "#     # accuracy = measure_accuracy(validation_labels, validation_predictions)\n",
    "#     # print('Validation accuracy after epoc {} is {}'.format(i, accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}