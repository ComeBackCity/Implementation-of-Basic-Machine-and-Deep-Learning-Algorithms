{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0) * 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.rand(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * 0.001\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.random.rand(self.output_channel_count) * 0.001\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 0.01) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dout_padded = np.zeros(padded_dimensions)\n",
    "        filter_shape = self.filters.shape\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            tmp_y = out_y = 0\n",
    "            while tmp_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                tmp_x = out_x = 0\n",
    "                while tmp_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension]\n",
    "                    # dw += np.sum(dz[i, :, out_x, out_y].reshape(self.output_channel_count, 1, 1) * image_slice)\n",
    "                    # dout_padded[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension] += np.sum(np.broadcast_to(dz[i, :, out_x, out_y].reshape(self.output_channel_count, padded_dimensions[1], 1, 1), filter_shape) * self.filters, axis=0)\n",
    "                    for f in range(self.output_channel_count):\n",
    "                        dout_padded[i, :, tmp_x: tmp_x+self.filter_dimension, tmp_y:tmp_y+self.filter_dimension] += dz[i, f, out_x, out_y] * self.filters[f, :, :, :]\n",
    "                    tmp_x += self.stride\n",
    "                    out_x += 1\n",
    "                tmp_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        self.filters -= learning_rate * dw\n",
    "        return dout_padded[:, :, self.padding:padded_dimensions[2]-self.padding, self.padding:padded_dimensions[3]-self.padding]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00072176, 0.00072176, 0.00072176],\n       [0.00072176, 0.00120162, 0.00072176],\n       [0.00072176, 0.00072176, 0.00072176]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(input_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00072176, 0.00072176, 0.00072176],\n       [0.00072176, 0.00120162, 0.00072176],\n       [0.00072176, 0.00072176, 0.00072176]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    max_val = np.max(image_slice, axis=(1, 2))\n",
    "                    output[i, :, out_x, out_y] = max_val\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == max_val.reshape(input_dimensions[1], 1, 1)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00120162, 0.00120162],\n       [0.00120162, 0.00120162]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00120162, 0.00120162, 0.00120162, 0.00120162, 0.00077174,\n        0.00077174, 0.00077174, 0.00077174, 0.00139764, 0.00139764,\n        0.00139764, 0.00139764, 0.00119444, 0.00119444, 0.00119444,\n        0.00119444],\n       [0.00166546, 0.00166546, 0.00166546, 0.00166546, 0.00123558,\n        0.00123558, 0.00123558, 0.00123558, 0.00186148, 0.00186148,\n        0.00186148, 0.00186148, 0.00165828, 0.00165828, 0.00165828,\n        0.00165828],\n       [0.00260606, 0.00260606, 0.00260606, 0.00260606, 0.00217618,\n        0.00217618, 0.00217618, 0.00217618, 0.00280208, 0.00280208,\n        0.00280208, 0.00280208, 0.00259888, 0.00259888, 0.00259888,\n        0.00259888],\n       [0.00213731, 0.00213731, 0.00213731, 0.00213731, 0.00170743,\n        0.00170743, 0.00170743, 0.00170743, 0.00233333, 0.00233333,\n        0.00233333, 0.00233333, 0.00213013, 0.00213013, 0.00213013,\n        0.00213013],\n       [0.00260565, 0.00260565, 0.00260565, 0.00260565, 0.00217577,\n        0.00217577, 0.00217577, 0.00217577, 0.00280167, 0.00280167,\n        0.00280167, 0.00280167, 0.00259847, 0.00259847, 0.00259847,\n        0.00259847],\n       [0.00165825, 0.00165825, 0.00165825, 0.00165825, 0.00122837,\n        0.00122837, 0.00122837, 0.00122837, 0.00185427, 0.00185427,\n        0.00185427, 0.00185427, 0.00165107, 0.00165107, 0.00165107,\n        0.00165107],\n       [0.00118719, 0.00118719, 0.00118719, 0.00118719, 0.00075731,\n        0.00075731, 0.00075731, 0.00075731, 0.00138321, 0.00138321,\n        0.00138321, 0.00138321, 0.00118001, 0.00118001, 0.00118001,\n        0.00118001],\n       [0.00214411, 0.00214411, 0.00214411, 0.00214411, 0.00171423,\n        0.00171423, 0.00171423, 0.00171423, 0.00234013, 0.00234013,\n        0.00234013, 0.00234013, 0.00213693, 0.00213693, 0.00213693,\n        0.00213693],\n       [0.00119099, 0.00119099, 0.00119099, 0.00119099, 0.00076111,\n        0.00076111, 0.00076111, 0.00076111, 0.00138701, 0.00138701,\n        0.00138701, 0.00138701, 0.00118381, 0.00118381, 0.00118381,\n        0.00118381],\n       [0.00213819, 0.00213819, 0.00213819, 0.00213819, 0.00170831,\n        0.00170831, 0.00170831, 0.00170831, 0.0023342 , 0.0023342 ,\n        0.0023342 , 0.0023342 , 0.00213101, 0.00213101, 0.00213101,\n        0.00213101],\n       [0.00213113, 0.00213113, 0.00213113, 0.00213113, 0.00170125,\n        0.00170125, 0.00170125, 0.00170125, 0.00232715, 0.00232715,\n        0.00232715, 0.00232715, 0.00212395, 0.00212395, 0.00212395,\n        0.00212395],\n       [0.00214071, 0.00214071, 0.00214071, 0.00214071, 0.00171084,\n        0.00171084, 0.00171084, 0.00171084, 0.00233673, 0.00233673,\n        0.00233673, 0.00233673, 0.00213353, 0.00213353, 0.00213353,\n        0.00213353],\n       [0.00214268, 0.00214268, 0.00214268, 0.00214268, 0.0017128 ,\n        0.0017128 , 0.0017128 , 0.0017128 , 0.0023387 , 0.0023387 ,\n        0.0023387 , 0.0023387 , 0.0021355 , 0.0021355 , 0.0021355 ,\n        0.0021355 ],\n       [0.00165204, 0.00165204, 0.00165204, 0.00165204, 0.00122216,\n        0.00122216, 0.00122216, 0.00122216, 0.00184805, 0.00184805,\n        0.00184805, 0.00184805, 0.00164486, 0.00164486, 0.00164486,\n        0.00164486],\n       [0.00118014, 0.00118014, 0.00118014, 0.00118014, 0.00075026,\n        0.00075026, 0.00075026, 0.00075026, 0.00137616, 0.00137616,\n        0.00137616, 0.00137616, 0.00117296, 0.00117296, 0.00117296,\n        0.00117296],\n       [0.00166244, 0.00166244, 0.00166244, 0.00166244, 0.00123256,\n        0.00123256, 0.00123256, 0.00123256, 0.00185846, 0.00185846,\n        0.00185846, 0.00185846, 0.00165526, 0.00165526, 0.00165526,\n        0.00165526],\n       [0.00119473, 0.00119473, 0.00119473, 0.00119473, 0.00076485,\n        0.00076485, 0.00076485, 0.00076485, 0.00139075, 0.00139075,\n        0.00139075, 0.00139075, 0.00118755, 0.00118755, 0.00118755,\n        0.00118755],\n       [0.00214628, 0.00214628, 0.00214628, 0.00214628, 0.0017164 ,\n        0.0017164 , 0.0017164 , 0.0017164 , 0.0023423 , 0.0023423 ,\n        0.0023423 , 0.0023423 , 0.0021391 , 0.0021391 , 0.0021391 ,\n        0.0021391 ],\n       [0.00259444, 0.00259444, 0.00259444, 0.00259444, 0.00216456,\n        0.00216456, 0.00216456, 0.00216456, 0.00279046, 0.00279046,\n        0.00279046, 0.00279046, 0.00258726, 0.00258726, 0.00258726,\n        0.00258726],\n       [0.00118399, 0.00118399, 0.00118399, 0.00118399, 0.00075411,\n        0.00075411, 0.00075411, 0.00075411, 0.00138001, 0.00138001,\n        0.00138001, 0.00138001, 0.00117681, 0.00117681, 0.00117681,\n        0.00117681],\n       [0.00120051, 0.00120051, 0.00120051, 0.00120051, 0.00077063,\n        0.00077063, 0.00077063, 0.00077063, 0.00139653, 0.00139653,\n        0.00139653, 0.00139653, 0.00119333, 0.00119333, 0.00119333,\n        0.00119333],\n       [0.00261144, 0.00261144, 0.00261144, 0.00261144, 0.00218156,\n        0.00218156, 0.00218156, 0.00218156, 0.00280746, 0.00280746,\n        0.00280746, 0.00280746, 0.00260426, 0.00260426, 0.00260426,\n        0.00260426],\n       [0.00120141, 0.00120141, 0.00120141, 0.00120141, 0.00077153,\n        0.00077153, 0.00077153, 0.00077153, 0.00139743, 0.00139743,\n        0.00139743, 0.00139743, 0.00119423, 0.00119423, 0.00119423,\n        0.00119423],\n       [0.00120156, 0.00120156, 0.00120156, 0.00120156, 0.00077169,\n        0.00077169, 0.00077169, 0.00077169, 0.00139758, 0.00139758,\n        0.00139758, 0.00139758, 0.00119439, 0.00119439, 0.00119439,\n        0.00119439],\n       [0.00261045, 0.00261045, 0.00261045, 0.00261045, 0.00218057,\n        0.00218057, 0.00218057, 0.00218057, 0.00280647, 0.00280647,\n        0.00280647, 0.00280647, 0.00260327, 0.00260327, 0.00260327,\n        0.00260327],\n       [0.00119101, 0.00119101, 0.00119101, 0.00119101, 0.00076113,\n        0.00076113, 0.00076113, 0.00076113, 0.00138703, 0.00138703,\n        0.00138703, 0.00138703, 0.00118383, 0.00118383, 0.00118383,\n        0.00118383],\n       [0.00261518, 0.00261518, 0.00261518, 0.00261518, 0.0021853 ,\n        0.0021853 , 0.0021853 , 0.0021853 , 0.0028112 , 0.0028112 ,\n        0.0028112 , 0.0028112 , 0.002608  , 0.002608  , 0.002608  ,\n        0.002608  ],\n       [0.00118756, 0.00118756, 0.00118756, 0.00118756, 0.00075768,\n        0.00075768, 0.00075768, 0.00075768, 0.00138358, 0.00138358,\n        0.00138358, 0.00138358, 0.00118038, 0.00118038, 0.00118038,\n        0.00118038],\n       [0.0016645 , 0.0016645 , 0.0016645 , 0.0016645 , 0.00123462,\n        0.00123462, 0.00123462, 0.00123462, 0.00186052, 0.00186052,\n        0.00186052, 0.00186052, 0.00165732, 0.00165732, 0.00165732,\n        0.00165732],\n       [0.00214145, 0.00214145, 0.00214145, 0.00214145, 0.00171157,\n        0.00171157, 0.00171157, 0.00171157, 0.00233747, 0.00233747,\n        0.00233747, 0.00233747, 0.00213427, 0.00213427, 0.00213427,\n        0.00213427],\n       [0.0021379 , 0.0021379 , 0.0021379 , 0.0021379 , 0.00170802,\n        0.00170802, 0.00170802, 0.00170802, 0.00233392, 0.00233392,\n        0.00233392, 0.00233392, 0.00213072, 0.00213072, 0.00213072,\n        0.00213072],\n       [0.00214757, 0.00214757, 0.00214757, 0.00214757, 0.00171769,\n        0.00171769, 0.00171769, 0.00171769, 0.00234359, 0.00234359,\n        0.00234359, 0.00234359, 0.00214039, 0.00214039, 0.00214039,\n        0.00214039],\n       [0.00214464, 0.00214464, 0.00214464, 0.00214464, 0.00171476,\n        0.00171476, 0.00171476, 0.00171476, 0.00234066, 0.00234066,\n        0.00234066, 0.00234066, 0.00213746, 0.00213746, 0.00213746,\n        0.00213746],\n       [0.00168637, 0.00168637, 0.00168637, 0.00168637, 0.00125649,\n        0.00125649, 0.00125649, 0.00125649, 0.00188239, 0.00188239,\n        0.00188239, 0.00188239, 0.00167919, 0.00167919, 0.00167919,\n        0.00167919],\n       [0.00118523, 0.00118523, 0.00118523, 0.00118523, 0.00075535,\n        0.00075535, 0.00075535, 0.00075535, 0.00138124, 0.00138124,\n        0.00138124, 0.00138124, 0.00117805, 0.00117805, 0.00117805,\n        0.00117805],\n       [0.00260924, 0.00260924, 0.00260924, 0.00260924, 0.00217936,\n        0.00217936, 0.00217936, 0.00217936, 0.00280526, 0.00280526,\n        0.00280526, 0.00280526, 0.00260206, 0.00260206, 0.00260206,\n        0.00260206],\n       [0.00119852, 0.00119852, 0.00119852, 0.00119852, 0.00076864,\n        0.00076864, 0.00076864, 0.00076864, 0.00139454, 0.00139454,\n        0.00139454, 0.00139454, 0.00119134, 0.00119134, 0.00119134,\n        0.00119134],\n       [0.00214602, 0.00214602, 0.00214602, 0.00214602, 0.00171614,\n        0.00171614, 0.00171614, 0.00171614, 0.00234204, 0.00234204,\n        0.00234204, 0.00234204, 0.00213884, 0.00213884, 0.00213884,\n        0.00213884],\n       [0.00262193, 0.00262193, 0.00262193, 0.00262193, 0.00219205,\n        0.00219205, 0.00219205, 0.00219205, 0.00281795, 0.00281795,\n        0.00281795, 0.00281795, 0.00261475, 0.00261475, 0.00261475,\n        0.00261475],\n       [0.00121072, 0.00121072, 0.00121072, 0.00121072, 0.00078084,\n        0.00078084, 0.00078084, 0.00078084, 0.00140674, 0.00140674,\n        0.00140674, 0.00140674, 0.00120354, 0.00120354, 0.00120354,\n        0.00120354],\n       [0.00260628, 0.00260628, 0.00260628, 0.00260628, 0.0021764 ,\n        0.0021764 , 0.0021764 , 0.0021764 , 0.0028023 , 0.0028023 ,\n        0.0028023 , 0.0028023 , 0.0025991 , 0.0025991 , 0.0025991 ,\n        0.0025991 ],\n       [0.00119285, 0.00119285, 0.00119285, 0.00119285, 0.00076297,\n        0.00076297, 0.00076297, 0.00076297, 0.00138887, 0.00138887,\n        0.00138887, 0.00138887, 0.00118567, 0.00118567, 0.00118567,\n        0.00118567],\n       [0.00260779, 0.00260779, 0.00260779, 0.00260779, 0.00217791,\n        0.00217791, 0.00217791, 0.00217791, 0.00280381, 0.00280381,\n        0.00280381, 0.00280381, 0.00260061, 0.00260061, 0.00260061,\n        0.00260061],\n       [0.00260938, 0.00260938, 0.00260938, 0.00260938, 0.0021795 ,\n        0.0021795 , 0.0021795 , 0.0021795 , 0.00280539, 0.00280539,\n        0.00280539, 0.00280539, 0.0026022 , 0.0026022 , 0.0026022 ,\n        0.0026022 ],\n       [0.00120667, 0.00120667, 0.00120667, 0.00120667, 0.00077679,\n        0.00077679, 0.00077679, 0.00077679, 0.00140269, 0.00140269,\n        0.00140269, 0.00140269, 0.00119949, 0.00119949, 0.00119949,\n        0.00119949],\n       [0.00213878, 0.00213878, 0.00213878, 0.00213878, 0.0017089 ,\n        0.0017089 , 0.0017089 , 0.0017089 , 0.0023348 , 0.0023348 ,\n        0.0023348 , 0.0023348 , 0.0021316 , 0.0021316 , 0.0021316 ,\n        0.0021316 ],\n       [0.002612  , 0.002612  , 0.002612  , 0.002612  , 0.00218212,\n        0.00218212, 0.00218212, 0.00218212, 0.00280802, 0.00280802,\n        0.00280802, 0.00280802, 0.00260482, 0.00260482, 0.00260482,\n        0.00260482],\n       [0.00166564, 0.00166564, 0.00166564, 0.00166564, 0.00123576,\n        0.00123576, 0.00123576, 0.00123576, 0.00186166, 0.00186166,\n        0.00186166, 0.00186166, 0.00165846, 0.00165846, 0.00165846,\n        0.00165846],\n       [0.00260199, 0.00260199, 0.00260199, 0.00260199, 0.00217211,\n        0.00217211, 0.00217211, 0.00217211, 0.00279801, 0.00279801,\n        0.00279801, 0.00279801, 0.00259481, 0.00259481, 0.00259481,\n        0.00259481],\n       [0.00212675, 0.00212675, 0.00212675, 0.00212675, 0.00169687,\n        0.00169687, 0.00169687, 0.00169687, 0.00232277, 0.00232277,\n        0.00232277, 0.00232277, 0.00211957, 0.00211957, 0.00211957,\n        0.00211957]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.rand(flattened_input.shape[1], self.output_dimension) * 0.001\n",
    "        if self.bias is None:\n",
    "            self.bias = np.random.rand(1, self.output_dimension) * 0.001\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 0.01) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[3.10923949e-04, 3.49990193e-05, 3.10825007e-04, 2.50688790e-04],\n       [3.14847440e-04, 3.89966825e-05, 3.13843585e-04, 2.54266757e-04],\n       [3.22803722e-04, 4.71033748e-05, 3.19964831e-04, 2.61522366e-04],\n       [3.18838660e-04, 4.30633552e-05, 3.16914271e-04, 2.57906489e-04],\n       [3.22800190e-04, 4.70997763e-05, 3.19962114e-04, 2.61519145e-04],\n       [3.14786446e-04, 3.89345353e-05, 3.13796658e-04, 2.54211135e-04],\n       [3.10801949e-04, 3.48747132e-05, 3.10731145e-04, 2.50577535e-04],\n       [3.18896228e-04, 4.31220114e-05, 3.16958561e-04, 2.57958987e-04],\n       [3.10834034e-04, 3.49074045e-05, 3.10755830e-04, 2.50606794e-04],\n       [3.18846084e-04, 4.30709193e-05, 3.16919982e-04, 2.57913259e-04],\n       [3.18786412e-04, 4.30101193e-05, 3.16874073e-04, 2.57858842e-04],\n       [3.18867477e-04, 4.30927162e-05, 3.16936441e-04, 2.57932768e-04],\n       [3.18884132e-04, 4.31096867e-05, 3.16949255e-04, 2.57947956e-04],\n       [3.14733898e-04, 3.88809943e-05, 3.13756230e-04, 2.54163215e-04],\n       [3.10742301e-04, 3.48139379e-05, 3.10685254e-04, 2.50523140e-04],\n       [3.14821938e-04, 3.89706981e-05, 3.13823964e-04, 2.54243501e-04],\n       [3.10865693e-04, 3.49396627e-05, 3.10780187e-04, 2.50635665e-04],\n       [3.18914543e-04, 4.31406722e-05, 3.16972652e-04, 2.57975689e-04],\n       [3.22705416e-04, 4.70032107e-05, 3.19889199e-04, 2.61432717e-04],\n       [3.10774834e-04, 3.48470860e-05, 3.10710284e-04, 2.50552808e-04],\n       [3.10914622e-04, 3.49895158e-05, 3.10817831e-04, 2.50680285e-04],\n       [3.22849189e-04, 4.71497009e-05, 3.19999811e-04, 2.61563828e-04],\n       [3.10922192e-04, 3.49972292e-05, 3.10823655e-04, 2.50687188e-04],\n       [3.10923510e-04, 3.49985718e-05, 3.10824669e-04, 2.50688390e-04],\n       [3.22840849e-04, 4.71412031e-05, 3.19993395e-04, 2.61556223e-04],\n       [3.10834209e-04, 3.49075835e-05, 3.10755965e-04, 2.50606954e-04],\n       [3.22880826e-04, 4.71819365e-05, 3.20024152e-04, 2.61592680e-04],\n       [3.10805049e-04, 3.48778722e-05, 3.10733530e-04, 2.50580362e-04],\n       [3.14839318e-04, 3.89884070e-05, 3.13837336e-04, 2.54259351e-04],\n       [3.18873735e-04, 4.30990926e-05, 3.16941255e-04, 2.57938474e-04],\n       [3.18843677e-04, 4.30684665e-05, 3.16918130e-04, 2.57911064e-04],\n       [3.18925480e-04, 4.31518158e-05, 3.16981066e-04, 2.57985663e-04],\n       [3.18900677e-04, 4.31265439e-05, 3.16961983e-04, 2.57963044e-04],\n       [3.15024336e-04, 3.91769228e-05, 3.13979681e-04, 2.54428075e-04],\n       [3.10785298e-04, 3.48577477e-05, 3.10718334e-04, 2.50562350e-04],\n       [3.22830585e-04, 4.71307456e-05, 3.19985499e-04, 2.61546863e-04],\n       [3.10897765e-04, 3.49723405e-05, 3.10804862e-04, 2.50664913e-04],\n       [3.18912372e-04, 4.31384607e-05, 3.16970982e-04, 2.57973710e-04],\n       [3.22937900e-04, 4.72400895e-05, 3.20068063e-04, 2.61644727e-04],\n       [3.11000922e-04, 3.50774478e-05, 3.10884227e-04, 2.50758985e-04],\n       [3.22805521e-04, 4.71052074e-05, 3.19966215e-04, 2.61524006e-04],\n       [3.10849787e-04, 3.49234561e-05, 3.10767950e-04, 2.50621160e-04],\n       [3.22818338e-04, 4.71182666e-05, 3.19976076e-04, 2.61535694e-04],\n       [3.22831731e-04, 4.71319133e-05, 3.19986380e-04, 2.61547908e-04],\n       [3.10966666e-04, 3.50425437e-05, 3.10857871e-04, 2.50727745e-04],\n       [3.18851150e-04, 4.30760815e-05, 3.16923880e-04, 2.57917879e-04],\n       [3.22853956e-04, 4.71545588e-05, 3.20003480e-04, 2.61568176e-04],\n       [3.14848991e-04, 3.89982625e-05, 3.13844778e-04, 2.54268172e-04],\n       [3.22769257e-04, 4.70682580e-05, 3.19938315e-04, 2.61490936e-04],\n       [3.18749360e-04, 4.29723667e-05, 3.16845566e-04, 2.57825053e-04]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1).reshape(-1, 1)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.25002102, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002109, 0.24995213, 0.25002084, 0.25000594],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002109, 0.24995213, 0.25002084, 0.25000594],\n       [0.25002101, 0.24995204, 0.250021  , 0.25000596],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002101, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002109, 0.24995213, 0.25002084, 0.25000594],\n       [0.25002101, 0.24995203, 0.250021  , 0.25000596],\n       [0.25002109, 0.24995213, 0.25002084, 0.25000594],\n       [0.25002101, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002117, 0.24995223, 0.25002068, 0.25000593],\n       [0.25002124, 0.24995231, 0.25002053, 0.25000592],\n       [0.25002101, 0.24995203, 0.250021  , 0.25000596],\n       [0.25002102, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002102, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002102, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002101, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002101, 0.24995204, 0.250021  , 0.25000596],\n       [0.25002109, 0.24995213, 0.25002084, 0.25000594],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002117, 0.24995223, 0.25002068, 0.25000593],\n       [0.25002116, 0.24995223, 0.25002068, 0.25000593],\n       [0.25002109, 0.24995213, 0.25002083, 0.25000594],\n       [0.25002101, 0.24995203, 0.250021  , 0.25000596],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002101, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002117, 0.24995223, 0.25002068, 0.25000593],\n       [0.25002124, 0.24995232, 0.25002052, 0.25000592],\n       [0.25002102, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002101, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002102, 0.24995204, 0.25002099, 0.25000596],\n       [0.25002116, 0.24995222, 0.25002068, 0.25000593],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002109, 0.24995213, 0.25002084, 0.25000594],\n       [0.25002124, 0.24995232, 0.25002053, 0.25000592],\n       [0.25002116, 0.24995222, 0.25002069, 0.25000593]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "1.3862649577055566"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.74997898,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002109, -0.75004787,  0.25002084,  0.25000594],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [ 0.25002109, -0.75004787,  0.25002084,  0.25000594],\n       [-0.74997899,  0.24995204,  0.250021  ,  0.25000596],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [-0.74997899,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002109, -0.75004787,  0.25002084,  0.25000594],\n       [-0.74997899,  0.24995203,  0.250021  ,  0.25000596],\n       [ 0.25002109, -0.75004787,  0.25002084,  0.25000594],\n       [-0.74997899,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002117,  0.24995223, -0.74997932,  0.25000593],\n       [ 0.25002124,  0.24995231,  0.25002053, -0.74999408],\n       [-0.74997899,  0.24995203,  0.250021  ,  0.25000596],\n       [-0.74997898,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [-0.74997898,  0.24995204,  0.25002099,  0.25000596],\n       [-0.74997898,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [-0.74997899,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [-0.74997899,  0.24995204,  0.250021  ,  0.25000596],\n       [ 0.25002109, -0.75004787,  0.25002084,  0.25000594],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002117,  0.24995223, -0.74997932,  0.25000593],\n       [ 0.25002116,  0.24995223, -0.74997932,  0.25000593],\n       [ 0.25002109, -0.75004787,  0.25002083,  0.25000594],\n       [-0.74997899,  0.24995203,  0.250021  ,  0.25000596],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [-0.74997899,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002117,  0.24995223, -0.74997932,  0.25000593],\n       [ 0.25002124,  0.24995232,  0.25002052, -0.74999408],\n       [-0.74997898,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [-0.74997899,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [-0.74997898,  0.24995204,  0.25002099,  0.25000596],\n       [ 0.25002116,  0.24995222, -0.74997932,  0.25000593],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [ 0.25002109, -0.75004787,  0.25002084,  0.25000594],\n       [ 0.25002124,  0.24995232,  0.25002053, -0.74999408],\n       [ 0.25002116,  0.24995222, -0.74997931,  0.25000593]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-1.09480736e-04,  5.98594155e-05, -5.81571038e-06,\n        -5.03880313e-04,  2.89945292e-04,  5.80630849e-05,\n         2.19879663e-04, -8.13746269e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530202e-05,  1.56089551e-04,\n         6.27541693e-05,  2.02723761e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 2.90893572e-04,  6.76785680e-05,  4.35424916e-05,\n         8.55325576e-05, -4.24751603e-04, -2.13637681e-04,\n        -1.83115351e-04,  7.91210912e-05,  4.24029887e-04,\n         1.74069896e-04, -2.46607054e-04, -1.61810421e-04,\n        -3.80356731e-04, -3.01291867e-04, -2.85474008e-05,\n        -1.86902947e-05],\n       [-2.41070021e-04, -1.14457390e-05,  1.22934239e-04,\n         1.04068749e-04, -2.22357284e-04, -2.51200837e-04,\n        -3.12404060e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234199e-05, -1.10088540e-04,\n        -3.60184430e-05,  2.92101977e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [ 5.97228468e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309377e-04,  3.57033474e-04,  4.06701851e-04,\n         2.75570673e-04, -1.48392277e-04,  1.91067461e-04,\n        -1.95748415e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514442e-04, -1.11478450e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [-2.41070021e-04, -1.14457390e-05,  1.22934239e-04,\n         1.04068749e-04, -2.22357284e-04, -2.51200837e-04,\n        -3.12404060e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234198e-05, -1.10088540e-04,\n        -3.60184431e-05,  2.92101977e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [ 2.90893572e-04,  6.76785685e-05,  4.35424920e-05,\n         8.55325564e-05, -4.24751604e-04, -2.13637682e-04,\n        -1.83115351e-04,  7.91210915e-05,  4.24029886e-04,\n         1.74069897e-04, -2.46607055e-04, -1.61810421e-04,\n        -3.80356732e-04, -3.01291867e-04, -2.85474008e-05,\n        -1.86902947e-05],\n       [-1.09480735e-04,  5.98594163e-05, -5.81570955e-06,\n        -5.03880315e-04,  2.89945290e-04,  5.80630826e-05,\n         2.19879661e-04, -8.13746262e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530190e-05,  1.56089551e-04,\n         6.27541667e-05,  2.02723752e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 5.97228466e-05, -1.16074447e-04, -1.60641779e-04,\n         3.14309378e-04,  3.57033475e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514443e-04, -1.11478446e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [-1.09480735e-04,  5.98594161e-05, -5.81570977e-06,\n        -5.03880314e-04,  2.89945290e-04,  5.80630832e-05,\n         2.19879662e-04, -8.13746264e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530193e-05,  1.56089551e-04,\n         6.27541674e-05,  2.02723755e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 5.97228468e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309377e-04,  3.57033474e-04,  4.06701851e-04,\n         2.75570673e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748415e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514442e-04, -1.11478450e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 5.97228470e-05, -1.16074446e-04, -1.60641778e-04,\n         3.14309376e-04,  3.57033473e-04,  4.06701850e-04,\n         2.75570673e-04, -1.48392277e-04,  1.91067461e-04,\n        -1.95748415e-04,  1.21666795e-04,  1.15758268e-04,\n         3.53514441e-04, -1.11478454e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 5.97228467e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309378e-04,  3.57033475e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514443e-04, -1.11478448e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 5.97228466e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309378e-04,  3.57033475e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514443e-04, -1.11478447e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 2.90893573e-04,  6.76785689e-05,  4.35424924e-05,\n         8.55325554e-05, -4.24751605e-04, -2.13637683e-04,\n        -1.83115352e-04,  7.91210918e-05,  4.24029886e-04,\n         1.74069897e-04, -2.46607055e-04, -1.61810421e-04,\n        -3.80356733e-04, -3.01291868e-04, -2.85474007e-05,\n        -1.86902947e-05],\n       [-1.09480735e-04,  5.98594168e-05, -5.81570915e-06,\n        -5.03880316e-04,  2.89945289e-04,  5.80630815e-05,\n         2.19879661e-04, -8.13746259e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530184e-05,  1.56089550e-04,\n         6.27541655e-05,  2.02723748e-05, -1.45221983e-04,\n        -1.10801229e-04],\n       [ 2.90893572e-04,  6.76785682e-05,  4.35424918e-05,\n         8.55325571e-05, -4.24751604e-04, -2.13637681e-04,\n        -1.83115351e-04,  7.91210913e-05,  4.24029887e-04,\n         1.74069896e-04, -2.46607054e-04, -1.61810421e-04,\n        -3.80356732e-04, -3.01291867e-04, -2.85474008e-05,\n        -1.86902947e-05],\n       [-1.09480735e-04,  5.98594159e-05, -5.81570998e-06,\n        -5.03880314e-04,  2.89945291e-04,  5.80630838e-05,\n         2.19879662e-04, -8.13746266e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530196e-05,  1.56089551e-04,\n         6.27541681e-05,  2.02723757e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 5.97228465e-05, -1.16074447e-04, -1.60641779e-04,\n         3.14309379e-04,  3.57033476e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666797e-04,  1.15758268e-04,\n         3.53514444e-04, -1.11478445e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [-2.41070021e-04, -1.14457383e-05,  1.22934239e-04,\n         1.04068747e-04, -2.22357286e-04, -2.51200839e-04,\n        -3.12404061e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000280e-04,  6.60234189e-05, -1.10088541e-04,\n        -3.60184451e-05,  2.92101977e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480735e-04,  5.98594165e-05, -5.81570937e-06,\n        -5.03880316e-04,  2.89945289e-04,  5.80630821e-05,\n         2.19879661e-04, -8.13746261e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530187e-05,  1.56089550e-04,\n         6.27541662e-05,  2.02723751e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-1.09480736e-04,  5.98594155e-05, -5.81571031e-06,\n        -5.03880313e-04,  2.89945292e-04,  5.80630848e-05,\n         2.19879663e-04, -8.13746269e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530201e-05,  1.56089551e-04,\n         6.27541691e-05,  2.02723761e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-2.41070021e-04, -1.14457393e-05,  1.22934239e-04,\n         1.04068750e-04, -2.22357284e-04, -2.51200836e-04,\n        -3.12404060e-04,  1.50676569e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234203e-05, -1.10088540e-04,\n        -3.60184420e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480736e-04,  5.98594155e-05, -5.81571036e-06,\n        -5.03880313e-04,  2.89945292e-04,  5.80630849e-05,\n         2.19879663e-04, -8.13746269e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530201e-05,  1.56089551e-04,\n         6.27541693e-05,  2.02723761e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-1.09480736e-04,  5.98594155e-05, -5.81571037e-06,\n        -5.03880313e-04,  2.89945292e-04,  5.80630849e-05,\n         2.19879663e-04, -8.13746269e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530201e-05,  1.56089551e-04,\n         6.27541693e-05,  2.02723761e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-2.41070021e-04, -1.14457393e-05,  1.22934239e-04,\n         1.04068750e-04, -2.22357284e-04, -2.51200836e-04,\n        -3.12404060e-04,  1.50676569e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234202e-05, -1.10088540e-04,\n        -3.60184422e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480735e-04,  5.98594161e-05, -5.81570977e-06,\n        -5.03880314e-04,  2.89945290e-04,  5.80630832e-05,\n         2.19879662e-04, -8.13746264e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530193e-05,  1.56089551e-04,\n         6.27541674e-05,  2.02723755e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-2.41070021e-04, -1.14457395e-05,  1.22934238e-04,\n         1.04068751e-04, -2.22357283e-04, -2.51200835e-04,\n        -3.12404059e-04,  1.50676569e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234206e-05, -1.10088540e-04,\n        -3.60184414e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480735e-04,  5.98594163e-05, -5.81570957e-06,\n        -5.03880315e-04,  2.89945290e-04,  5.80630827e-05,\n         2.19879661e-04, -8.13746263e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530190e-05,  1.56089551e-04,\n         6.27541668e-05,  2.02723753e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 2.90893572e-04,  6.76785681e-05,  4.35424917e-05,\n         8.55325575e-05, -4.24751603e-04, -2.13637681e-04,\n        -1.83115351e-04,  7.91210912e-05,  4.24029887e-04,\n         1.74069896e-04, -2.46607054e-04, -1.61810421e-04,\n        -3.80356731e-04, -3.01291867e-04, -2.85474008e-05,\n        -1.86902947e-05],\n       [ 5.97228467e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309378e-04,  3.57033475e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514443e-04, -1.11478448e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 5.97228468e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309377e-04,  3.57033474e-04,  4.06701851e-04,\n         2.75570673e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748415e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514442e-04, -1.11478450e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 5.97228465e-05, -1.16074447e-04, -1.60641779e-04,\n         3.14309379e-04,  3.57033476e-04,  4.06701853e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666797e-04,  1.15758268e-04,\n         3.53514444e-04, -1.11478444e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 5.97228466e-05, -1.16074447e-04, -1.60641779e-04,\n         3.14309378e-04,  3.57033475e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514443e-04, -1.11478446e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [ 2.90893572e-04,  6.76785668e-05,  4.35424904e-05,\n         8.55325612e-05, -4.24751600e-04, -2.13637677e-04,\n        -1.83115349e-04,  7.91210902e-05,  4.24029887e-04,\n         1.74069896e-04, -2.46607052e-04, -1.61810420e-04,\n        -3.80356727e-04, -3.01291866e-04, -2.85474011e-05,\n        -1.86902949e-05],\n       [-1.09480735e-04,  5.98594165e-05, -5.81570944e-06,\n        -5.03880315e-04,  2.89945289e-04,  5.80630823e-05,\n         2.19879661e-04, -8.13746262e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530188e-05,  1.56089550e-04,\n         6.27541664e-05,  2.02723751e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-2.41070021e-04, -1.14457392e-05,  1.22934239e-04,\n         1.04068750e-04, -2.22357284e-04, -2.51200836e-04,\n        -3.12404060e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234201e-05, -1.10088540e-04,\n        -3.60184424e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480735e-04,  5.98594156e-05, -5.81571020e-06,\n        -5.03880313e-04,  2.89945291e-04,  5.80630844e-05,\n         2.19879662e-04, -8.13746268e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530199e-05,  1.56089551e-04,\n         6.27541688e-05,  2.02723759e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 5.97228466e-05, -1.16074447e-04, -1.60641779e-04,\n         3.14309379e-04,  3.57033476e-04,  4.06701852e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666797e-04,  1.15758268e-04,\n         3.53514444e-04, -1.11478445e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [-2.41070021e-04, -1.14457400e-05,  1.22934238e-04,\n         1.04068752e-04, -2.22357282e-04, -2.51200834e-04,\n        -3.12404059e-04,  1.50676569e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234212e-05, -1.10088540e-04,\n        -3.60184401e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480736e-04,  5.98594149e-05, -5.81571090e-06,\n        -5.03880311e-04,  2.89945293e-04,  5.80630864e-05,\n         2.19879663e-04, -8.13746274e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530209e-05,  1.56089552e-04,\n         6.27541710e-05,  2.02723767e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-2.41070021e-04, -1.14457390e-05,  1.22934239e-04,\n         1.04068749e-04, -2.22357284e-04, -2.51200837e-04,\n        -3.12404060e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234199e-05, -1.10088540e-04,\n        -3.60184430e-05,  2.92101977e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480735e-04,  5.98594160e-05, -5.81570987e-06,\n        -5.03880314e-04,  2.89945290e-04,  5.80630835e-05,\n         2.19879662e-04, -8.13746265e-05, -2.45923755e-04,\n        -3.57251016e-04,  5.88530194e-05,  1.56089551e-04,\n         6.27541678e-05,  2.02723756e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [-2.41070021e-04, -1.14457391e-05,  1.22934239e-04,\n         1.04068750e-04, -2.22357284e-04, -2.51200836e-04,\n        -3.12404060e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234200e-05, -1.10088540e-04,\n        -3.60184427e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-2.41070021e-04, -1.14457392e-05,  1.22934239e-04,\n         1.04068750e-04, -2.22357284e-04, -2.51200836e-04,\n        -3.12404060e-04,  1.50676569e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234201e-05, -1.10088540e-04,\n        -3.60184424e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [-1.09480736e-04,  5.98594151e-05, -5.81571066e-06,\n        -5.03880312e-04,  2.89945293e-04,  5.80630857e-05,\n         2.19879663e-04, -8.13746272e-05, -2.45923755e-04,\n        -3.57251017e-04,  5.88530206e-05,  1.56089551e-04,\n         6.27541703e-05,  2.02723764e-05, -1.45221984e-04,\n        -1.10801229e-04],\n       [ 5.97228468e-05, -1.16074446e-04, -1.60641779e-04,\n         3.14309377e-04,  3.57033475e-04,  4.06701851e-04,\n         2.75570674e-04, -1.48392278e-04,  1.91067461e-04,\n        -1.95748416e-04,  1.21666796e-04,  1.15758268e-04,\n         3.53514442e-04, -1.11478449e-05, -1.43403461e-04,\n        -1.03060318e-04],\n       [-2.41070021e-04, -1.14457393e-05,  1.22934238e-04,\n         1.04068750e-04, -2.22357284e-04, -2.51200836e-04,\n        -3.12404060e-04,  1.50676569e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234203e-05, -1.10088540e-04,\n        -3.60184419e-05,  2.92101978e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [ 2.90893572e-04,  6.76785680e-05,  4.35424916e-05,\n         8.55325577e-05, -4.24751603e-04, -2.13637681e-04,\n        -1.83115351e-04,  7.91210911e-05,  4.24029887e-04,\n         1.74069896e-04, -2.46607054e-04, -1.61810421e-04,\n        -3.80356731e-04, -3.01291867e-04, -2.85474009e-05,\n        -1.86902947e-05],\n       [-2.41070021e-04, -1.14457387e-05,  1.22934239e-04,\n         1.04068749e-04, -2.22357285e-04, -2.51200837e-04,\n        -3.12404060e-04,  1.50676570e-04, -3.69078799e-04,\n         3.79000279e-04,  6.60234195e-05, -1.10088541e-04,\n        -3.60184437e-05,  2.92101977e-04,  3.17184033e-04,\n         2.32560666e-04],\n       [ 5.97228471e-05, -1.16074445e-04, -1.60641778e-04,\n         3.14309375e-04,  3.57033473e-04,  4.06701849e-04,\n         2.75570673e-04, -1.48392277e-04,  1.91067461e-04,\n        -1.95748415e-04,  1.21666795e-04,  1.15758267e-04,\n         3.53514440e-04, -1.11478457e-05, -1.43403461e-04,\n        -1.03060318e-04]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.09480736e-04,  5.98594155e-05],\n       [-5.81571038e-06, -5.03880313e-04]])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        , -0.00055932,  0.        ],\n       [ 0.        ,  0.        ,  0.        ]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [ 0., -0.,  0.],\n       [ 0.,  0.,  0.]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 1, 2, 2)"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "test_conv_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[2.42530348e-07, 1.09366583e-07],\n       [9.63602402e-08, 3.70008392e-07]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fbd7dd5ed30>"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "(64, 1, 28, 28)"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parse_input_model()\n",
    "mnist_batch_1 = x_train[0:64].reshape(64, 1, 28, 28)\n",
    "mnist_batch_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# train\n",
    "mnist_subsample_x = x_train[:9984]\n",
    "mnist_subsample_y = y_train[:9984]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:5000]\n",
    "mnist_validation_y = y_test[:5000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:]\n",
    "mnist_test_y = y_test[5001:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoc 0 batch 0 loss = 2.302532296198772\n",
      "Epoc 0 batch 1 loss = 2.3025858864948385\n",
      "Epoc 0 batch 2 loss = 2.302648214453312\n",
      "Epoc 0 batch 3 loss = 2.3024853203457747\n",
      "Epoc 0 batch 4 loss = 2.302640144174545\n",
      "Epoc 0 batch 5 loss = 2.3024702054266513\n",
      "Epoc 0 batch 6 loss = 2.3025440658683465\n",
      "Epoc 0 batch 7 loss = 2.302467885284111\n",
      "Epoc 0 batch 8 loss = 2.3026356350740254\n",
      "Epoc 0 batch 9 loss = 2.3025587321837917\n",
      "Epoc 0 batch 10 loss = 2.3026059158003456\n",
      "Epoc 0 batch 11 loss = 2.302789039378776\n",
      "Epoc 0 batch 12 loss = 2.3026724927579707\n",
      "Epoc 0 batch 13 loss = 2.3026834226779\n",
      "Epoc 0 batch 14 loss = 2.302521746670356\n",
      "Epoc 0 batch 15 loss = 2.3023209063146997\n",
      "Epoc 0 batch 16 loss = 2.3023117386542533\n",
      "Epoc 0 batch 17 loss = 2.302566159987611\n",
      "Epoc 0 batch 18 loss = 2.302169001902477\n",
      "Epoc 0 batch 19 loss = 2.302492605349512\n",
      "Epoc 0 batch 20 loss = 2.302657510111539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))\n",
    "for i in range(10):\n",
    "    for j in range(0, 9984, 64):\n",
    "        batch_x = mnist_subsample_x[j:j+64].reshape(64, 1, 28, 28)\n",
    "        batch_y = mnist_subsample_y[j:j+64]\n",
    "        model_out = batch_x\n",
    "        for layer in model:\n",
    "            # print(layer)\n",
    "            model_out = layer.forward(model_out)\n",
    "            # print(model_out.shape)\n",
    "\n",
    "        true_labels = label_binarizer.transform(batch_y)\n",
    "        l = loss_function(true_labels, model_out)\n",
    "        print(\"Epoc {} batch {} loss = {}\".format(i, j//64, l))\n",
    "\n",
    "        model_back = true_labels\n",
    "        for layer in reversed(model):\n",
    "            # print(layer)\n",
    "            model_back = layer.backward(model_back)\n",
    "            # print(model_back.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}