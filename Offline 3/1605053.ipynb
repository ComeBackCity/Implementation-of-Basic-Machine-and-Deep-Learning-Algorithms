{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0) * 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * np.sqrt(2/input_shape[1] * self.filter_dimension ** 2)\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 10e-6) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dz_prime_dim = (dz.shape[2] - 1) * self.stride + 1\n",
    "        dz_prime = np.zeros((dz.shape[0], dz.shape[1], dz_prime_dim, dz_prime_dim))\n",
    "        dz_prime[:, :, ::self.stride, ::self.stride] = dz\n",
    "\n",
    "        # calculate dw\n",
    "        for i in range(padded_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + dz_prime_dim <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + dz_prime_dim <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+dz_prime_dim, image_y:image_y+dz_prime_dim]\n",
    "                    dz_slice = dz_prime[i, :, :, :]\n",
    "                    dz_slice_shape = dz_slice.shape\n",
    "                    dz_slice = np.broadcast_to(dz_slice, (image_slice.shape[0], dz_slice_shape[0], dz_slice_shape[1], dz_slice_shape[2])).transpose((1, 0, 2, 3))\n",
    "                    dw[:, :, out_x, out_y] += np.sum(image_slice * dz_slice, axis=(2, 3))\n",
    "                    image_x += 1\n",
    "                    out_x += 1\n",
    "                image_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        rotated_filter = np.rot90(self.filters, 2, axes=(2, 3))\n",
    "        dx = np.zeros(self.input_batch.shape)\n",
    "        padding = self.filter_dimension - 1 - self.padding\n",
    "        if padding < 0:\n",
    "            dz_prime_padded = dz_prime[:, :, -padding:padding, -padding:padding]\n",
    "        else:\n",
    "            dz_prime_padded = np.pad(dz_prime, [(0, 0), (0, 0), (padding, padding), (padding, padding)], mode='constant')\n",
    "\n",
    "        dz_padded_dimensions = dz_prime_padded.shape\n",
    "\n",
    "        # calculate dx\n",
    "        for i in range(dz_padded_dimensions[0]):\n",
    "            dz_y = out_y = 0\n",
    "            while dz_y + self.filter_dimension <= dz_padded_dimensions[3]:\n",
    "                dz_x = out_x = 0\n",
    "                while dz_x + self.filter_dimension <= dz_padded_dimensions[2]:\n",
    "                    dzp_slice = dz_prime_padded[i, :, dz_x:dz_x+self.filter_dimension, dz_y:dz_y+self.filter_dimension]\n",
    "                    dzp_slice = dzp_slice.reshape(dz_padded_dimensions[1], 1, self.filter_dimension, self.filter_dimension)\n",
    "                    dx[i, :, out_x, out_y] = np.sum(dzp_slice * rotated_filter, axis=(0, 2, 3))\n",
    "                    dz_x += 1\n",
    "                    out_x += 1\n",
    "                dz_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        self.filters -= learning_rate * dw / batch_size\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        , -0.67496596,  0.        ],\n       [ 0.        ,  0.        ,  0.        ]])"
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(input_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [ 0., -0.,  0.],\n       [ 0.,  0.,  0.]])"
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    max_val = np.max(image_slice, axis=(1, 2))\n",
    "                    output[i, :, out_x, out_y] = max_val\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == max_val.reshape(input_dimensions[1], 1, 1)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.],\n       [ 0., -0.]])"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.]])"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            # self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * np.sqrt(2/flattened_input.shape[1])\n",
    "            self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * 0\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros((1, self.output_dimension))\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 0.01) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        input_matrix -= np.max(input_matrix)\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1).reshape(-1, 1)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25]])"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "data": {
      "text/plain": "1.3862943611198904"
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.75,  0.25,  0.25,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25,  0.25, -0.75,  0.25]])"
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [ 0.24620846, -0.15331187, -0.23403519, -0.57539192,  0.2116063 ,\n         0.06779583, -0.19977829,  0.10618642,  0.42891257, -0.52649464,\n         0.17567305, -0.296693  , -0.27723751, -0.12117404, -0.39549734,\n         0.14022672],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [-0.37715949,  0.16288444,  0.54643948, -0.50233602,  0.2977898 ,\n         0.24093945,  0.3663977 , -0.49106435, -0.29163012,  0.62712154,\n        -0.0327237 ,  0.31612033,  0.54363107, -0.85932417, -0.1129209 ,\n        -0.13740434],\n       [ 0.18334086,  0.21779816, -0.61442125,  0.48345543, -0.12794938,\n        -0.65678613,  0.24948269,  0.09834281,  0.2422355 , -0.31168892,\n        -0.15259337,  0.05160291, -0.48861153,  0.96308939,  0.42864002,\n         0.04402557],\n       [-0.05238983, -0.22737073,  0.30201697,  0.59427252, -0.38144672,\n         0.34805085, -0.41610209,  0.28653513, -0.37951795,  0.21106201,\n         0.00964403, -0.07103024,  0.22221797,  0.01740882,  0.07977821,\n        -0.04684795]])"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.24620846, -0.15331187],\n       [-0.23403519, -0.57539192]])"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.24620846,  0.09289659, -0.15331187],\n       [ 0.01217327, -0.71653052, -0.72870379],\n       [-0.23403519, -0.80942712, -0.57539192]])"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.24620846,  0.09289659, -0.        ],\n       [ 0.01217327, -0.        , -0.        ],\n       [-0.        , -0.        , -0.        ]])"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 1, 2, 2)"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "test_conv_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.12380341, -0.9268641 ],\n       [-0.57593114, -0.57167612]])"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fe493f1a220>"
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 1, 28, 28)"
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parse_input_model()\n",
    "mnist_batch_1 = x_train[0:32].reshape(32, 1, 28, 28)\n",
    "mnist_labels_1 = y_train[0:32]\n",
    "mnist_batch_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "# train\n",
    "random_index = random.sample(range(0, 10000), 4992)\n",
    "mnist_subsample_x = x_train[random_index]\n",
    "mnist_subsample_y = y_train[random_index]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:2000]\n",
    "mnist_validation_y = y_test[:2000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:7001]\n",
    "mnist_test_y = y_test[5001:7001]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "validation_batch = mnist_validation_x.reshape(2000, 1, 28, 28)\n",
    "validation_labels = label_binarizer.transform(mnist_validation_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [],
   "source": [
    "test_batch = mnist_test_x.reshape(2000, 1, 28, 28)\n",
    "test_labels = label_binarizer.transform(mnist_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "def measure_accuracy(y_true, y_pred):\n",
    "    accurate = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total = y_true.shape[0]\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "def predict_labels(a):\n",
    "    return (a == a.max(axis=1)[:,None]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# inp = mnist_batch_1\n",
    "# for layer in model:\n",
    "#     inp = layer.forward(inp)\n",
    "#\n",
    "# labels_true = label_binarizer.transform(mnist_labels_1)\n",
    "# l = loss_function(labels_true, inp)\n",
    "#\n",
    "# out = labels_true\n",
    "# for layer in reversed(model):\n",
    "#     out = layer.backward(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     losses = []\n",
    "#     index = [i for i in range(1,157)]\n",
    "#     for j in range(0, 4992, 32):\n",
    "#         batch_x = mnist_subsample_x[j:j+32].reshape(32, 1, 28, 28)\n",
    "#         batch_y = mnist_subsample_y[j:j+32]\n",
    "#         model_out = batch_x\n",
    "#         # train\n",
    "#         for layer in model:\n",
    "#             # print(layer)\n",
    "#             model_out = layer.forward(model_out)\n",
    "#             # print(model_out.shape)\n",
    "#\n",
    "#         true_labels = label_binarizer.transform(batch_y)\n",
    "#         l = loss_function(true_labels, model_out)\n",
    "#         losses.append(l)\n",
    "#         # print(\"Epoc {} batch {} loss = {}\".format(i, j//32, l))\n",
    "#\n",
    "#\n",
    "#         model_back = true_labels\n",
    "#         for layer in reversed(model):\n",
    "#             # print(layer)\n",
    "#             model_back = layer.backward(model_back)\n",
    "#             # print(model_back.shape)\n",
    "#\n",
    "#     plt.plot(index, losses)\n",
    "#     plt.show()\n",
    "#\n",
    "#     #validation\n",
    "#     validation_out = validation_batch\n",
    "#     for layer in model:\n",
    "#         validation_out = layer.forward(validation_out)\n",
    "#     validation_loss = loss_function(validation_labels, validation_out)\n",
    "#     print('Validation loss after epoc {} is {}'.format(i, validation_loss))\n",
    "#     validation_predictions = predict_labels(validation_out)\n",
    "#     accuracy = measure_accuracy(validation_labels, validation_predictions)\n",
    "#     print('Validation accuracy after epoc {} is {}'.format(i, accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer_toy = LabelBinarizer()\n",
    "label_binarizer_toy.fit(range(1,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "toy_model = [FullyConnectedLayerBatch(4), SoftmaxLayerBatch()]\n",
    "x_train, y_train, x_test, y_test = process_toy_dataset()\n",
    "x_validation, y_validation = x_test[:250], y_test[:250]\n",
    "x_test, y_test = x_test[250:], y_test[250:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "y_validation = label_binarizer_toy.transform(y_validation)\n",
    "y_test = label_binarizer_toy.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_losses = []\n",
    "validation_accuracy = []\n",
    "validation_index = [i for i in range(1,50001)]\n",
    "for i in range(50000):\n",
    "    losses = []\n",
    "    index = [i for i in range(1,21)]\n",
    "    for j in range(0, 500, 25):\n",
    "        batch_x = x_train[j:j+25]\n",
    "        batch_y = y_train[j:j+25]\n",
    "        model_out = batch_x\n",
    "        # train\n",
    "        for layer in toy_model:\n",
    "            model_out = layer.forward(model_out)\n",
    "\n",
    "        true_labels = label_binarizer_toy.transform(batch_y)\n",
    "        l = loss_function(true_labels, model_out)\n",
    "        losses.append(l)\n",
    "        # print(\"Epoc {} batch {} loss = {}\".format(i, j//32, l))\n",
    "\n",
    "        model_back_toy = true_labels\n",
    "        for layer in reversed(toy_model):\n",
    "            model_back_toy = layer.backward(model_back_toy)\n",
    "\n",
    "    # plt.plot(index, losses)\n",
    "    # plt.show()\n",
    "\n",
    "    #validation\n",
    "    validation_out = x_validation\n",
    "    for layer in toy_model:\n",
    "        validation_out = layer.forward(validation_out)\n",
    "    validation_loss = loss_function(y_validation, validation_out)\n",
    "    # print('Validation loss after epoc {} is {}'.format(i, validation_loss))\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_predictions = predict_labels(validation_out)\n",
    "    # accuracy = accuracy(validation_out, y_validation)\n",
    "    accuracy = measure_accuracy(y_validation, validation_predictions)\n",
    "    validation_accuracy.append(accuracy*100)\n",
    "    # print('Validation accuracy after epoc {} is {}'.format(i, accuracy))\n",
    "\n",
    "# print(validation_losses)\n",
    "plt.plot(validation_index, validation_losses)\n",
    "plt.show()\n",
    "plt.plot(validation_index, validation_accuracy)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "data": {
      "text/plain": "0.472"
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = x_test\n",
    "for layer in toy_model:\n",
    "    test_out = layer.forward(test_out)\n",
    "test_prediction = predict_labels(test_out)\n",
    "# accuracy = accuracy(validation_out, y_validation)\n",
    "accuracy = measure_accuracy(y_test, test_prediction)\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}