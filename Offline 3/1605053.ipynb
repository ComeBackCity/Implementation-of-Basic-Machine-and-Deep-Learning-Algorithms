{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "np.random.seed(120)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6, -1],\n       [ 9,  7]])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randint(-1, 10, (2, 2))\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0).astype(np.int32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * np.sqrt(2/(input_shape[1] * self.filter_dimension * self.filter_dimension))\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters, axis=(1, 2, 3)) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 1e-3) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dz_prime_dim = (dz.shape[2] - 1) * self.stride + 1\n",
    "        dz_prime = np.zeros((dz.shape[0], dz.shape[1], dz_prime_dim, dz_prime_dim))\n",
    "        dz_prime[:, :, ::self.stride, ::self.stride] = dz\n",
    "\n",
    "        # calculate dw\n",
    "        for i in range(padded_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + dz_prime_dim <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + dz_prime_dim <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+dz_prime_dim, image_y:image_y+dz_prime_dim]\n",
    "                    dz_slice = dz_prime[i, :, :, :]\n",
    "                    dz_slice_shape = dz_slice.shape\n",
    "                    dz_slice = np.broadcast_to(dz_slice, (image_slice.shape[0], dz_slice_shape[0], dz_slice_shape[1], dz_slice_shape[2])).transpose((1, 0, 2, 3))\n",
    "                    dw[:, :, out_x, out_y] += np.sum(image_slice * dz_slice, axis=(2, 3))\n",
    "                    image_x += 1\n",
    "                    out_x += 1\n",
    "                image_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        rotated_filter = np.rot90(self.filters, 2, axes=(2, 3))\n",
    "        dx = np.zeros(self.input_batch.shape)\n",
    "        padding = self.filter_dimension - 1 - self.padding\n",
    "        if padding < 0:\n",
    "            dz_prime_padded = dz_prime[:, :, -padding:padding, -padding:padding]\n",
    "        else:\n",
    "            dz_prime_padded = np.pad(dz_prime, [(0, 0), (0, 0), (padding, padding), (padding, padding)], mode='constant')\n",
    "\n",
    "        dz_padded_dimensions = dz_prime_padded.shape\n",
    "\n",
    "        # calculate dx\n",
    "        for i in range(dz_padded_dimensions[0]):\n",
    "            dz_y = out_y = 0\n",
    "            while dz_y + self.filter_dimension <= dz_padded_dimensions[3]:\n",
    "                dz_x = out_x = 0\n",
    "                while dz_x + self.filter_dimension <= dz_padded_dimensions[2]:\n",
    "                    dzp_slice = dz_prime_padded[i, :, dz_x:dz_x+self.filter_dimension, dz_y:dz_y+self.filter_dimension]\n",
    "                    dzp_slice = dzp_slice.reshape(dz_padded_dimensions[1], 1, self.filter_dimension, self.filter_dimension)\n",
    "                    dx[i, :, out_x, out_y] = np.sum(dzp_slice * rotated_filter, axis=(0, 2, 3))\n",
    "                    dz_x += 1\n",
    "                    out_x += 1\n",
    "                dz_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        self.filters -= learning_rate * dw / batch_size\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        , -6.33984923,  0.        ],\n       [ 0.        ,  0.        ,  0.        ]])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_matrix\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    def backward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(self.input_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [ 0., -0.,  0.],\n       [ 0.,  0.,  0.]])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.max(image_slice, axis=(1, 2))\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == np.max(image_slice, axis=(1, 2), keepdims=True)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[25.97456031, 25.97456031],\n       [25.97456031, 25.97456031]])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 1, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.97456031,  25.97456031,  25.97456031,  25.97456031,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.95987843,  38.95987843,  38.95987843,  38.95987843],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.48349051,  51.48349051,  51.48349051,  51.48349051,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         75.88419886,  75.88419886,  75.88419886,  75.88419886],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.12366923, 103.12366923, 103.12366923, 103.12366923,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.78603067, 151.78603067, 151.78603067, 151.78603067],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.24987482,  77.24987482,  77.24987482,  77.24987482,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        113.81456442, 113.81456442, 113.81456442, 113.81456442],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.69022593, 102.69022593, 102.69022593, 102.69022593,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.19914393, 151.19914393, 151.19914393, 151.19914393],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         50.63017963,  50.63017963,  50.63017963,  50.63017963,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         74.43870699,  74.43870699,  74.43870699,  74.43870699],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.33780319,  25.33780319,  25.33780319,  25.33780319,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.30451302,  37.30451302,  37.30451302,  37.30451302],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.82280721,  77.82280721,  77.82280721,  77.82280721,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.75905594, 114.75905594, 114.75905594, 114.75905594],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.55241593,  25.55241593,  25.55241593,  25.55241593,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.63542595,  37.63542595,  37.63542595,  37.63542595],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.57311056,  77.57311056,  77.57311056,  77.57311056,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        115.335647  , 115.335647  , 115.335647  , 115.335647  ],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.80133299,  76.80133299,  76.80133299,  76.80133299,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        113.14638816, 113.14638816, 113.14638816, 113.14638816],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.90878409,  76.90878409,  76.90878409,  76.90878409,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.38025919, 112.38025919, 112.38025919, 112.38025919],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.96540578,  76.96540578,  76.96540578,  76.96540578,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.78945909, 112.78945909, 112.78945909, 112.78945909],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.18594076,  51.18594076,  51.18594076,  51.18594076,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         76.16121195,  76.16121195,  76.16121195,  76.16121195],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.29357666,  25.29357666,  25.29357666,  25.29357666,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.33278144,  38.33278144,  38.33278144,  38.33278144],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         50.83522836,  50.83522836,  50.83522836,  50.83522836,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         74.07785337,  74.07785337,  74.07785337,  74.07785337],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.43382406,  25.43382406,  25.43382406,  25.43382406,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         36.37256885,  36.37256885,  36.37256885,  36.37256885],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.93017211,  77.93017211,  77.93017211,  77.93017211,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.89815057, 114.89815057, 114.89815057, 114.89815057],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        101.85742406, 101.85742406, 101.85742406, 101.85742406,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        148.99937623, 148.99937623, 148.99937623, 148.99937623],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.14186055,  25.14186055,  25.14186055,  25.14186055,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.44707834,  37.44707834,  37.44707834,  37.44707834],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.01282617,  26.01282617,  26.01282617,  26.01282617,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.72209436,  38.72209436,  38.72209436,  38.72209436],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.23340602, 103.23340602, 103.23340602, 103.23340602,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.69179934, 151.69179934, 151.69179934, 151.69179934],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.36870771,  26.36870771,  26.36870771,  26.36870771,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.82935721,  38.82935721,  38.82935721,  38.82935721],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.60410268,  26.60410268,  26.60410268,  26.60410268,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         39.08134678,  39.08134678,  39.08134678,  39.08134678],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.79306759, 103.79306759, 103.79306759, 103.79306759,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        153.27149018, 153.27149018, 153.27149018, 153.27149018],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         24.92205808,  24.92205808,  24.92205808,  24.92205808,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         36.03780327,  36.03780327,  36.03780327,  36.03780327],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.58776074, 103.58776074, 103.58776074, 103.58776074,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        152.56965228, 152.56965228, 152.56965228, 152.56965228],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.69304089,  25.69304089,  25.69304089,  25.69304089,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.78374544,  37.78374544,  37.78374544,  37.78374544],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.83347051,  51.83347051,  51.83347051,  51.83347051,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         76.1209857 ,  76.1209857 ,  76.1209857 ,  76.1209857 ],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.4610109 ,  77.4610109 ,  77.4610109 ,  77.4610109 ,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.89108624, 112.89108624, 112.89108624, 112.89108624],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.16054177,  77.16054177,  77.16054177,  77.16054177,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.65319492, 112.65319492, 112.65319492, 112.65319492],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.88300575,  77.88300575,  77.88300575,  77.88300575,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.54661232, 114.54661232, 114.54661232, 114.54661232],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.66378676,  77.66378676,  77.66378676,  77.66378676,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.01694212, 114.01694212, 114.01694212, 114.01694212],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         52.82206911,  52.82206911,  52.82206911,  52.82206911,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         78.02859262,  78.02859262,  78.02859262,  78.02859262],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.58698607,  25.58698607,  25.58698607,  25.58698607,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.80189349,  37.80189349,  37.80189349,  37.80189349],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.59501988, 102.59501988, 102.59501988, 102.59501988,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.33202437, 150.33202437, 150.33202437, 150.33202437],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.17406697,  26.17406697,  26.17406697,  26.17406697,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.86604643,  38.86604643,  38.86604643,  38.86604643],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.71149408,  77.71149408,  77.71149408,  77.71149408,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        114.17581935, 114.17581935, 114.17581935, 114.17581935],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.89250028, 102.89250028, 102.89250028, 102.89250028,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.12686045, 151.12686045, 151.12686045, 151.12686045],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.77252229,  26.77252229,  26.77252229,  26.77252229,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         39.26081147,  39.26081147,  39.26081147,  39.26081147],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.55661109, 102.55661109, 102.55661109, 102.55661109,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.43799466, 150.43799466, 150.43799466, 150.43799466],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         25.83942619,  25.83942619,  25.83942619,  25.83942619,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         37.23077925,  37.23077925,  37.23077925,  37.23077925],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.05835601, 103.05835601, 103.05835601, 103.05835601,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        152.4055621 , 152.4055621 , 152.4055621 , 152.4055621 ],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.76962261, 102.76962261, 102.76962261, 102.76962261,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.59642139, 150.59642139, 150.59642139, 150.59642139],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         26.435151  ,  26.435151  ,  26.435151  ,  26.435151  ,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         38.69273213,  38.69273213,  38.69273213,  38.69273213],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         77.51450566,  77.51450566,  77.51450566,  77.51450566,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        113.72373158, 113.72373158, 113.72373158, 113.72373158],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        103.12257936, 103.12257936, 103.12257936, 103.12257936,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        151.45159925, 151.45159925, 151.45159925, 151.45159925],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         51.76629409,  51.76629409,  51.76629409,  51.76629409,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n         76.30137878,  76.30137878,  76.30137878,  76.30137878],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n        102.51169287, 102.51169287, 102.51169287, 102.51169287,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        150.58005653, 150.58005653, 150.58005653, 150.58005653],\n       [  0.        ,   0.        ,   0.        ,  -0.        ,\n         76.5940859 ,  76.5940859 ,  76.5940859 ,  76.5940859 ,\n          0.        ,   0.        ,   0.        ,  -0.        ,\n        112.72757775, 112.72757775, 112.72757775, 112.72757775]])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * np.sqrt(2/flattened_input.shape[1])\n",
    "            # self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * 0.01\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros((1, self.output_dimension))\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 1e-3) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 6.46683713e+01,  1.23203857e+01, -3.75496823e-01,\n        -1.80154893e+01],\n       [ 1.26509648e+02,  2.37432235e+01, -2.96171882e-01,\n        -3.46999234e+01],\n       [ 2.53138252e+02,  4.74507553e+01, -5.21760363e-01,\n        -6.93446301e+01],\n       [ 1.89765083e+02,  3.56018814e+01, -4.28297350e-01,\n        -5.20302655e+01],\n       [ 2.52138002e+02,  4.72771688e+01, -5.36688497e-01,\n        -6.90916836e+01],\n       [ 1.24178636e+02,  2.32546770e+01, -2.28345419e-01,\n        -3.39832406e+01],\n       [ 6.22095928e+01,  1.16639676e+01, -1.31622183e-01,\n        -1.70458926e+01],\n       [ 1.91297693e+02,  3.59167224e+01, -4.65109771e-01,\n        -5.24918270e+01],\n       [ 6.27551483e+01,  1.17703235e+01, -1.37744039e-01,\n        -1.72015370e+01],\n       [ 1.91862356e+02,  3.62796149e+01, -7.80228091e-01,\n        -5.30357081e+01],\n       [ 1.88654098e+02,  3.53914560e+01, -4.23355461e-01,\n        -5.17226349e+01],\n       [ 1.87764999e+02,  3.49731472e+01, -1.14162979e-01,\n        -5.10980529e+01],\n       [ 1.88310433e+02,  3.51641078e+01, -2.23647897e-01,\n        -5.13817996e+01],\n       [ 1.26671035e+02,  2.39681349e+01, -5.34259922e-01,\n        -3.50388700e+01],\n       [ 6.34647832e+01,  1.21969343e+01, -4.97794878e-01,\n        -1.78404623e+01],\n       [ 1.23855432e+02,  2.30136752e+01, -7.31899378e-03,\n        -3.36215302e+01],\n       [ 6.11065994e+01,  1.11649997e+01,  2.27556449e-01,\n        -1.63012970e+01],\n       [ 1.91537626e+02,  3.59565428e+01, -4.59308035e-01,\n        -5.25497489e+01],\n       [ 2.48878874e+02,  4.64010677e+01, -2.06085214e-01,\n        -6.77973699e+01],\n       [ 6.22661601e+01,  1.17919078e+01, -2.75052785e-01,\n        -1.72390496e+01],\n       [ 6.43954632e+01,  1.21891550e+01, -2.77132512e-01,\n        -1.78194878e+01],\n       [ 2.53088625e+02,  4.73718216e+01, -4.36611064e-01,\n        -6.92256058e+01],\n       [ 6.47494808e+01,  1.21421041e+01, -1.39342306e-01,\n        -1.77447494e+01],\n       [ 6.52094558e+01,  1.22026019e+01, -1.08870719e-01,\n        -1.78318051e+01],\n       [ 2.55405306e+02,  4.80118952e+01, -6.92763125e-01,\n        -7.01718593e+01],\n       [ 6.03723524e+01,  1.11413051e+01,  8.99044952e-02,\n        -1.62726756e+01],\n       [ 2.54402855e+02,  4.77151763e+01, -5.57797973e-01,\n        -6.97324983e+01],\n       [ 6.30271695e+01,  1.18053419e+01, -1.18796674e-01,\n        -1.72518710e+01],\n       [ 1.27021569e+02,  2.37634009e+01, -2.04667371e-01,\n        -3.47254161e+01],\n       [ 1.88743886e+02,  3.50744201e+01, -1.57585820e-02,\n        -5.12417195e+01],\n       [ 1.88261226e+02,  3.50395861e+01, -8.27226582e-02,\n        -5.11937459e+01],\n       [ 1.91069964e+02,  3.57920706e+01, -3.64530140e-01,\n        -5.23053402e+01],\n       [ 1.90273648e+02,  3.55864416e+01, -2.94051602e-01,\n        -5.20018628e+01],\n       [ 1.30012696e+02,  2.44472875e+01, -3.61307759e-01,\n        -3.57313433e+01],\n       [ 6.29841761e+01,  1.18447239e+01, -1.76652304e-01,\n        -1.73119247e+01],\n       [ 2.50997528e+02,  4.68654562e+01, -2.92586721e-01,\n        -6.84795728e+01],\n       [ 6.46747945e+01,  1.22160930e+01, -2.46657307e-01,\n        -1.78575150e+01],\n       [ 1.90501338e+02,  3.56532602e+01, -3.24002953e-01,\n        -5.21007839e+01],\n       [ 2.52173016e+02,  4.71829931e+01, -4.13718947e-01,\n        -6.89487450e+01],\n       [ 6.55375244e+01,  1.22454676e+01, -8.67915463e-02,\n        -1.78934671e+01],\n       [ 2.51105937e+02,  4.69300208e+01, -3.46848807e-01,\n        -6.85762598e+01],\n       [ 6.24280420e+01,  1.14838219e+01,  1.37969994e-01,\n        -1.67709731e+01],\n       [ 2.53870576e+02,  4.77828775e+01, -7.61255719e-01,\n        -6.98402648e+01],\n       [ 2.51435359e+02,  4.69495429e+01, -2.95950661e-01,\n        -6.86025636e+01],\n       [ 6.46201852e+01,  1.20540435e+01, -6.11215827e-02,\n        -1.76126933e+01],\n       [ 1.89815548e+02,  3.54805632e+01, -2.68670430e-01,\n        -5.18460773e+01],\n       [ 2.52720441e+02,  4.72818206e+01, -4.10221407e-01,\n        -6.90929721e+01],\n       [ 1.27205002e+02,  2.38738198e+01, -2.97913050e-01,\n        -3.48907904e+01],\n       [ 2.51255334e+02,  4.70147038e+01, -4.16383302e-01,\n        -6.87030032e+01],\n       [ 1.88003466e+02,  3.52385208e+01, -3.84182581e-01,\n        -5.14975023e+01]])"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        # input_matrix -= np.max(input_matrix, axis=1).reshape(-1, 1)\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1, keepdims=True)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.00000000e+000, 1.84314163e-023, 5.64685974e-029,\n        1.23269487e-036],\n       [1.00000000e+000, 2.33942433e-045, 8.49047979e-056,\n        9.71790841e-071],\n       [1.00000000e+000, 4.68873005e-090, 6.86841274e-111,\n        8.86061433e-141],\n       [1.00000000e+000, 1.11627731e-067, 2.51225814e-083,\n        9.76369616e-106],\n       [1.00000000e+000, 1.07169304e-089, 1.83982295e-110,\n        3.10255826e-140],\n       [1.00000000e+000, 1.47666408e-044, 9.34834377e-055,\n        2.04724560e-069],\n       [1.00000000e+000, 1.11767075e-022, 8.42469773e-028,\n        3.79996882e-035],\n       [1.00000000e+000, 3.30295006e-068, 5.22965534e-084,\n        1.32909758e-106],\n       [1.00000000e+000, 7.20397052e-023, 4.85248163e-028,\n        1.88474584e-035],\n       [1.00000000e+000, 2.69944302e-068, 2.16964584e-084,\n        4.38654131e-107],\n       [1.00000000e+000, 2.74713674e-067, 7.66840907e-083,\n        4.03377146e-105],\n       [1.00000000e+000, 4.39887701e-067, 2.54165507e-082,\n        1.83271335e-104],\n       [1.00000000e+000, 3.08600073e-067, 1.32034977e-082,\n        7.99808492e-105],\n       [1.00000000e+000, 2.49285680e-045, 5.69432796e-056,\n        5.89224381e-071],\n       [1.00000000e+000, 5.42820377e-023, 1.66496574e-028,\n        4.89305981e-036],\n       [1.00000000e+000, 1.60317646e-044, 1.61098159e-054,\n        4.06091730e-069],\n       [1.00000000e+000, 2.04474352e-022, 3.63552891e-027,\n        2.41089207e-034],\n       [1.00000000e+000, 2.70392270e-068, 4.13800693e-084,\n        9.86734229e-107],\n       [1.00000000e+000, 1.16146518e-088, 6.66464269e-109,\n        2.94615045e-138],\n       [1.00000000e+000, 1.20035782e-022, 6.89757343e-028,\n        2.96023468e-035],\n       [1.00000000e+000, 2.12367683e-023, 8.18556297e-029,\n        1.97015531e-036],\n       [1.00000000e+000, 4.55331146e-090, 7.85939072e-111,\n        1.04883713e-140],\n       [1.00000000e+000, 1.42202288e-023, 6.59389753e-029,\n        1.49008500e-036],\n       [1.00000000e+000, 9.53708932e-024, 4.29152096e-029,\n        8.62260823e-037],\n       [1.00000000e+000, 8.51502825e-091, 5.99821088e-112,\n        4.01452017e-142],\n       [1.00000000e+000, 4.16128847e-022, 6.60184814e-027,\n        5.16996139e-034],\n       [1.00000000e+000, 1.72457391e-090, 1.87066252e-111,\n        1.69748059e-141],\n       [1.00000000e+000, 5.68385394e-023, 3.76751793e-028,\n        1.36538677e-035],\n       [1.00000000e+000, 1.43069665e-045, 5.57632087e-056,\n        5.67776040e-071],\n       [1.00000000e+000, 1.82893820e-067, 1.05372686e-082,\n        5.96452559e-105],\n       [1.00000000e+000, 2.86211649e-067, 1.59684371e-082,\n        1.01397459e-104],\n       [1.00000000e+000, 3.66157258e-068, 7.26198685e-084,\n        2.01116753e-106],\n       [1.00000000e+000, 6.60997279e-068, 1.72782385e-083,\n        6.04063211e-106],\n       [1.00000000e+000, 1.42405059e-046, 2.39491115e-057,\n        1.04298520e-072],\n       [1.00000000e+000, 6.17188912e-023, 3.71193828e-028,\n        1.34229047e-035],\n       [1.00000000e+000, 2.22109650e-089, 7.34665437e-110,\n        1.79001968e-139],\n       [1.00000000e+000, 1.64996761e-023, 6.38221912e-029,\n        1.43441023e-036],\n       [1.00000000e+000, 5.62774072e-068, 1.33538728e-083,\n        4.35749569e-106],\n       [1.00000000e+000, 9.41812169e-090, 2.00897666e-110,\n        3.45613939e-140],\n       [1.00000000e+000, 7.17057217e-024, 3.16025188e-029,\n        5.83956851e-037],\n       [1.00000000e+000, 2.12581927e-089, 6.24371013e-110,\n        1.45809566e-139],\n       [1.00000000e+000, 7.50250558e-023, 8.86677046e-028,\n        4.02072890e-035],\n       [1.00000000e+000, 3.14231606e-090, 2.59896641e-111,\n        2.59518422e-141],\n       [1.00000000e+000, 1.55933076e-089, 4.72586256e-110,\n        1.02163562e-139],\n       [1.00000000e+000, 1.48188581e-023, 8.11456769e-029,\n        1.93515005e-036],\n       [1.00000000e+000, 9.40086390e-068, 2.80203046e-083,\n        1.11606540e-105],\n       [1.00000000e+000, 6.01368507e-090, 1.16613813e-110,\n        1.73065449e-140],\n       [1.00000000e+000, 1.32995704e-045, 4.22851052e-056,\n        4.00582240e-071],\n       [1.00000000e+000, 1.99260535e-089, 5.01605066e-110,\n        1.10626634e-139],\n       [1.00000000e+000, 4.51884229e-067, 1.52856424e-082,\n        9.68391572e-105]])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "151.16192651773153"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.00000000e+000,  1.84314163e-023,  5.64685974e-029,\n         1.23269487e-036],\n       [ 1.00000000e+000, -1.00000000e+000,  8.49047979e-056,\n         9.71790841e-071],\n       [ 1.00000000e+000,  4.68873005e-090,  6.86841274e-111,\n        -1.00000000e+000],\n       [ 1.00000000e+000,  1.11627731e-067, -1.00000000e+000,\n         9.76369616e-106],\n       [ 1.00000000e+000,  1.07169304e-089,  1.83982295e-110,\n        -1.00000000e+000],\n       [ 1.00000000e+000, -1.00000000e+000,  9.34834377e-055,\n         2.04724560e-069],\n       [ 0.00000000e+000,  1.11767075e-022,  8.42469773e-028,\n         3.79996882e-035],\n       [ 1.00000000e+000,  3.30295006e-068, -1.00000000e+000,\n         1.32909758e-106],\n       [ 0.00000000e+000,  7.20397052e-023,  4.85248163e-028,\n         1.88474584e-035],\n       [ 1.00000000e+000,  2.69944302e-068, -1.00000000e+000,\n         4.38654131e-107],\n       [ 1.00000000e+000,  2.74713674e-067, -1.00000000e+000,\n         4.03377146e-105],\n       [ 1.00000000e+000,  4.39887701e-067, -1.00000000e+000,\n         1.83271335e-104],\n       [ 1.00000000e+000,  3.08600073e-067, -1.00000000e+000,\n         7.99808492e-105],\n       [ 1.00000000e+000, -1.00000000e+000,  5.69432796e-056,\n         5.89224381e-071],\n       [ 0.00000000e+000,  5.42820377e-023,  1.66496574e-028,\n         4.89305981e-036],\n       [ 1.00000000e+000, -1.00000000e+000,  1.61098159e-054,\n         4.06091730e-069],\n       [ 0.00000000e+000,  2.04474352e-022,  3.63552891e-027,\n         2.41089207e-034],\n       [ 1.00000000e+000,  2.70392270e-068, -1.00000000e+000,\n         9.86734229e-107],\n       [ 1.00000000e+000,  1.16146518e-088,  6.66464269e-109,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.20035782e-022,  6.89757343e-028,\n         2.96023468e-035],\n       [ 0.00000000e+000,  2.12367683e-023,  8.18556297e-029,\n         1.97015531e-036],\n       [ 1.00000000e+000,  4.55331146e-090,  7.85939072e-111,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.42202288e-023,  6.59389753e-029,\n         1.49008500e-036],\n       [ 0.00000000e+000,  9.53708932e-024,  4.29152096e-029,\n         8.62260823e-037],\n       [ 1.00000000e+000,  8.51502825e-091,  5.99821088e-112,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  4.16128847e-022,  6.60184814e-027,\n         5.16996139e-034],\n       [ 1.00000000e+000,  1.72457391e-090,  1.87066252e-111,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  5.68385394e-023,  3.76751793e-028,\n         1.36538677e-035],\n       [ 1.00000000e+000, -1.00000000e+000,  5.57632087e-056,\n         5.67776040e-071],\n       [ 1.00000000e+000,  1.82893820e-067, -1.00000000e+000,\n         5.96452559e-105],\n       [ 1.00000000e+000,  2.86211649e-067, -1.00000000e+000,\n         1.01397459e-104],\n       [ 1.00000000e+000,  3.66157258e-068, -1.00000000e+000,\n         2.01116753e-106],\n       [ 1.00000000e+000,  6.60997279e-068, -1.00000000e+000,\n         6.04063211e-106],\n       [ 1.00000000e+000, -1.00000000e+000,  2.39491115e-057,\n         1.04298520e-072],\n       [ 0.00000000e+000,  6.17188912e-023,  3.71193828e-028,\n         1.34229047e-035],\n       [ 1.00000000e+000,  2.22109650e-089,  7.34665437e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.64996761e-023,  6.38221912e-029,\n         1.43441023e-036],\n       [ 1.00000000e+000,  5.62774072e-068, -1.00000000e+000,\n         4.35749569e-106],\n       [ 1.00000000e+000,  9.41812169e-090,  2.00897666e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  7.17057217e-024,  3.16025188e-029,\n         5.83956851e-037],\n       [ 1.00000000e+000,  2.12581927e-089,  6.24371013e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  7.50250558e-023,  8.86677046e-028,\n         4.02072890e-035],\n       [ 1.00000000e+000,  3.14231606e-090,  2.59896641e-111,\n        -1.00000000e+000],\n       [ 1.00000000e+000,  1.55933076e-089,  4.72586256e-110,\n        -1.00000000e+000],\n       [ 0.00000000e+000,  1.48188581e-023,  8.11456769e-029,\n         1.93515005e-036],\n       [ 1.00000000e+000,  9.40086390e-068, -1.00000000e+000,\n         1.11606540e-105],\n       [ 1.00000000e+000,  6.01368507e-090,  1.16613813e-110,\n        -1.00000000e+000],\n       [ 1.00000000e+000, -1.00000000e+000,  4.22851052e-056,\n         4.00582240e-071],\n       [ 1.00000000e+000,  1.99260535e-089,  5.01605066e-110,\n        -1.00000000e+000],\n       [ 1.00000000e+000,  4.51884229e-067, -1.00000000e+000,\n         9.68391572e-105]])"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 1.20044931e-24,  1.52950073e-24, -5.56178809e-24,\n        -5.87148094e-24,  4.47679910e-24, -2.96856391e-24,\n        -3.97999500e-24, -2.77634886e-24, -5.66841387e-24,\n        -1.52833991e-24,  3.24762342e-24, -2.84228294e-24,\n        -7.53772731e-25,  8.62944213e-24, -2.24583386e-25,\n         1.67644442e-24],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 7.27973479e-24,  9.27460654e-24, -3.37264250e-23,\n        -3.56043633e-23,  2.71470474e-23, -1.80009992e-23,\n        -2.41342182e-23, -1.68358227e-23, -3.43728711e-23,\n        -9.26765133e-24,  1.96932268e-23, -1.72352955e-23,\n        -4.57097408e-24,  5.23286139e-23, -1.36184660e-24,\n         1.01656609e-23],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [ 4.69213642e-24,  5.97799041e-24, -2.17384324e-23,\n        -2.29488641e-23,  1.74976882e-23, -1.16026059e-23,\n        -1.55557901e-23, -1.08515422e-23, -2.21551153e-23,\n        -5.97349771e-24,  1.26933318e-23, -1.11090634e-23,\n        -2.94621502e-24,  3.37285003e-23, -8.77782783e-25,\n         6.55232184e-24],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 3.53542211e-24,  4.50450542e-24, -1.63799236e-23,\n        -1.72919945e-23,  1.31845417e-23, -8.74266492e-24,\n        -1.17214127e-23, -8.17657592e-24, -1.66939453e-23,\n        -4.50108674e-24,  9.56451814e-24, -8.37075708e-24,\n        -2.21992277e-24,  2.54144172e-23, -6.61416547e-25,\n         4.93726667e-24],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 1.33192081e-23,  1.69667629e-23, -6.17016595e-23,\n        -6.51371977e-23,  4.96646223e-23, -3.29313995e-23,\n        -4.41517533e-23, -3.08014415e-23, -6.28837819e-23,\n        -1.69543902e-23,  3.60273874e-23, -3.15308076e-23,\n        -8.36303341e-24,  9.57342266e-23, -2.49139500e-24,\n         1.85969025e-23],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 7.81818183e-24,  9.96084276e-24, -3.62215365e-23,\n        -3.82384170e-23,  2.91554341e-23, -1.93328303e-23,\n        -2.59198129e-23, -1.80812782e-23, -3.69158621e-23,\n        -9.95333689e-24,  2.11502403e-23, -1.85104541e-23,\n        -4.90908246e-24,  5.61998990e-23, -1.46260440e-24,\n         1.09178204e-23],\n       [ 1.38317290e-24,  1.76229152e-24, -6.40832132e-24,\n        -6.76515022e-24,  5.15818950e-24, -3.42038698e-24,\n        -4.58576142e-24, -3.19892945e-24, -6.53117182e-24,\n        -1.76095683e-24,  3.74192160e-24, -3.27488734e-24,\n        -8.68505289e-25,  9.94289093e-24, -2.58765593e-25,\n         1.93160063e-24],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 9.26184528e-25,  1.18003331e-24, -4.29103993e-24,\n        -4.52997356e-24,  3.45394499e-24, -2.29030083e-24,\n        -3.07063948e-24, -2.14202095e-24, -4.37329831e-24,\n        -1.17914147e-24,  2.50560234e-24, -2.19287493e-24,\n        -5.81557852e-25,  6.65780488e-24, -1.73270201e-25,\n         1.29340331e-24],\n       [ 6.21164004e-25,  7.91414168e-25, -2.87787417e-24,\n        -3.03811999e-24,  2.31645938e-24, -1.53603796e-24,\n        -2.05938826e-24, -1.43658995e-24, -2.93304278e-24,\n        -7.90815816e-25,  1.68043429e-24, -1.47069713e-24,\n        -3.90033389e-25,  4.46519356e-24, -1.16207267e-25,\n         8.67448103e-25],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 2.71056797e-23,  3.45296349e-23, -1.25569895e-22,\n        -1.32561646e-22,  1.01073242e-22, -6.70195176e-23,\n        -8.98542971e-23, -6.26841626e-23, -1.27975852e-22,\n        -3.45043212e-23,  7.33201674e-23, -6.41690610e-23,\n        -1.70195151e-23,  1.94829917e-22, -5.07029869e-24,\n         3.78471629e-23],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 3.70204093e-24,  4.71657149e-24, -1.71513848e-23,\n        -1.81064024e-23,  1.38054848e-23, -9.15433169e-24,\n        -1.22733513e-23, -8.56174569e-24, -1.74801445e-23,\n        -4.71302578e-24,  1.00149021e-23, -8.76493134e-24,\n        -2.32452974e-24,  2.66114159e-23, -6.92561191e-25,\n         5.16971276e-24],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [ 4.01988982e-24,  5.12156765e-24, -1.86240574e-23,\n        -1.96610777e-23,  1.49908719e-23, -9.94036959e-24,\n        -1.33272021e-23, -9.29687144e-24, -1.89810551e-23,\n        -5.11771113e-24,  1.08748296e-23, -9.51752967e-24,\n        -2.52411133e-24,  2.88963511e-23, -7.52027965e-25,\n         5.61361748e-24],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 1.07464126e-24,  1.36919313e-24, -4.97887556e-24,\n        -5.25610987e-24,  4.00759921e-24, -2.65743236e-24,\n        -3.56285733e-24, -2.48537352e-24, -5.07432286e-24,\n        -1.36815620e-24,  2.90724520e-24, -2.54438802e-24,\n        -6.74775798e-25,  7.72502101e-24, -2.01045106e-25,\n         1.50073600e-24],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 4.67029055e-25,  5.95034266e-25, -2.16376329e-24,\n        -2.28424601e-24,  1.74165710e-24, -1.15488839e-24,\n        -1.54837549e-24, -1.08011670e-24, -2.20524262e-24,\n        -5.94584277e-25,  1.26345442e-24, -1.10576104e-24,\n        -2.93250983e-25,  3.35720780e-24, -8.73718121e-26,\n         6.52201275e-25],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 4.88679179e-24,  6.22556781e-24, -2.26393217e-23,\n        -2.38998975e-23,  1.82227899e-23, -1.20832649e-23,\n        -1.62002381e-23, -1.13013856e-23, -2.30731754e-23,\n        -6.22095296e-24,  1.32192110e-23, -1.15693133e-23,\n        -3.06841282e-24,  3.51263436e-23, -9.14147382e-25,\n         6.82369676e-24],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [ 9.65181037e-25,  1.22970422e-24, -4.47168127e-24,\n        -4.72067276e-24,  3.59934551e-24, -2.38671056e-24,\n        -3.19989809e-24, -2.23219860e-24, -4.55739934e-24,\n        -1.22877693e-24,  2.61107628e-24, -2.28518474e-24,\n        -6.06043149e-25,  6.93808264e-24, -1.80564001e-25,\n         1.34784666e-24],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-6.18146105e-01, -2.47626759e-01,  3.92084334e-01,\n         3.09761976e-03,  3.38368866e-02,  4.33790349e-01,\n         5.67855248e-01, -1.31918071e-01,  2.55641470e-02,\n        -1.63439918e-01,  6.34707680e-02,  1.80699667e-01,\n         7.40833116e-01, -1.20094500e-01,  2.24574791e-01,\n        -1.04081698e-01],\n       [-8.41547872e-01, -2.18788056e-01,  3.53759733e-01,\n        -7.84049321e-01,  5.34411712e-01,  6.63296410e-01,\n         2.57745782e-02, -1.04189788e+00, -6.83017325e-01,\n         3.22566295e-01,  4.48923639e-03,  2.65832026e-01,\n         7.47671269e-01,  5.55504822e-01,  6.02118562e-01,\n         9.59250065e-02],\n       [-1.10789425e+00,  2.35605769e-01,  1.97764732e-01,\n        -2.51459043e-01,  2.99515860e-01, -1.39220849e-01,\n        -1.44831301e-01,  1.15180073e-01, -4.27808454e-01,\n        -4.78709452e-01,  5.98772229e-01, -2.71344442e-01,\n         9.76886469e-01,  2.71128928e-02,  1.83471163e-01,\n         3.94938245e-01]])"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.20044931e-24,  1.52950073e-24],\n       [-5.56178809e-24, -5.87148094e-24]])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.20044931e-24,  2.72995004e-24,  1.52950073e-24],\n       [-4.36133878e-24, -8.70331900e-24, -4.34198022e-24],\n       [-5.56178809e-24, -1.14332690e-23, -5.87148094e-24]])"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [-0., -0., -0.],\n       [-0., -0., -0.]])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 1, 2, 2)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "test_conv_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1.60687076e-24, -2.44012914e-24],\n       [ 1.62296586e-23, -7.61966575e-27]])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Predictiona and Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "def measure_accuracy(y_true, y_pred):\n",
    "    accurate = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total = y_true.shape[0]\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "def predict_labels(a):\n",
    "    return (a == a.max(axis=1)[:,None]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7faff07a4280>"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mnist Dataset Train-test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "model = parse_input_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "# train\n",
    "random_index = random.sample(range(0, 60000), 6400)\n",
    "mnist_subsample_x = x_train[random_index]\n",
    "mnist_subsample_y = y_train[random_index]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:2000]\n",
    "mnist_validation_y = y_test[:2000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:7001]\n",
    "mnist_test_y = y_test[5001:7001]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "validation_batch = mnist_validation_x.reshape(2000, 1, 28, 28) / 255.0\n",
    "validation_labels = label_binarizer.transform(mnist_validation_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "test_batch = mnist_test_x.reshape(2000, 1, 28, 28) / 255.0\n",
    "test_labels = label_binarizer.transform(mnist_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_losses = []\n",
    "validation_accuracy = []\n",
    "validation_index = [i for i in range(1,51)]\n",
    "for i in range(50):\n",
    "    for j in range(0, 6400, 32):\n",
    "        batch_x = mnist_subsample_x[j:j+32].reshape(32, 1, 28, 28).astype(np.float64)\n",
    "        batch_x /= 255.0\n",
    "        batch_y = mnist_subsample_y[j:j+32]\n",
    "        model_out = batch_x\n",
    "        # train\n",
    "        for layer in model:\n",
    "            model_out = layer.forward(model_out)\n",
    "\n",
    "        true_labels = label_binarizer.transform(batch_y)\n",
    "        l = loss_function(true_labels, model_out)\n",
    "\n",
    "        model_back = true_labels\n",
    "        for layer in reversed(model):\n",
    "            model_back = layer.backward(model_back)\n",
    "\n",
    "    #validation\n",
    "    validation_out = validation_batch\n",
    "    for layer in model:\n",
    "        validation_out = layer.forward(validation_out)\n",
    "    validation_loss = loss_function(validation_labels, validation_out)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_predictions = predict_labels(validation_out)\n",
    "    accuracy = measure_accuracy(validation_labels, validation_predictions)\n",
    "    validation_accuracy.append(accuracy * 100)\n",
    "\n",
    "plt.plot(validation_index, validation_losses)\n",
    "plt.show()\n",
    "plt.plot(validation_index, validation_accuracy)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_out = test_batch\n",
    "for layer in model:\n",
    "    test_out = layer.forward(test_out)\n",
    "test_prediction = predict_labels(test_out)\n",
    "accuracy = measure_accuracy(test_labels, test_prediction)\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Toy Dataset Train-test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# label_binarizer_toy = LabelBinarizer()\n",
    "# label_binarizer_toy.fit(range(1,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# toy_model = [FullyConnectedLayerBatch(4), SoftmaxLayerBatch()]\n",
    "# x_train, y_train, x_test, y_test = process_toy_dataset()\n",
    "# x_validation, y_validation = x_test[:250], y_test[:250]\n",
    "# x_test, y_test = x_test[250:], y_test[250:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y_validation = label_binarizer_toy.transform(y_validation)\n",
    "# y_test = label_binarizer_toy.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# validation_losses = []\n",
    "# validation_accuracy = []\n",
    "# validation_index = [i for i in range(1,30001)]\n",
    "# for i in range(30000):\n",
    "#     for j in range(0, 500, 25):\n",
    "#         batch_x = x_train[j:j+25]\n",
    "#         batch_y = y_train[j:j+25]\n",
    "#         model_out = batch_x\n",
    "#         # train\n",
    "#         for layer in toy_model:\n",
    "#             model_out = layer.forward(model_out)\n",
    "#\n",
    "#         true_labels = label_binarizer_toy.transform(batch_y)\n",
    "#         l = loss_function(true_labels, model_out)\n",
    "#\n",
    "#         model_back_toy = true_labels\n",
    "#         for layer in reversed(toy_model):\n",
    "#             model_back_toy = layer.backward(model_back_toy)\n",
    "#\n",
    "#     #validation\n",
    "#     validation_out = x_validation\n",
    "#     for layer in toy_model:\n",
    "#         validation_out = layer.forward(validation_out)\n",
    "#     validation_loss = loss_function(y_validation, validation_out)\n",
    "#     validation_losses.append(validation_loss)\n",
    "#     validation_predictions = predict_labels(validation_out)\n",
    "#     accuracy = measure_accuracy(y_validation, validation_predictions)\n",
    "#     validation_accuracy.append(accuracy*100)\n",
    "#\n",
    "# # print(validation_losses)\n",
    "# plt.plot(validation_index, validation_losses)\n",
    "# plt.show()\n",
    "# plt.plot(validation_index, validation_accuracy)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_out = x_test\n",
    "# for layer in toy_model:\n",
    "#     test_out = layer.forward(test_out)\n",
    "# test_prediction = predict_labels(test_out)\n",
    "# accuracy = measure_accuracy(y_test, test_prediction)\n",
    "# accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}