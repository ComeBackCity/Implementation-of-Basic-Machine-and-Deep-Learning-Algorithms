{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayer(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayer(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayer())\n",
    "                model.append(FullyConnectedLayer(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayer())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        # for i in range(self.output_channel_count):\n",
    "        #     image_y = out_y = 0\n",
    "        #     while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "        #         image_x = out_x = 0\n",
    "        #         while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "        #             image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "        #             output[out_x, out_y, i] = np.sum(image_slice * filters[i, :, :, :]) + bias[i]\n",
    "        #             image_x += self.stride\n",
    "        #             out_x += 1\n",
    "        #         image_y += self.stride\n",
    "        #         out_y += 1\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[1] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[3],\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[2]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[1]:\n",
    "                    image_slice = padded_image[i, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                    output[i, out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray):\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f8fe6e81790>"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.50685553,  0.66834963,  0.89596803,  1.13410289,  2.25748146,\n         3.86244971,  5.74042395,  6.55064158,  7.71019649,  8.85131378,\n        10.23603885, 10.47954221, 10.57472602,  8.29567262,  4.94502718,\n         1.9181084 ,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.72990762,  1.15523747,  2.1069094 ,  3.80302558,\n         5.90741127,  8.61573426, 11.46799282, 13.64181038, 16.40455942,\n        18.6182604 , 19.9573516 , 19.37386418, 21.63381383, 22.95591318,\n        23.09389221, 20.63519293, 19.5892588 , 15.43091809,  9.32774825,\n         3.29102396,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.8868702 ,  3.40509309,  6.86801171, 11.20662856, 16.09167599,\n        20.29766615, 23.29768117, 26.15116184, 28.36399083, 31.63393809,\n        34.60323475, 34.99629733, 31.30154107, 31.18879488, 30.16530298,\n        28.38000552, 24.61476717, 20.98214531, 15.05532234,  8.45084065,\n         2.70498427,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         1.19119046,  6.12649828, 11.87025677, 19.16499519, 28.01234313,\n        36.08241419, 39.66589713, 41.75930532, 42.83402705, 44.17794103,\n        46.5214866 , 43.77456408, 39.4534508 , 35.97369958, 31.44504311,\n        26.71676429, 22.97253038, 19.3614414 , 13.19413359,  7.38563121,\n         2.46086346,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         1.46021055,  8.3346773 , 15.84579984, 23.60675378, 33.54819294,\n        44.27639467, 48.87446329, 50.20693099, 49.99045599, 49.1588892 ,\n        49.63365299, 43.77509351, 40.53851393, 37.67075174, 33.30905003,\n        26.32763287, 23.6790613 , 21.31099296, 15.43660105,  9.01164878,\n         2.99734791,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         1.06219838,  7.45055622, 16.13610445, 23.71475551, 34.77860619,\n        47.2215008 , 53.6233551 , 54.81798368, 55.92251191, 53.22124075,\n        49.4503058 , 39.74031917, 34.16426522, 30.2018871 , 25.85270793,\n        18.06409236, 15.80461682, 12.8847943 ,  8.64220686,  4.66112567,\n         1.39708948,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.98177429,  5.91164221, 14.90839546, 22.12547334, 34.56375025,\n        47.81982149, 52.91279537, 51.24915726, 49.97979978, 43.76647484,\n        36.75656701, 27.81869354, 22.11996995, 17.57564226, 12.13486078,\n         5.15406544,  4.12717522,  2.9785476 ,  1.87065479,  1.03966077,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.62563403,  2.96540486,  8.10574754, 12.95826683, 22.5156611 ,\n        36.54563498, 43.70773552, 43.40158906, 42.04375954, 36.26387398,\n        26.36213282, 16.097447  , 11.84899579,  9.54180118,  5.61094825,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  1.12012557,  2.93401961,  4.71631936, 10.54260512,\n        22.99063273, 33.4545295 , 34.35373901, 35.13673701, 32.0808119 ,\n        22.44555644, 10.58933265,  6.26785589,  4.65281464,  2.69514024,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.59373135,  0.68658094,  2.91090381,\n        11.93617246, 23.32530506, 28.52913442, 32.03782605, 33.07039531,\n        27.92994826, 19.1779577 , 12.67855946,  7.02585206,  2.17281578,\n         0.76475631,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  1.65723673,\n         6.30979571, 16.78541755, 25.32519019, 30.61521831, 34.57128292,\n        33.0908346 , 27.99904978, 22.83172975, 16.0292064 ,  7.97931783,\n         2.94371279,  0.78737105,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.56980434,\n         2.36360757,  8.53829476, 17.58181015, 25.29533331, 32.49644269,\n        35.06776899, 32.34556737, 29.64631218, 24.76214365, 16.94777921,\n         8.8661843 ,  3.03987807,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.76122042,  3.38556687,  9.41152113, 17.9270797 , 27.02572501,\n        36.65804893, 39.03953509, 37.31306511, 32.2659861 , 25.72264731,\n        16.09211544,  7.45883031,  1.20574387,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  1.12810124,  4.18556915, 10.53768016, 20.32190462,\n        35.33327876, 45.48067059, 48.77061171, 41.91222068, 33.37691713,\n        20.58128644,  9.83699677,  1.55553653,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.80425832,  2.19321938,  5.62923943, 12.00200545, 20.7725297 ,\n        35.59478866, 48.97722924, 56.73507843, 50.80639172, 40.44401573,\n        25.16344501, 11.28278974,  1.10555669,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.68034049,  1.72445762,\n         4.49963555,  9.18880961, 15.41160271, 22.82195811, 30.83482791,\n        41.14545061, 50.81971062, 54.50061733, 48.12469725, 38.71838555,\n        24.43329563, 11.38666307,  1.2000677 ,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.6720793 ,  1.31539515,  3.64752376,  7.92378537,\n        14.02900875, 21.64310209, 30.12772702, 37.5975952 , 42.79970536,\n        46.43405847, 48.61627071, 47.04416804, 39.95675678, 30.43758432,\n        18.60482026,  8.96761611,  1.41895005,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.63077336,\n         2.12019282,  4.9345949 ,  8.76653731, 14.68869422, 21.64556297,\n        28.67558856, 36.03609912, 42.90134798, 46.50466921, 47.5620357 ,\n        46.62469453, 43.76893697, 38.49396134, 29.86806412, 20.1553875 ,\n        12.38116394,  5.46104399,  0.51066627,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.93643733,  2.59189468,  5.43785345,\n        10.51256094, 16.10678696, 21.97308663, 29.3532144 , 37.13885652,\n        42.74956138, 46.7047087 , 48.79045592, 48.21366628, 45.08141423,\n        40.02868106, 34.03904056, 27.14517824, 19.78565133, 13.04484562,\n         7.65265277,  3.08415303,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  2.2346332 ,  6.7085191 , 12.39690266,\n        21.01502003, 30.00362615, 36.83020911, 41.07354374, 45.2066622 ,\n        46.75139666, 46.57485043, 45.3701799 , 41.07354196, 34.86779823,\n        27.44181554, 20.80009353, 14.50019673,  8.99304565,  4.5535444 ,\n         1.59724957,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  2.90435023,  7.91387385, 13.27648157,\n        20.46294135, 29.26245562, 35.25095422, 39.59811448, 42.794651  ,\n        42.19689603, 39.5875757 , 35.87273622, 28.45908847, 21.35100819,\n        14.5740261 ,  9.06665885,  4.65700073,  1.67029191,  0.51066627,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  2.95825136,  7.96864726, 13.79580696,\n        20.65460773, 27.75044006, 32.28461123, 35.26733993, 35.6641561 ,\n        33.26245199, 28.52269552, 22.39720045, 15.67289473,  9.7052078 ,\n         4.70702748,  1.7615229 ,  0.61074631,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  1.74349217,  6.10407925, 12.01115216,\n        17.78664767, 23.70152548, 26.74648895, 26.35808365, 24.70925999,\n        20.74629808, 15.74195541, 11.15947222,  6.2858474 ,  2.54942268,\n         0.6393406 ,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  1.56676309,  4.40909707,  7.72499667,\n        11.21491064, 14.59614787, 15.07922588, 13.46987196, 10.93702265,\n         7.39708638,  4.59902186,  2.61050277,  0.71082635,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197],\n       [ 0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197,  0.48207197,  0.48207197,\n         0.48207197,  0.48207197,  0.48207197]])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = parse_input_model()\n",
    "c1 = ConvolutionLayerBatch(6, 5, 1, 2)\n",
    "mnist_batch_1 = x_train[0:64].reshape(64, 28, 28, 1)\n",
    "o = c1.forward(mnist_batch_1)\n",
    "print(o.shape)\n",
    "o[0, :, :, 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}