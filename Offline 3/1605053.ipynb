{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayer(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayer(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayer())\n",
    "                model.append(FullyConnectedLayer(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayer())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            input_shape[1],\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * filters) + bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.72175532, 0.72175532, 0.72175532],\n       [0.72175532, 1.20161669, 0.72175532],\n       [0.72175532, 0.72175532, 0.72175532]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.72175532, 0.72175532, 0.72175532],\n       [0.72175532, 1.20161669, 0.72175532],\n       [0.72175532, 0.72175532, 0.72175532]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.index_tracker = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        if not self.index_tracker:\n",
    "            self.index_tracker = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.max(image_slice, axis=(1, 2))\n",
    "                    # self.index_tracker[i, out_x, out_y, :] = np.argmax(image[i, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :], axis=(0, 1))\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.20161669, 1.20161669],\n       [1.20161669, 1.20161669]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.20161669, 1.20161669, 1.20161669, 1.20161669, 0.77173744,\n        0.77173744, 0.77173744, 0.77173744, 1.3976355 , 1.3976355 ,\n        1.3976355 , 1.3976355 , 1.19443716, 1.19443716, 1.19443716,\n        1.19443716],\n       [1.66545862, 1.66545862, 1.66545862, 1.66545862, 1.23557937,\n        1.23557937, 1.23557937, 1.23557937, 1.86147742, 1.86147742,\n        1.86147742, 1.86147742, 1.65827908, 1.65827908, 1.65827908,\n        1.65827908],\n       [2.60606404, 2.60606404, 2.60606404, 2.60606404, 2.17618479,\n        2.17618479, 2.17618479, 2.17618479, 2.80208285, 2.80208285,\n        2.80208285, 2.80208285, 2.59888451, 2.59888451, 2.59888451,\n        2.59888451],\n       [2.13730758, 2.13730758, 2.13730758, 2.13730758, 1.70742833,\n        1.70742833, 1.70742833, 1.70742833, 2.33332639, 2.33332639,\n        2.33332639, 2.33332639, 2.13012805, 2.13012805, 2.13012805,\n        2.13012805],\n       [2.60564651, 2.60564651, 2.60564651, 2.60564651, 2.17576727,\n        2.17576727, 2.17576727, 2.17576727, 2.80166532, 2.80166532,\n        2.80166532, 2.80166532, 2.59846698, 2.59846698, 2.59846698,\n        2.59846698],\n       [1.65824778, 1.65824778, 1.65824778, 1.65824778, 1.22836853,\n        1.22836853, 1.22836853, 1.22836853, 1.85426659, 1.85426659,\n        1.85426659, 1.85426659, 1.65106825, 1.65106825, 1.65106825,\n        1.65106825],\n       [1.18719367, 1.18719367, 1.18719367, 1.18719367, 0.75731442,\n        0.75731442, 0.75731442, 0.75731442, 1.38321248, 1.38321248,\n        1.38321248, 1.38321248, 1.18001414, 1.18001414, 1.18001414,\n        1.18001414],\n       [2.14411336, 2.14411336, 2.14411336, 2.14411336, 1.71423411,\n        1.71423411, 1.71423411, 1.71423411, 2.34013216, 2.34013216,\n        2.34013216, 2.34013216, 2.13693382, 2.13693382, 2.13693382,\n        2.13693382],\n       [1.19098679, 1.19098679, 1.19098679, 1.19098679, 0.76110754,\n        0.76110754, 0.76110754, 0.76110754, 1.38700559, 1.38700559,\n        1.38700559, 1.38700559, 1.18380725, 1.18380725, 1.18380725,\n        1.18380725],\n       [2.13818523, 2.13818523, 2.13818523, 2.13818523, 1.70830598,\n        1.70830598, 1.70830598, 1.70830598, 2.33420404, 2.33420404,\n        2.33420404, 2.33420404, 2.1310057 , 2.1310057 , 2.1310057 ,\n        2.1310057 ],\n       [2.13113072, 2.13113072, 2.13113072, 2.13113072, 1.70125147,\n        1.70125147, 1.70125147, 1.70125147, 2.32714952, 2.32714952,\n        2.32714952, 2.32714952, 2.12395118, 2.12395118, 2.12395118,\n        2.12395118],\n       [2.14071429, 2.14071429, 2.14071429, 2.14071429, 1.71083504,\n        1.71083504, 1.71083504, 1.71083504, 2.33673309, 2.33673309,\n        2.33673309, 2.33673309, 2.13353475, 2.13353475, 2.13353475,\n        2.13353475],\n       [2.14268334, 2.14268334, 2.14268334, 2.14268334, 1.71280409,\n        1.71280409, 1.71280409, 1.71280409, 2.33870215, 2.33870215,\n        2.33870215, 2.33870215, 2.13550381, 2.13550381, 2.13550381,\n        2.13550381],\n       [1.6520355 , 1.6520355 , 1.6520355 , 1.6520355 , 1.22215626,\n        1.22215626, 1.22215626, 1.22215626, 1.84805431, 1.84805431,\n        1.84805431, 1.84805431, 1.64485597, 1.64485597, 1.64485597,\n        1.64485597],\n       [1.18014202, 1.18014202, 1.18014202, 1.18014202, 0.75026278,\n        0.75026278, 0.75026278, 0.75026278, 1.37616083, 1.37616083,\n        1.37616083, 1.37616083, 1.17296249, 1.17296249, 1.17296249,\n        1.17296249],\n       [1.66244368, 1.66244368, 1.66244368, 1.66244368, 1.23256443,\n        1.23256443, 1.23256443, 1.23256443, 1.85846249, 1.85846249,\n        1.85846249, 1.85846249, 1.65526415, 1.65526415, 1.65526415,\n        1.65526415],\n       [1.19472965, 1.19472965, 1.19472965, 1.19472965, 0.7648504 ,\n        0.7648504 , 0.7648504 , 0.7648504 , 1.39074845, 1.39074845,\n        1.39074845, 1.39074845, 1.18755011, 1.18755011, 1.18755011,\n        1.18755011],\n       [2.14627855, 2.14627855, 2.14627855, 2.14627855, 1.7163993 ,\n        1.7163993 , 1.7163993 , 1.7163993 , 2.34229735, 2.34229735,\n        2.34229735, 2.34229735, 2.13909901, 2.13909901, 2.13909901,\n        2.13909901],\n       [2.59444217, 2.59444217, 2.59444217, 2.59444217, 2.16456292,\n        2.16456292, 2.16456292, 2.16456292, 2.79046098, 2.79046098,\n        2.79046098, 2.79046098, 2.58726264, 2.58726264, 2.58726264,\n        2.58726264],\n       [1.18398814, 1.18398814, 1.18398814, 1.18398814, 0.75410889,\n        0.75410889, 0.75410889, 0.75410889, 1.38000694, 1.38000694,\n        1.38000694, 1.38000694, 1.17680861, 1.17680861, 1.17680861,\n        1.17680861],\n       [1.20051401, 1.20051401, 1.20051401, 1.20051401, 0.77063477,\n        0.77063477, 0.77063477, 0.77063477, 1.39653282, 1.39653282,\n        1.39653282, 1.39653282, 1.19333448, 1.19333448, 1.19333448,\n        1.19333448],\n       [2.61143918, 2.61143918, 2.61143918, 2.61143918, 2.18155993,\n        2.18155993, 2.18155993, 2.18155993, 2.80745798, 2.80745798,\n        2.80745798, 2.80745798, 2.60425964, 2.60425964, 2.60425964,\n        2.60425964],\n       [1.20140899, 1.20140899, 1.20140899, 1.20140899, 0.77152974,\n        0.77152974, 0.77152974, 0.77152974, 1.39742779, 1.39742779,\n        1.39742779, 1.39742779, 1.19422945, 1.19422945, 1.19422945,\n        1.19422945],\n       [1.20156477, 1.20156477, 1.20156477, 1.20156477, 0.77168552,\n        0.77168552, 0.77168552, 0.77168552, 1.39758358, 1.39758358,\n        1.39758358, 1.39758358, 1.19438524, 1.19438524, 1.19438524,\n        1.19438524],\n       [2.6104532 , 2.6104532 , 2.6104532 , 2.6104532 , 2.18057395,\n        2.18057395, 2.18057395, 2.18057395, 2.806472  , 2.806472  ,\n        2.806472  , 2.806472  , 2.60327366, 2.60327366, 2.60327366,\n        2.60327366],\n       [1.19100756, 1.19100756, 1.19100756, 1.19100756, 0.76112831,\n        0.76112831, 0.76112831, 0.76112831, 1.38702636, 1.38702636,\n        1.38702636, 1.38702636, 1.18382802, 1.18382802, 1.18382802,\n        1.18382802],\n       [2.61517942, 2.61517942, 2.61517942, 2.61517942, 2.18530017,\n        2.18530017, 2.18530017, 2.18530017, 2.81119823, 2.81119823,\n        2.81119823, 2.81119823, 2.60799989, 2.60799989, 2.60799989,\n        2.60799989],\n       [1.18756021, 1.18756021, 1.18756021, 1.18756021, 0.75768096,\n        0.75768096, 0.75768096, 0.75768096, 1.38357901, 1.38357901,\n        1.38357901, 1.38357901, 1.18038067, 1.18038067, 1.18038067,\n        1.18038067],\n       [1.66449842, 1.66449842, 1.66449842, 1.66449842, 1.23461917,\n        1.23461917, 1.23461917, 1.23461917, 1.86051723, 1.86051723,\n        1.86051723, 1.86051723, 1.65731889, 1.65731889, 1.65731889,\n        1.65731889],\n       [2.14145413, 2.14145413, 2.14145413, 2.14145413, 1.71157488,\n        1.71157488, 1.71157488, 1.71157488, 2.33747293, 2.33747293,\n        2.33747293, 2.33747293, 2.13427459, 2.13427459, 2.13427459,\n        2.13427459],\n       [2.13790064, 2.13790064, 2.13790064, 2.13790064, 1.70802139,\n        1.70802139, 1.70802139, 1.70802139, 2.33391944, 2.33391944,\n        2.33391944, 2.33391944, 2.1307211 , 2.1307211 , 2.1307211 ,\n        2.1307211 ],\n       [2.14757151, 2.14757151, 2.14757151, 2.14757151, 1.71769227,\n        1.71769227, 1.71769227, 1.71769227, 2.34359032, 2.34359032,\n        2.34359032, 2.34359032, 2.14039198, 2.14039198, 2.14039198,\n        2.14039198],\n       [2.14463926, 2.14463926, 2.14463926, 2.14463926, 1.71476001,\n        1.71476001, 1.71476001, 1.71476001, 2.34065806, 2.34065806,\n        2.34065806, 2.34065806, 2.13745972, 2.13745972, 2.13745972,\n        2.13745972],\n       [1.68637158, 1.68637158, 1.68637158, 1.68637158, 1.25649234,\n        1.25649234, 1.25649234, 1.25649234, 1.88239039, 1.88239039,\n        1.88239039, 1.88239039, 1.67919205, 1.67919205, 1.67919205,\n        1.67919205],\n       [1.1852252 , 1.1852252 , 1.1852252 , 1.1852252 , 0.75534595,\n        0.75534595, 0.75534595, 0.75534595, 1.381244  , 1.381244  ,\n        1.381244  , 1.381244  , 1.17804566, 1.17804566, 1.17804566,\n        1.17804566],\n       [2.60923983, 2.60923983, 2.60923983, 2.60923983, 2.17936058,\n        2.17936058, 2.17936058, 2.17936058, 2.80525864, 2.80525864,\n        2.80525864, 2.80525864, 2.6020603 , 2.6020603 , 2.6020603 ,\n        2.6020603 ],\n       [1.1985212 , 1.1985212 , 1.1985212 , 1.1985212 , 0.76864195,\n        0.76864195, 0.76864195, 0.76864195, 1.39454   , 1.39454   ,\n        1.39454   , 1.39454   , 1.19134166, 1.19134166, 1.19134166,\n        1.19134166],\n       [2.14602194, 2.14602194, 2.14602194, 2.14602194, 1.71614269,\n        1.71614269, 1.71614269, 1.71614269, 2.34204075, 2.34204075,\n        2.34204075, 2.34204075, 2.13884241, 2.13884241, 2.13884241,\n        2.13884241],\n       [2.62192681, 2.62192681, 2.62192681, 2.62192681, 2.19204756,\n        2.19204756, 2.19204756, 2.19204756, 2.81794562, 2.81794562,\n        2.81794562, 2.81794562, 2.61474728, 2.61474728, 2.61474728,\n        2.61474728],\n       [1.21071661, 1.21071661, 1.21071661, 1.21071661, 0.78083736,\n        0.78083736, 0.78083736, 0.78083736, 1.40673541, 1.40673541,\n        1.40673541, 1.40673541, 1.20353708, 1.20353708, 1.20353708,\n        1.20353708],\n       [2.60627667, 2.60627667, 2.60627667, 2.60627667, 2.17639742,\n        2.17639742, 2.17639742, 2.17639742, 2.80229548, 2.80229548,\n        2.80229548, 2.80229548, 2.59909714, 2.59909714, 2.59909714,\n        2.59909714],\n       [1.19284922, 1.19284922, 1.19284922, 1.19284922, 0.76296997,\n        0.76296997, 0.76296997, 0.76296997, 1.38886803, 1.38886803,\n        1.38886803, 1.38886803, 1.18566969, 1.18566969, 1.18566969,\n        1.18566969],\n       [2.60779191, 2.60779191, 2.60779191, 2.60779191, 2.17791267,\n        2.17791267, 2.17791267, 2.17791267, 2.80381072, 2.80381072,\n        2.80381072, 2.80381072, 2.60061238, 2.60061238, 2.60061238,\n        2.60061238],\n       [2.60937532, 2.60937532, 2.60937532, 2.60937532, 2.17949607,\n        2.17949607, 2.17949607, 2.17949607, 2.80539412, 2.80539412,\n        2.80539412, 2.80539412, 2.60219578, 2.60219578, 2.60219578,\n        2.60219578],\n       [1.20666675, 1.20666675, 1.20666675, 1.20666675, 0.7767875 ,\n        0.7767875 , 0.7767875 , 0.7767875 , 1.40268556, 1.40268556,\n        1.40268556, 1.40268556, 1.19948722, 1.19948722, 1.19948722,\n        1.19948722],\n       [2.13878419, 2.13878419, 2.13878419, 2.13878419, 1.70890494,\n        1.70890494, 1.70890494, 1.70890494, 2.334803  , 2.334803  ,\n        2.334803  , 2.334803  , 2.13160466, 2.13160466, 2.13160466,\n        2.13160466],\n       [2.61200283, 2.61200283, 2.61200283, 2.61200283, 2.18212358,\n        2.18212358, 2.18212358, 2.18212358, 2.80802163, 2.80802163,\n        2.80802163, 2.80802163, 2.60482329, 2.60482329, 2.60482329,\n        2.60482329],\n       [1.66564193, 1.66564193, 1.66564193, 1.66564193, 1.23576268,\n        1.23576268, 1.23576268, 1.23576268, 1.86166074, 1.86166074,\n        1.86166074, 1.86166074, 1.6584624 , 1.6584624 , 1.6584624 ,\n        1.6584624 ],\n       [2.6019895 , 2.6019895 , 2.6019895 , 2.6019895 , 2.17211025,\n        2.17211025, 2.17211025, 2.17211025, 2.7980083 , 2.7980083 ,\n        2.7980083 , 2.7980083 , 2.59480996, 2.59480996, 2.59480996,\n        2.59480996],\n       [2.12675034, 2.12675034, 2.12675034, 2.12675034, 1.69687109,\n        1.69687109, 1.69687109, 1.69687109, 2.32276914, 2.32276914,\n        2.32276914, 2.32276914, 2.11957081, 2.11957081, 2.11957081,\n        2.11957081]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if not self.weights:\n",
    "            self.weights = np.random.rand(flattened_input.shape[1], self.output_dimension)\n",
    "        if not self.bias:\n",
    "            self.bias = np.random.rand(1, self.output_dimension)\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: int) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[10.35351109,  9.62463702,  8.06550849,  8.8549909 ],\n       [14.27700238, 13.62230028, 11.08408648, 12.43295797],\n       [22.23328444, 21.72899254, 17.20533315, 19.68856609],\n       [18.26822266, 17.68897292, 14.15477247, 16.07268945],\n       [22.22975271, 21.72539405, 17.20261597, 19.68534538],\n       [14.21600823, 13.56015306, 11.03716001, 12.3773353 ],\n       [10.23151135,  9.50033093,  7.97164675,  8.74373513],\n       [18.32579054, 17.7476291 , 14.19906291, 16.12518759],\n       [10.2635961 ,  9.53302223,  7.99633148,  8.77299432],\n       [18.27564643, 17.69653703, 14.16048402, 16.07945944],\n       [18.21597454, 17.63573707, 14.11457485, 16.02504259],\n       [18.29703892, 17.71833394, 14.17694255, 16.09896799],\n       [18.31369452, 17.73530441, 14.18975671, 16.11415681],\n       [14.16346055, 13.50661199, 10.99673192, 12.32941526],\n       [10.17186372,  9.43955568,  7.92575623,  8.6893404 ],\n       [14.25150001, 13.5963158 , 11.06446598, 12.40970149],\n       [10.29525579,  9.56528043,  8.02068918,  8.80186588],\n       [18.34410519, 17.76628998, 14.21315347, 16.14188935],\n       [22.13497872, 21.6288284 , 17.12970064, 19.59891772],\n       [10.20439678,  9.47270377,  7.95078588,  8.71900842],\n       [10.34418391,  9.61513351,  8.05833253,  8.84648512],\n       [22.278751  , 21.77531863, 17.24031331, 19.73002862],\n       [10.3517542 ,  9.62284692,  8.06415681,  8.85338873],\n       [10.35307194,  9.62418957,  8.06517063,  8.85459043],\n       [22.27041091, 21.76682087, 17.23389678, 19.722423  ],\n       [10.2637718 ,  9.53320125,  7.99646666,  8.77315455],\n       [22.31038854, 21.80755426, 17.26465397, 19.75887998],\n       [10.23461178,  9.50348997,  7.97403209,  8.74656252],\n       [14.26888041, 13.61402476, 11.07783776, 12.42555127],\n       [18.30329696, 17.72471028, 14.18175724, 16.10467492],\n       [18.27323915, 17.69408424, 14.15863196, 16.07726416],\n       [18.35504201, 17.77743356, 14.22156782, 16.15186302],\n       [18.33023897, 17.75216162, 14.20248535, 16.12924427],\n       [14.45389853, 13.80254057, 11.22018334, 12.59427567],\n       [10.21486065,  9.48336545,  7.95883636,  8.72855078],\n       [22.26014743, 21.75636337, 17.22600046, 19.71306338],\n       [10.3273273 ,  9.59795824,  8.04536372,  8.831113  ],\n       [18.34193469, 17.76407844, 14.21148357, 16.13990999],\n       [22.36746256, 21.86570724, 17.30856445, 19.81092775],\n       [10.43048441,  9.70306549,  8.1247287 ,  8.92518553],\n       [22.235083  , 21.73082511, 17.20671689, 19.69020626],\n       [10.27934986,  9.5490738 ,  8.00845179,  8.78736071],\n       [22.24789997, 21.74388437, 17.21657775, 19.70189449],\n       [22.26129347, 21.75753108, 17.22688218, 19.7141085 ],\n       [10.39622797,  9.66816144,  8.09837315,  8.8939459 ],\n       [18.28071283, 17.70169921, 14.16438191, 16.08407967],\n       [22.28351875, 21.78017651, 17.24398143, 19.73437649],\n       [14.278553  , 13.62388021, 11.08527947, 12.43437204],\n       [22.19881916, 21.69387571, 17.17881693, 19.65713602],\n       [18.17892232, 17.59798439, 14.08606834, 15.99125339]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1).reshape(-1, 1)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.55328664, 0.26693419, 0.05614141, 0.12363776],\n       [0.58179272, 0.30229765, 0.02388375, 0.09202588],\n       [0.59207317, 0.35757256, 0.00387939, 0.04647488],\n       [0.59242792, 0.33194838, 0.00968694, 0.06593675],\n       [0.59207687, 0.35755093, 0.00388258, 0.04648962],\n       [0.58151032, 0.30180271, 0.02421036, 0.09247661],\n       [0.55200449, 0.2657021 , 0.05760974, 0.12468368],\n       [0.59248702, 0.33234299, 0.00956013, 0.06560986],\n       [0.55234436, 0.266027  , 0.05722021, 0.12440843],\n       [0.59243567, 0.33199931, 0.0096705 , 0.06589452],\n       [0.59237235, 0.33158956, 0.00980346, 0.06623463],\n       [0.59245778, 0.332146  , 0.00962326, 0.06577295],\n       [0.59247479, 0.33226014, 0.00958664, 0.06567843],\n       [0.58126358, 0.30137513, 0.02449516, 0.09286613],\n       [0.55136752, 0.2650964 , 0.05834036, 0.12519572],\n       [0.58167516, 0.30209089, 0.02401979, 0.09221415],\n       [0.55267786, 0.26634699, 0.0568382 , 0.12413695],\n       [0.59250536, 0.33246836, 0.00952013, 0.06550615],\n       [0.59217434, 0.35696965, 0.00396904, 0.04688698],\n       [0.55171576, 0.26542703, 0.05794082, 0.12491639],\n       [0.55318959, 0.26684031, 0.05625244, 0.12371766],\n       [0.59202504, 0.35785095, 0.00383861, 0.0462854 ],\n       [0.55326838, 0.26691651, 0.0561623 , 0.12365281],\n       [0.55328208, 0.26692977, 0.05614663, 0.12364152],\n       [0.59203393, 0.35779991, 0.00384606, 0.0463201 ],\n       [0.55234622, 0.26602878, 0.05721808, 0.12440692],\n       [0.59199105, 0.35804449, 0.00381049, 0.04615397],\n       [0.55203741, 0.26573352, 0.05757199, 0.12465707],\n       [0.58175536, 0.30223183, 0.023927  , 0.09208581],\n       [0.59246419, 0.33218889, 0.00960949, 0.06573743],\n       [0.59243316, 0.3319828 , 0.00967583, 0.06590821],\n       [0.5925162 , 0.33254319, 0.00949632, 0.06544429],\n       [0.5924915 , 0.33237345, 0.0095504 , 0.06558466],\n       [0.58258779, 0.30372477, 0.02296026, 0.09072718],\n       [0.55182735, 0.26553324, 0.05781284, 0.12482657],\n       [0.59204483, 0.35773708, 0.00385525, 0.04636284],\n       [0.55301379, 0.26667052, 0.05645362, 0.12386207],\n       [0.5925032 , 0.33245351, 0.00952486, 0.06551844],\n       [0.59192873, 0.35839328, 0.00376026, 0.04591774],\n       [0.55408144, 0.26770691, 0.05523274, 0.1229789 ],\n       [0.59207128, 0.35758358, 0.00387777, 0.04646737],\n       [0.55251054, 0.2661863 , 0.05702983, 0.12427332],\n       [0.59205779, 0.35766209, 0.00386624, 0.04641389],\n       [0.59204362, 0.35774409, 0.00385422, 0.04635807],\n       [0.55372907, 0.26736346, 0.05563546, 0.12327202],\n       [0.59244093, 0.33203406, 0.00965929, 0.06586571],\n       [0.59201994, 0.35788012, 0.00383436, 0.04626557],\n       [0.58179984, 0.30231021, 0.0238755 , 0.09201444],\n       [0.59210909, 0.35736134, 0.00391059, 0.04661898],\n       [0.59233181, 0.3313347 , 0.00988692, 0.06644656]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "2.4556159699350304"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.44671336,  0.26693419,  0.05614141,  0.12363776],\n       [ 0.58179272, -0.69770235,  0.02388375,  0.09202588],\n       [ 0.59207317,  0.35757256,  0.00387939, -0.95352512],\n       [ 0.59242792,  0.33194838, -0.99031306,  0.06593675],\n       [ 0.59207687,  0.35755093,  0.00388258, -0.95351038],\n       [ 0.58151032, -0.69819729,  0.02421036,  0.09247661],\n       [-0.44799551,  0.2657021 ,  0.05760974,  0.12468368],\n       [ 0.59248702,  0.33234299, -0.99043987,  0.06560986],\n       [-0.44765564,  0.266027  ,  0.05722021,  0.12440843],\n       [ 0.59243567,  0.33199931, -0.9903295 ,  0.06589452],\n       [ 0.59237235,  0.33158956, -0.99019654,  0.06623463],\n       [ 0.59245778,  0.332146  , -0.99037674,  0.06577295],\n       [ 0.59247479,  0.33226014, -0.99041336,  0.06567843],\n       [ 0.58126358, -0.69862487,  0.02449516,  0.09286613],\n       [-0.44863248,  0.2650964 ,  0.05834036,  0.12519572],\n       [ 0.58167516, -0.69790911,  0.02401979,  0.09221415],\n       [-0.44732214,  0.26634699,  0.0568382 ,  0.12413695],\n       [ 0.59250536,  0.33246836, -0.99047987,  0.06550615],\n       [ 0.59217434,  0.35696965,  0.00396904, -0.95311302],\n       [-0.44828424,  0.26542703,  0.05794082,  0.12491639],\n       [-0.44681041,  0.26684031,  0.05625244,  0.12371766],\n       [ 0.59202504,  0.35785095,  0.00383861, -0.9537146 ],\n       [-0.44673162,  0.26691651,  0.0561623 ,  0.12365281],\n       [-0.44671792,  0.26692977,  0.05614663,  0.12364152],\n       [ 0.59203393,  0.35779991,  0.00384606, -0.9536799 ],\n       [-0.44765378,  0.26602878,  0.05721808,  0.12440692],\n       [ 0.59199105,  0.35804449,  0.00381049, -0.95384603],\n       [-0.44796259,  0.26573352,  0.05757199,  0.12465707],\n       [ 0.58175536, -0.69776817,  0.023927  ,  0.09208581],\n       [ 0.59246419,  0.33218889, -0.99039051,  0.06573743],\n       [ 0.59243316,  0.3319828 , -0.99032417,  0.06590821],\n       [ 0.5925162 ,  0.33254319, -0.99050368,  0.06544429],\n       [ 0.5924915 ,  0.33237345, -0.9904496 ,  0.06558466],\n       [ 0.58258779, -0.69627523,  0.02296026,  0.09072718],\n       [-0.44817265,  0.26553324,  0.05781284,  0.12482657],\n       [ 0.59204483,  0.35773708,  0.00385525, -0.95363716],\n       [-0.44698621,  0.26667052,  0.05645362,  0.12386207],\n       [ 0.5925032 ,  0.33245351, -0.99047514,  0.06551844],\n       [ 0.59192873,  0.35839328,  0.00376026, -0.95408226],\n       [-0.44591856,  0.26770691,  0.05523274,  0.1229789 ],\n       [ 0.59207128,  0.35758358,  0.00387777, -0.95353263],\n       [-0.44748946,  0.2661863 ,  0.05702983,  0.12427332],\n       [ 0.59205779,  0.35766209,  0.00386624, -0.95358611],\n       [ 0.59204362,  0.35774409,  0.00385422, -0.95364193],\n       [-0.44627093,  0.26736346,  0.05563546,  0.12327202],\n       [ 0.59244093,  0.33203406, -0.99034071,  0.06586571],\n       [ 0.59201994,  0.35788012,  0.00383436, -0.95373443],\n       [ 0.58179984, -0.69768979,  0.0238755 ,  0.09201444],\n       [ 0.59210909,  0.35736134,  0.00391059, -0.95338102],\n       [ 0.59233181,  0.3313347 , -0.99011308,  0.06644656]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.10010352,  0.01660596, -0.02040161, -0.27843417,  0.25035061,\n         0.09118991,  0.17025697, -0.06776964, -0.18814022, -0.24192317,\n         0.07712478,  0.1200323 ,  0.11416985,  0.05399208, -0.08841738,\n        -0.06747464],\n       [ 0.2874105 ,  0.01621933,  0.02628684,  0.33574623, -0.45310295,\n        -0.16943281, -0.23351681,  0.09222417,  0.46832469,  0.29909239,\n        -0.21528032, -0.19634103, -0.30701436, -0.24862106,  0.03880738,\n         0.03248295],\n       [-0.26929293, -0.07010458,  0.10571787,  0.36576255, -0.23319781,\n        -0.19909087, -0.36366209,  0.16413782, -0.35868374,  0.51142183,\n         0.11581752, -0.13997878,  0.06313395,  0.37430073,  0.39918908,\n         0.29443783],\n       [ 0.0433375 , -0.17212348, -0.17819997,  0.57452298,  0.33746025,\n         0.4558439 ,  0.22402208, -0.13494542,  0.21848852, -0.06497898,\n         0.16312943,  0.08313667,  0.44154626,  0.05770327, -0.06741858,\n        -0.04555032],\n       [-0.26928287, -0.0701028 ,  0.10571753,  0.36576374, -0.23320593,\n        -0.1990933 , -0.36366313,  0.16413809, -0.35866882,  0.51142195,\n         0.11581061, -0.1399816 ,  0.06312489,  0.37428986,  0.39918478,\n         0.29443473],\n       [ 0.28761271,  0.0163128 ,  0.02630381,  0.3354967 , -0.45324768,\n        -0.16954175, -0.23349454,  0.0922209 ,  0.46856906,  0.29897076,\n        -0.21545525, -0.19636522, -0.30728411, -0.24889248,  0.03865611,\n         0.03237125],\n       [-0.09972104,  0.01694851, -0.02024813, -0.27954519,  0.24990735,\n         0.0906667 ,  0.1702354 , -0.06771619, -0.18782761, -0.24227573,\n         0.07664869,  0.11997824,  0.11330027,  0.05335771, -0.08875993,\n        -0.06773165],\n       [ 0.04315795, -0.17217219, -0.17819699,  0.57459289,  0.33758331,\n         0.45589423,  0.22401417, -0.13494139,  0.21823931, -0.06492749,\n         0.16326028,  0.08316999,  0.4417257 ,  0.05791504, -0.06731323,\n        -0.04547344],\n       [-0.09982143,  0.01685781, -0.02028903, -0.27925065,  0.25002468,\n         0.09080565,  0.17024151, -0.06773057, -0.18790896, -0.2421828 ,\n         0.07677438,  0.11999255,  0.11353031,  0.05352477, -0.08866985,\n        -0.06766405],\n       [ 0.04331434, -0.17212979, -0.17819959,  0.57453209,  0.33747611,\n         0.45585041,  0.22402104, -0.13494489,  0.21845638, -0.06497229,\n         0.16314632,  0.08314096,  0.44156944,  0.05773061, -0.06740496,\n        -0.04554038],\n       [ 0.04350065, -0.17207894, -0.17820257,  0.57445805,  0.33734858,\n         0.45579791,  0.22402954, -0.13494914,  0.21871468, -0.06502646,\n         0.16301037,  0.08310659,  0.44138281,  0.05751058, -0.06751466,\n        -0.04562045],\n       [ 0.0432476 , -0.17214791, -0.17819849,  0.57455819,  0.33752184,\n         0.45586914,  0.22401808, -0.13494339,  0.21836378, -0.0649531 ,\n         0.16319497,  0.08315333,  0.44163617,  0.05780934, -0.06736578,\n        -0.04551178],\n       [ 0.04319566, -0.17216199, -0.17819763,  0.57457834,  0.33755745,\n         0.45588369,  0.2240158 , -0.13494223,  0.21829167, -0.06493824,\n         0.16323281,  0.08316298,  0.44168805,  0.05787059, -0.06733532,\n        -0.04548956],\n       [ 0.28778697,  0.01639403,  0.02631886,  0.33527889, -0.45337283,\n        -0.16963676, -0.23347538,  0.09221822,  0.46877904,  0.29886517,\n        -0.21560654, -0.19638598, -0.30751792, -0.24912691,  0.03852536,\n         0.03227468],\n       [-0.09953478,  0.01711829, -0.02017104, -0.28009727,  0.24968777,\n         0.09040576,  0.17022317, -0.06768884, -0.18767804, -0.2424489 ,\n         0.07641411,  0.11995145,  0.11287002,  0.05304671, -0.08892736,\n        -0.06785733],\n       [ 0.28749504,  0.01625831,  0.02629387,  0.33564233, -0.45316339,\n        -0.16947819, -0.2335075 ,  0.09222278,  0.46842695,  0.29904166,\n        -0.21535337, -0.19635116, -0.30712693, -0.24873445,  0.0387442 ,\n         0.0324363 ],\n       [-0.09992064,  0.01676874, -0.02032902, -0.27896166,  0.25013993,\n         0.09094182,  0.17024724, -0.06774453, -0.18798984, -0.24209124,\n         0.07689807,  0.12000661,  0.11375636,  0.05368946, -0.08858096,\n        -0.06759735],\n       [ 0.04310088, -0.1721876 , -0.17819602,  0.57461477,  0.33762247,\n         0.45591017,  0.22401172, -0.13494013,  0.21816002, -0.06491129,\n         0.16330183,  0.08318063,  0.44178264,  0.05798229, -0.06727983,\n        -0.04544907],\n       [-0.26901248, -0.07005471,  0.10570845,  0.36579404, -0.23342361,\n        -0.19915849, -0.3636907 ,  0.16414497, -0.35826823,  0.51142428,\n         0.11562477, -0.14005714,  0.06288143,  0.37399764,  0.3990687 ,\n         0.29435117],\n       [-0.09963631,  0.0170255 , -0.02021325, -0.27979543,  0.24980777,\n         0.0905485 ,  0.17022998, -0.06770386, -0.18775935, -0.24235439,\n         0.07654221,  0.11996609,  0.11310511,  0.05321641, -0.08883605,\n        -0.06778878],\n       [-0.10007421,  0.01663193, -0.02039008, -0.27851825,  0.25031699,\n         0.0911504 ,  0.17025548, -0.06776567, -0.188116  , -0.24195005,\n         0.07708856,  0.1200282 ,  0.11410386,  0.05394367, -0.08844357,\n        -0.06749429],\n       [-0.26942242, -0.07012744,  0.10572221,  0.36574703, -0.23309319,\n        -0.19905962, -0.36364848,  0.16413438, -0.35887576,  0.51142001,\n         0.11590648, -0.13994236,  0.06325045,  0.37444047,  0.39924429,\n         0.29447756],\n       [-0.100098  ,  0.01661085, -0.02039944, -0.27845   ,  0.25034428,\n         0.09118247,  0.17025669, -0.06776889, -0.18813565, -0.24192823,\n         0.07711796,  0.12003153,  0.11415742,  0.05398296, -0.08842231,\n        -0.06747834],\n       [-0.10010214,  0.01660719, -0.02040107, -0.27843812,  0.25034903,\n         0.09118805,  0.1702569 , -0.06776945, -0.18813907, -0.24192444,\n         0.07712307,  0.12003211,  0.11416674,  0.0539898 , -0.08841861,\n        -0.06747557],\n       [-0.26939868, -0.07012326,  0.10572142,  0.36574992, -0.23311239,\n        -0.19906535, -0.363651  ,  0.16413502, -0.35884054,  0.51142038,\n         0.11589017, -0.13994905,  0.06322909,  0.37441486,  0.39923419,\n         0.29447029],\n       [-0.09982199,  0.01685731, -0.02028926, -0.27924904,  0.25002533,\n         0.09080641,  0.17024155, -0.06773065, -0.18790941, -0.24218229,\n         0.07677507,  0.11999263,  0.11353157,  0.05352569, -0.08866936,\n        -0.06766368],\n       [-0.26951245, -0.07014328,  0.10572523,  0.36573587, -0.23302031,\n        -0.19903787, -0.36363888,  0.16413193, -0.35900932,  0.51141849,\n         0.11596831, -0.13991695,  0.0633314 ,  0.37453755,  0.39928253,\n         0.29450508],\n       [-0.09973074,  0.01693972, -0.0202521 , -0.27951666,  0.24991872,\n         0.09068017,  0.170236  , -0.06771759, -0.18783544, -0.24226675,\n         0.07666086,  0.11997963,  0.11332254,  0.05337386, -0.08875123,\n        -0.06772511],\n       [ 0.28743743,  0.01623173,  0.02628907,  0.33571321, -0.45312219,\n        -0.16944723, -0.23351385,  0.09222372,  0.46835727,  0.29907626,\n        -0.21530357, -0.19634426, -0.30705018, -0.24865716,  0.03878727,\n         0.0324681 ],\n       [ 0.04322808, -0.1721532 , -0.17819817,  0.57456578,  0.33753522,\n         0.45587461,  0.22401722, -0.13494296,  0.21833669, -0.0649475 ,\n         0.16320919,  0.08315695,  0.44165567,  0.05783236, -0.06735433,\n        -0.04550343],\n       [ 0.04332185, -0.17212774, -0.17819971,  0.57452914,  0.33747097,\n         0.4558483 ,  0.22402138, -0.13494506,  0.2184668 , -0.06497446,\n         0.16314085,  0.08313957,  0.44156193,  0.05772175, -0.06740938,\n        -0.0455436 ],\n       [ 0.04306681, -0.17219679, -0.17819544,  0.57462775,  0.33764586,\n         0.45591967,  0.22401027, -0.13493938,  0.21811267, -0.06490165,\n         0.16332663,  0.08318699,  0.44181662,  0.05802242, -0.06725991,\n        -0.04543453],\n       [ 0.04314409, -0.17217594, -0.17819676,  0.57459822,  0.33759282,\n         0.45589811,  0.22401357, -0.13494109,  0.21822005, -0.06492354,\n         0.16327037,  0.08317258,  0.44173954,  0.05793138, -0.06730511,\n        -0.04546752],\n       [ 0.28682449,  0.0159531 ,  0.02624063,  0.3364502 , -0.45268636,\n        -0.16912473, -0.23358154,  0.09223459,  0.4676122 ,  0.29943945,\n        -0.21477707, -0.19627028, -0.30624174, -0.24783815,  0.03924307,\n         0.03280457],\n       [-0.099669  ,  0.01699575, -0.02022674, -0.27969872,  0.24984625,\n         0.0905942 ,  0.1702321 , -0.06770864, -0.18778564, -0.24232402,\n         0.07658333,  0.11997079,  0.1131805 ,  0.05327095, -0.08880668,\n        -0.06776673],\n       [-0.26936946, -0.07011811,  0.10572044,  0.36575346, -0.23313601,\n        -0.19907241, -0.36365408,  0.1641358 , -0.3587972 ,  0.51142081,\n         0.1158701 , -0.13995727,  0.0632028 ,  0.37438332,  0.39922174,\n         0.29446133],\n       [-0.10002127,  0.01667895, -0.02036914, -0.27867057,  0.25025613,\n         0.09107879,  0.17025272, -0.06775845, -0.18807238, -0.24199865,\n         0.07702302,  0.12002078,  0.11398439,  0.05385613, -0.08849091,\n        -0.06752979],\n       [ 0.04310764, -0.17218578, -0.17819614,  0.57461218,  0.33761783,\n         0.45590828,  0.22401201, -0.13494028,  0.21816942, -0.0649132 ,\n         0.16329691,  0.08317937,  0.4417759 ,  0.05797432, -0.06728379,\n        -0.04545195],\n       [-0.26967468, -0.07017169,  0.10573065,  0.365715  , -0.23288868,\n        -0.19899866, -0.36362127,  0.1641274 , -0.35925013,  0.51141521,\n         0.1160797 , -0.13987098,  0.06347722,  0.37471234,  0.39935117,\n         0.29455445],\n       [-0.10034585,  0.01639308, -0.02049561, -0.27774561,  0.25062629,\n         0.09151289,  0.17026828, -0.06780166, -0.18834197, -0.2417019 ,\n         0.07742262,  0.12006593,  0.11471137,  0.05439111, -0.08820122,\n        -0.06731256],\n       [-0.26929806, -0.07010549,  0.10571804,  0.36576195, -0.23319368,\n        -0.19908964, -0.36366155,  0.16413769, -0.35869134,  0.51142177,\n         0.11582105, -0.13997734,  0.06313856,  0.37430626,  0.39919127,\n         0.2944394 ],\n       [-0.09987078,  0.01681343, -0.02030898, -0.27910664,  0.25008209,\n         0.09087353,  0.1702444 , -0.06773755, -0.18794913, -0.24213722,\n         0.07683597,  0.11999956,  0.11364291,  0.05360674, -0.08862562,\n        -0.06763086],\n       [-0.26933457, -0.07011195,  0.10571927,  0.36575763, -0.23316419,\n        -0.19908083, -0.36365774,  0.16413673, -0.35874548,  0.51142129,\n         0.11584613, -0.13996708,  0.06317142,  0.37434568,  0.39920686,\n         0.29445062],\n       [-0.26937272, -0.07011868,  0.10572055,  0.36575306, -0.23313337,\n        -0.19907162, -0.36365374,  0.16413571, -0.35880204,  0.51142076,\n         0.11587234, -0.13995636,  0.06320574,  0.37438684,  0.39922313,\n         0.29446233],\n       [-0.10023791,  0.01648751, -0.02045405, -0.27805087,  0.25050398,\n         0.09136982,  0.17026346, -0.06778757, -0.18825175, -0.24180026,\n         0.07729031,  0.12005101,  0.11447104,  0.05421364, -0.08829742,\n        -0.06738468],\n       [ 0.04329853, -0.17213408, -0.17819933,  0.57453829,  0.33748694,\n         0.45585485,  0.22402034, -0.13494454,  0.21843445, -0.06496774,\n         0.16315785,  0.08314389,  0.44158525,  0.05774927, -0.06739567,\n        -0.0455336 ],\n       [-0.269436  , -0.07012983,  0.10572267,  0.36574537, -0.23308221,\n        -0.19905634, -0.36364704,  0.16413401, -0.35889589,  0.51141979,\n         0.1159158 , -0.13993853,  0.06326265,  0.37445511,  0.39925006,\n         0.29448171],\n       [ 0.28740536,  0.01621697,  0.02628642,  0.33575253, -0.45309928,\n        -0.16943006, -0.23351738,  0.09222425,  0.46831847,  0.29909547,\n        -0.21527588, -0.19634041, -0.30700752, -0.24861417,  0.03881122,\n         0.03248579],\n       [-0.26919468, -0.07008717,  0.10571457,  0.36577392, -0.23327704,\n        -0.19911457, -0.36367224,  0.16414038, -0.35853812,  0.51142292,\n         0.11575001, -0.14000631,  0.06304552,  0.37419461,  0.39914703,\n         0.29440756],\n       [ 0.04361645, -0.17204715, -0.17820435,  0.57441113,  0.33726941,\n         0.45576511,  0.224035  , -0.13495182,  0.21887505, -0.06506056,\n         0.16292576,  0.08308535,  0.44126654,  0.05737364, -0.06758308,\n        -0.04567039]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.10010352,  0.01660596],\n       [-0.02040161, -0.27843417]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f89ea87ac40>"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 6, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "m = parse_input_model()\n",
    "c1 = ConvolutionLayerBatch(6, 5, 1, 2)\n",
    "mnist_batch_1 = x_train[0:64].reshape(64, 28, 28, 1)\n",
    "o = c1.forward(mnist_batch_1)\n",
    "print(o.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.13991021e-01, 5.13991021e-01, 2.34726495e+00, 6.22690095e+00,\n        1.12072901e+01, 1.88108206e+01, 3.38048208e+01, 4.77333037e+01,\n        6.16420428e+01, 7.92249902e+01, 1.01786992e+02, 1.21545266e+02,\n        1.28731697e+02, 1.37192884e+02, 1.41634120e+02, 1.42932379e+02,\n        1.29960855e+02, 1.16581002e+02, 9.55354550e+01, 7.46750801e+01,\n        5.01304588e+01, 3.25171184e+01, 2.02950790e+01, 1.41210649e+01,\n        8.04039959e+00, 2.75846706e+00, 5.13991021e-01, 5.13991021e-01],\n       [4.87361328e-02, 4.87361328e-02, 1.88201006e+00, 5.76164606e+00,\n        1.07420352e+01, 1.83455657e+01, 3.33395659e+01, 4.72680488e+01,\n        6.11767879e+01, 7.87597353e+01, 1.01321737e+02, 1.21080011e+02,\n        1.28266442e+02, 1.36727629e+02, 1.41168866e+02, 1.42467124e+02,\n        1.29495600e+02, 1.16115747e+02, 9.50702001e+01, 7.42098252e+01,\n        4.96652039e+01, 3.20518635e+01, 1.98298241e+01, 1.36558100e+01,\n        7.57514470e+00, 2.29321217e+00, 4.87361328e-02, 4.87361328e-02],\n       [6.20273616e-01, 6.20273616e-01, 2.45354755e+00, 6.33318354e+00,\n        1.13135727e+01, 1.89171032e+01, 3.39111034e+01, 4.78395863e+01,\n        6.17483254e+01, 7.93312728e+01, 1.01893275e+02, 1.21651549e+02,\n        1.28837980e+02, 1.37299166e+02, 1.41740403e+02, 1.43038661e+02,\n        1.30067138e+02, 1.16687284e+02, 9.56417376e+01, 7.47813627e+01,\n        5.02367414e+01, 3.26234010e+01, 2.04013616e+01, 1.42273475e+01,\n        8.14668219e+00, 2.86474965e+00, 6.20273616e-01, 6.20273616e-01],\n       [1.67393803e-01, 1.67393803e-01, 2.00066774e+00, 5.88030373e+00,\n        1.08606929e+01, 1.84642234e+01, 3.34582236e+01, 4.73867065e+01,\n        6.12954456e+01, 7.88783930e+01, 1.01440395e+02, 1.21198669e+02,\n        1.28385100e+02, 1.36846287e+02, 1.41287523e+02, 1.42585781e+02,\n        1.29614258e+02, 1.16234405e+02, 9.51888578e+01, 7.43284829e+01,\n        4.97838616e+01, 3.21705212e+01, 1.99484817e+01, 1.37744677e+01,\n        7.69380237e+00, 2.41186984e+00, 1.67393803e-01, 1.67393803e-01],\n       [6.35028362e-01, 6.35028362e-01, 2.46830229e+00, 6.34793829e+00,\n        1.13283275e+01, 1.89318579e+01, 3.39258581e+01, 4.78543410e+01,\n        6.17630802e+01, 7.93460275e+01, 1.01908029e+02, 1.21666303e+02,\n        1.28852734e+02, 1.37313921e+02, 1.41755158e+02, 1.43053416e+02,\n        1.30081892e+02, 1.16702039e+02, 9.56564923e+01, 7.47961174e+01,\n        5.02514962e+01, 3.26381558e+01, 2.04161163e+01, 1.42421022e+01,\n        8.16143693e+00, 2.87950440e+00, 6.35028362e-01, 6.35028362e-01],\n       [9.62639272e-01, 9.62639272e-01, 2.79591320e+00, 6.67554920e+00,\n        1.16559384e+01, 1.92594689e+01, 3.42534690e+01, 4.81819519e+01,\n        6.20906911e+01, 7.96736384e+01, 1.02235640e+02, 1.21993914e+02,\n        1.29180345e+02, 1.37641532e+02, 1.42082769e+02, 1.43381027e+02,\n        1.30409503e+02, 1.17029650e+02, 9.59841032e+01, 7.51237284e+01,\n        5.05791071e+01, 3.29657667e+01, 2.07437272e+01, 1.45697131e+01,\n        8.48904784e+00, 3.20711531e+00, 9.62639272e-01, 9.62639272e-01]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0, :, :, 0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "(64, 6, 28, 28)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = ActivationLayer()\n",
    "o1 = a1.forward(o)\n",
    "o1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(64, 6, 14, 14)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = MaxPoolingLayerBatch(2, 2)\n",
    "o2 = m1.forward(o1)\n",
    "o2.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]],\n\n       [[13, 14, 15, 16],\n        [17, 18, 19, 20],\n        [21, 22, 23, 24]]])"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(1,25).reshape(2, 3, 4)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([12, 24])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.max(x, axis=(1, 2))\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}