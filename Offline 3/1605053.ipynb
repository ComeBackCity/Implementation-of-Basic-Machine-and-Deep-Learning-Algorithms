{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting Numpy Seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing MNIST Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "def process_mnist_data() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    mnist_path = './MNIST/'\n",
    "    train_images, train_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './train-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './train-labels.idx1-ubyte'\n",
    "    )\n",
    "    test_images, test_labels = loadlocal_mnist(\n",
    "        images_path = mnist_path + './t10k-images.idx3-ubyte',\n",
    "        labels_path = mnist_path + './t10k-labels.idx1-ubyte'\n",
    "    )\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing CIFAR-10 Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "def process_cifar_dataset() -> (np.ndarray, np.ndarray, np.ndarray, np.ndarray):\n",
    "    cifar_path = './cifar-10-python/cifar-10-batches-py'\n",
    "    data_batch = unpickle(cifar_path + '/data_batch_1')\n",
    "    train_images, train_labels = data_batch[b'data'], np.array(data_batch[b'labels'])\n",
    "    for i in range(2,6):\n",
    "        data_batch = unpickle(cifar_path + '/data_batch_' + str(i))\n",
    "        train_images = np.concatenate((train_images, data_batch[b'data']), axis=0)\n",
    "        train_labels = np.concatenate((train_labels, np.array(data_batch[b'labels'])), axis=0)\n",
    "    test_batch = unpickle(cifar_path + '/test_batch')\n",
    "    test_images, test_labels = test_batch[b'data'], np.array(test_batch[b'labels'])\n",
    "    return train_images, train_labels, test_images, test_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing Toy Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [],
   "source": [
    "def process_toy_dataset():\n",
    "    toy_dataset_path = './Toy Dataset/'\n",
    "    a = np.loadtxt(toy_dataset_path + 'trainNN.txt')\n",
    "    b = np.loadtxt(toy_dataset_path + 'testNN.txt')\n",
    "    train_x, train_y, test_x, test_y = a[:, 0:4], a[:, -1], b[:, 0:4], b[:, -1]\n",
    "    return train_x, train_y, test_x, test_y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 9.21323266, 11.82445528],\n       [16.69098092, 19.56967227]])"
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_toy, y_train_toy, x_test_toy, y_test_toy = process_toy_dataset()\n",
    "toy_batch_1 = x_train_toy[0:50].reshape(50, 1, 2, 2)\n",
    "toy_batch_1[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 0, 1, 0],\n       [0, 1, 0, 0],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 0, 1],\n       [1, 0, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1],\n       [0, 1, 0, 0],\n       [0, 0, 0, 1],\n       [0, 0, 1, 0]])"
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(1,5))\n",
    "toy_labels_1 = label_binarizer.transform(y_train_toy[0:50].T)\n",
    "toy_labels_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parsing Input Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def parse_input_model():\n",
    "    path = './input_model.txt'\n",
    "    model = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.split()\n",
    "            if tokens[0] == 'Conv':\n",
    "                model.append(ConvolutionLayerBatch(int(tokens[1]), int(tokens[2]), int(tokens[3]), int(tokens[4])))\n",
    "            if tokens[0] == 'ReLU':\n",
    "                model.append(ActivationLayer())\n",
    "            if tokens[0] == 'Pool':\n",
    "                model.append(MaxPoolingLayerBatch(int(tokens[1]), int(tokens[2])))\n",
    "            if tokens[0] == 'FC':\n",
    "                model.append(FlatteningLayerBatch())\n",
    "                model.append(FullyConnectedLayerBatch(int(tokens[1])))\n",
    "            if tokens[0] == 'Softmax':\n",
    "                model.append(SoftmaxLayerBatch())\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ReLU and ReLU Derivative Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def relu(matrix:np.ndarray) -> np.ndarray:\n",
    "    return matrix * (matrix > 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "def relu_derivative(matrix: np.ndarray) -> np.ndarray:\n",
    "    return (matrix > 0) * 1.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, input_image: np.ndarray) -> np.ndarray:\n",
    "        input_dimentions = input_image.shape[0]\n",
    "        output_dimentions = (input_dimentions - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_image.shape\n",
    "\n",
    "        filters = np.random.rand(\n",
    "            self.output_channel_count,\n",
    "            self.filter_dimension,\n",
    "            self.filter_dimension,\n",
    "            input_shape[2]\n",
    "        )\n",
    "\n",
    "        bias = np.random.rand(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_image, [(self.padding,self.padding), (self.padding,self.padding), (0,0)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((output_dimentions, output_dimentions, self.output_channel_count))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= padded_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= padded_dimensions[0]:\n",
    "                image_slice = padded_image[image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.sum(image_slice * filters) + bias\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "class ConvolutionLayerBatch:\n",
    "    def __init__(self, output_channel_count: int, filter_dimension: int, stride: int, padding: int):\n",
    "        self.output_channel_count = output_channel_count\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.bias = None\n",
    "        self.filters = None\n",
    "        self.input_batch = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        self.input_batch = input_batch\n",
    "\n",
    "        input_dimentions = input_batch.shape\n",
    "        output_dimentions = (input_dimentions[2] - self.filter_dimension + 2 * self.padding) // self.stride + 1\n",
    "        input_shape = input_batch.shape\n",
    "\n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(\n",
    "                self.output_channel_count,\n",
    "                input_shape[1],\n",
    "                self.filter_dimension,\n",
    "                self.filter_dimension\n",
    "            ) * np.sqrt(2/input_shape[1] * self.filter_dimension ** 2)\n",
    "\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.output_channel_count)\n",
    "\n",
    "        padded_image = np.pad(input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_image /= 255.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        output = np.zeros((input_dimentions[0], self.output_channel_count, output_dimentions, output_dimentions))\n",
    "\n",
    "        for i in range(input_dimentions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+self.filter_dimension, image_y:image_y+self.filter_dimension]\n",
    "                    output[i, :, out_x, out_y] = np.sum(image_slice * self.filters) + self.bias\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dz: np.ndarray, learning_rate:float = 10e-6) -> np.ndarray:\n",
    "        batch_size = dz.shape[0]\n",
    "        db = np.sum(dz, axis=(0, 2, 3))\n",
    "        self.bias = self.bias - learning_rate * db / batch_size\n",
    "        padded_image = np.pad(self.input_batch, [(0, 0), (0, 0), (self.padding,self.padding), (self.padding,self.padding)], mode='constant') * 1.0\n",
    "        padded_dimensions = padded_image.shape\n",
    "\n",
    "        dw = np.zeros(self.filters.shape)\n",
    "        dz_prime_dim = (dz.shape[2] - 1) * self.stride + 1\n",
    "        dz_prime = np.zeros((dz.shape[0], dz.shape[1], dz_prime_dim, dz_prime_dim))\n",
    "        dz_prime[:, :, ::self.stride, ::self.stride] = dz\n",
    "\n",
    "        # calculate dw\n",
    "        for i in range(padded_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + dz_prime_dim <= padded_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + dz_prime_dim <= padded_dimensions[2]:\n",
    "                    image_slice = padded_image[i, :, image_x:image_x+dz_prime_dim, image_y:image_y+dz_prime_dim]\n",
    "                    dz_slice = dz_prime[i, :, :, :]\n",
    "                    dz_slice_shape = dz_slice.shape\n",
    "                    dz_slice = np.broadcast_to(dz_slice, (image_slice.shape[0], dz_slice_shape[0], dz_slice_shape[1], dz_slice_shape[2])).transpose((1, 0, 2, 3))\n",
    "                    dw[:, :, out_x, out_y] += np.sum(image_slice * dz_slice, axis=(2, 3))\n",
    "                    image_x += 1\n",
    "                    out_x += 1\n",
    "                image_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        rotated_filter = np.rot90(self.filters, 2, axes=(2, 3))\n",
    "        dx = np.zeros(self.input_batch.shape)\n",
    "        padding = self.filter_dimension - 1 - self.padding\n",
    "        if padding < 0:\n",
    "            dz_prime_padded = dz_prime[:, :, -padding:padding, -padding:padding]\n",
    "        else:\n",
    "            dz_prime_padded = np.pad(dz_prime, [(0, 0), (0, 0), (padding, padding), (padding, padding)], mode='constant')\n",
    "\n",
    "        dz_padded_dimensions = dz_prime_padded.shape\n",
    "\n",
    "        # calculate dx\n",
    "        for i in range(dz_padded_dimensions[0]):\n",
    "            dz_y = out_y = 0\n",
    "            while dz_y + self.filter_dimension <= dz_padded_dimensions[3]:\n",
    "                dz_x = out_x = 0\n",
    "                while dz_x + self.filter_dimension <= dz_padded_dimensions[2]:\n",
    "                    dzp_slice = dz_prime_padded[i, :, dz_x:dz_x+self.filter_dimension, dz_y:dz_y+self.filter_dimension]\n",
    "                    dzp_slice = dzp_slice.reshape(dz_padded_dimensions[1], 1, self.filter_dimension, self.filter_dimension)\n",
    "                    dx[i, :, out_x, out_y] = np.sum(dzp_slice * rotated_filter, axis=(0, 2, 3))\n",
    "                    dz_x += 1\n",
    "                    out_x += 1\n",
    "                dz_y += 1\n",
    "                out_y += 1\n",
    "\n",
    "        self.filters -= learning_rate * dw / batch_size\n",
    "        return dx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv = ConvolutionLayerBatch(4, 2, 2, 2)\n",
    "test_conv_out = test_conv.forward(toy_batch_1)\n",
    "test_conv_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        , -0.34705184,  0.        ],\n       [ 0.        ,  0.        ,  0.        ]])"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return relu(input_matrix)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        return input_matrix * relu_derivative(input_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation = ActivationLayer()\n",
    "test_activation_out = test_activation.forward(test_conv_out)\n",
    "test_activation_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0.],\n       [ 0., -0.,  0.],\n       [ 0.,  0.,  0.]])"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Max Pooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        output_dimension = (input_dimensions[0] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((output_dimension, output_dimension, input_dimensions[2]))\n",
    "\n",
    "        image_y = out_y = 0\n",
    "        while image_y + self.filter_dimension <= input_dimensions[1]:\n",
    "            image_x = out_x = 0\n",
    "            while image_x + self.filter_dimension <= input_dimensions[0]:\n",
    "                image_slice = image[image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension, :]\n",
    "                output[out_x, out_y, :] = np.max(image_slice, axis=(0, 1))\n",
    "                image_x += self.stride\n",
    "                out_x += 1\n",
    "            image_y += self.stride\n",
    "            out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "class MaxPoolingLayerBatch:\n",
    "    def __init__(self, filter_dimension: int, stride: int):\n",
    "        self.filter_dimension = filter_dimension\n",
    "        self.stride = stride\n",
    "        self.mask = None\n",
    "        self.input_dimensions = None\n",
    "\n",
    "    def forward(self, image: np.ndarray) -> np.ndarray:\n",
    "        input_dimensions = image.shape\n",
    "        self.input_dimensions = input_dimensions\n",
    "        output_dimension = (input_dimensions[2] - self.filter_dimension) // self.stride + 1\n",
    "\n",
    "        output = np.zeros((input_dimensions[0], input_dimensions[1], output_dimension, output_dimension))\n",
    "        self.mask = np.zeros(input_dimensions)\n",
    "\n",
    "        for i in range(input_dimensions[0]):\n",
    "            image_y = out_y = 0\n",
    "            while image_y + self.filter_dimension <= input_dimensions[3]:\n",
    "                image_x = out_x = 0\n",
    "                while image_x + self.filter_dimension <= input_dimensions[2]:\n",
    "                    image_slice = image[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension]\n",
    "                    max_val = np.max(image_slice, axis=(1, 2))\n",
    "                    output[i, :, out_x, out_y] = max_val\n",
    "                    self.mask[i, :, image_x: image_x+self.filter_dimension, image_y: image_y+self.filter_dimension] = image_slice == max_val.reshape(input_dimensions[1], 1, 1)\n",
    "                    image_x += self.stride\n",
    "                    out_x += 1\n",
    "                image_y += self.stride\n",
    "                out_y += 1\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dh:np.ndarray) -> np.ndarray:\n",
    "        output = np.zeros(self.input_dimensions)\n",
    "\n",
    "        for i in range(self.input_dimensions[0]):\n",
    "            out_y = dh_y = 0\n",
    "            while out_y + self.filter_dimension <= self.input_dimensions[3]:\n",
    "                out_x = dh_x = 0\n",
    "                while out_x + self.filter_dimension <= self.input_dimensions[2]:\n",
    "                    mask_patch = self.mask[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension]\n",
    "                    output[i, :, out_x: out_x+self.filter_dimension, out_y: out_y+self.filter_dimension] += mask_patch * dh[i, :, dh_x, dh_y].reshape(self.input_dimensions[1], 1, 1)\n",
    "                    out_x += self.stride\n",
    "                    dh_x += 1\n",
    "                out_y += self.stride\n",
    "                dh_y += 1\n",
    "\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool = MaxPoolingLayerBatch(2, 1)\n",
    "test_maxpool_out = test_maxpool.forward(test_activation_out)\n",
    "test_maxpool_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.],\n       [ 0., -0.]])"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_out[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "class FlatteningLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(image: np.ndarray) -> np.ndarray:\n",
    "        return image.flatten().reshape(-1, 1)\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "class FlatteningLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.input_shape = None\n",
    "\n",
    "    def forward(self, input_batch: np.ndarray) -> np.ndarray:\n",
    "        input_shape = input_batch.shape\n",
    "        self.input_shape = input_shape\n",
    "        return input_batch.reshape((input_shape[0], -1))\n",
    "\n",
    "    def backward(self, dh_flattened: np.ndarray) -> np.ndarray:\n",
    "        return dh_flattened.reshape(self.input_shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 16)"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening = FlatteningLayerBatch()\n",
    "test_flattening_out = test_flattening.forward(test_maxpool_out)\n",
    "test_flattening_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.],\n       [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n         0.,  0., -0.]])"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        weights = np.random.rand(flattened_input.shape[0], self.output_dimension)\n",
    "        bias = np.random.rand(self.output_dimension, 1)\n",
    "\n",
    "        return weights.T @ flattened_input + bias\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "class FullyConnectedLayerBatch:\n",
    "    def __init__(self, output_dimension: int):\n",
    "        self.output_dimension = output_dimension\n",
    "        self.input_matrix = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, flattened_input: np.ndarray) -> np.ndarray:\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * np.sqrt(2/flattened_input.shape[1])\n",
    "            # self.weights = np.random.randn(flattened_input.shape[1], self.output_dimension) * 0.01\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros((1, self.output_dimension))\n",
    "        self.input_matrix = flattened_input\n",
    "\n",
    "        return flattened_input @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, d_theta: np.ndarray, learning_rate: float = 10e-4) -> np.ndarray:\n",
    "        n = d_theta.shape[0]\n",
    "        dw = self.input_matrix.T @ d_theta\n",
    "        db = np.sum(d_theta, axis=0, keepdims=True)\n",
    "        dh = d_theta @ self.weights.T\n",
    "        self.weights = self.weights - learning_rate * dw / n\n",
    "        self.bias = self.bias - learning_rate * db / n\n",
    "\n",
    "        return dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc = FullyConnectedLayerBatch(4)\n",
    "test_fc_out = test_fc.forward(test_flattening_out)\n",
    "test_fc_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(input_matrix: np.ndarray) -> np.ndarray:\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp /= np.sum(exp)\n",
    "        return exp\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "class SoftmaxLayerBatch:\n",
    "    def __init__(self):\n",
    "        self.y_hat = None\n",
    "\n",
    "    def forward(self, input_matrix: np.ndarray) -> np.ndarray:\n",
    "        input_matrix -= np.max(input_matrix)\n",
    "        exp = np.exp(input_matrix)\n",
    "        exp_sum = np.sum(exp, axis=1).reshape(-1, 1)\n",
    "        exp /= exp_sum\n",
    "        self.y_hat = exp\n",
    "        return exp\n",
    "\n",
    "    def backward(self, y: np.ndarray) -> np.ndarray:\n",
    "        return self.y_hat - y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4)"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax = SoftmaxLayerBatch()\n",
    "test_softmax_out = test_softmax.forward(test_fc_out)\n",
    "test_softmax_out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25],\n       [0.25, 0.25, 0.25, 0.25]])"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backprop Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Loss Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    labels = y_true * np.log(y_pred) * -1.0\n",
    "    return np.sum(labels) / y_true.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "data": {
      "text/plain": "1.3862943611198904"
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function_test = loss_function(toy_labels_1, test_softmax_out)\n",
    "loss_function_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Softmax Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.75,  0.25,  0.25,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [-0.75,  0.25,  0.25,  0.25],\n       [ 0.25,  0.25, -0.75,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25, -0.75,  0.25,  0.25],\n       [ 0.25,  0.25,  0.25, -0.75],\n       [ 0.25,  0.25, -0.75,  0.25]])"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_softmax_back = test_softmax.backward(toy_labels_1)\n",
    "print(test_softmax_back.shape)\n",
    "test_softmax_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fully Connected Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [-0.17882558,  0.22368029, -0.13573111,  0.07236715, -0.31792458,\n         0.20858897, -0.22713849, -0.19504869, -0.13525172, -0.30938507,\n         0.14336458, -0.29243851, -0.63778565,  0.27575066, -0.63413681,\n        -0.41074099],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [ 0.4699357 , -0.24439348,  0.57291641, -0.13981712,  0.22475302,\n         0.08249373, -0.20005411,  0.02328059,  0.05248093,  0.11655076,\n        -0.01385192,  0.32337023, -0.01794242,  0.20250966, -0.07631876,\n        -0.07433409],\n       [-0.43450874,  0.36310455, -0.3281631 ,  0.32941578, -0.20327875,\n        -0.38656347,  0.48400231,  0.01091864,  0.35476599, -0.00829376,\n        -0.12535082, -0.11264001,  0.38030633, -0.10344436,  0.60155579,\n         0.46925633],\n       [ 0.14339862, -0.34239136, -0.1090222 , -0.26196581,  0.2964503 ,\n         0.09548077, -0.05680971,  0.16084945, -0.2719952 ,  0.20112808,\n        -0.00416184,  0.08170829,  0.27542174, -0.37481596,  0.10889978,\n         0.01581875]])"
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fc_back = test_fc.backward(test_softmax_back, learning_rate=0.01)\n",
    "print(test_fc_back.shape)\n",
    "test_fc_back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Flattening Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.17882558,  0.22368029],\n       [-0.13573111,  0.07236715]])"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_flattening_back = test_flattening.backward(test_fc_back)\n",
    "test_flattening_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### MaxPooling Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 2, 2)"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back = test_maxpool.backward(test_flattening_back)\n",
    "test_flattening_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.17882558,  0.04485471,  0.22368029],\n       [-0.31455669, -0.01850924,  0.29604745],\n       [-0.13573111, -0.06336395,  0.07236715]])"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_maxpool_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Activation Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 4, 3, 3)"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back = test_activation.backward(test_maxpool_back)\n",
    "test_activation_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.        ,  0.04485471,  0.22368029],\n       [-0.        , -0.        ,  0.29604745],\n       [-0.        , -0.        ,  0.07236715]])"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_activation_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Convolution Layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "(50, 1, 2, 2)"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back = test_conv.backward(test_activation_back, learning_rate=0.01)\n",
    "test_conv_back.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0.],\n       [0., 0.]])"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_back[0, 0, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fe52083d820>"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = process_mnist_data()\n",
    "img = x_train[0].reshape(28, 28, 1)\n",
    "plt.imshow(img, cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "(32, 1, 28, 28)"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = parse_input_model()\n",
    "mnist_batch_1 = x_train[0:32].reshape(32, 1, 28, 28)\n",
    "mnist_labels_1 = y_train[0:32]\n",
    "mnist_batch_1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "# train\n",
    "random_index = random.sample(range(0, 10000), 4992)\n",
    "mnist_subsample_x = x_train[random_index]\n",
    "mnist_subsample_y = y_train[random_index]\n",
    "# validation\n",
    "mnist_validation_x = x_test[:2000]\n",
    "mnist_validation_y = y_test[:2000]\n",
    "# test\n",
    "mnist_test_x = x_test[5001:7001]\n",
    "mnist_test_y = y_test[5001:7001]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(0,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "validation_batch = mnist_validation_x.reshape(2000, 1, 28, 28)\n",
    "validation_labels = label_binarizer.transform(mnist_validation_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "test_batch = mnist_test_x.reshape(2000, 1, 28, 28)\n",
    "test_labels = label_binarizer.transform(mnist_test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "def measure_accuracy(y_true, y_pred):\n",
    "    accurate = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total = y_true.shape[0]\n",
    "    return accurate / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "def predict_labels(a):\n",
    "    return (a == a.max(axis=1)[:,None]).astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# inp = mnist_batch_1\n",
    "# for layer in model:\n",
    "#     inp = layer.forward(inp)\n",
    "#\n",
    "# labels_true = label_binarizer.transform(mnist_labels_1)\n",
    "# l = loss_function(labels_true, inp)\n",
    "#\n",
    "# out = labels_true\n",
    "# for layer in reversed(model):\n",
    "#     out = layer.backward(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     losses = []\n",
    "#     index = [i for i in range(1,157)]\n",
    "#     for j in range(0, 4992, 32):\n",
    "#         batch_x = mnist_subsample_x[j:j+32].reshape(32, 1, 28, 28)\n",
    "#         batch_y = mnist_subsample_y[j:j+32]\n",
    "#         model_out = batch_x\n",
    "#         # train\n",
    "#         for layer in model:\n",
    "#             # print(layer)\n",
    "#             model_out = layer.forward(model_out)\n",
    "#             # print(model_out.shape)\n",
    "#\n",
    "#         true_labels = label_binarizer.transform(batch_y)\n",
    "#         l = loss_function(true_labels, model_out)\n",
    "#         losses.append(l)\n",
    "#         # print(\"Epoc {} batch {} loss = {}\".format(i, j//32, l))\n",
    "#\n",
    "#\n",
    "#         model_back = true_labels\n",
    "#         for layer in reversed(model):\n",
    "#             # print(layer)\n",
    "#             model_back = layer.backward(model_back)\n",
    "#             # print(model_back.shape)\n",
    "#\n",
    "#     plt.plot(index, losses)\n",
    "#     plt.show()\n",
    "#\n",
    "#     #validation\n",
    "#     validation_out = validation_batch\n",
    "#     for layer in model:\n",
    "#         validation_out = layer.forward(validation_out)\n",
    "#     validation_loss = loss_function(validation_labels, validation_out)\n",
    "#     print('Validation loss after epoc {} is {}'.format(i, validation_loss))\n",
    "#     validation_predictions = predict_labels(validation_out)\n",
    "#     accuracy = measure_accuracy(validation_labels, validation_predictions)\n",
    "#     print('Validation accuracy after epoc {} is {}'.format(i, accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "LabelBinarizer()"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_binarizer_toy = LabelBinarizer()\n",
    "label_binarizer_toy.fit(range(1,5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "toy_model = [FullyConnectedLayerBatch(4), SoftmaxLayerBatch()]\n",
    "x_train, y_train, x_test, y_test = process_toy_dataset()\n",
    "x_validation, y_validation = x_test[:250], y_test[:250]\n",
    "x_test, y_test = x_test[250:], y_test[250:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "y_validation = label_binarizer_toy.transform(y_validation)\n",
    "y_test = label_binarizer_toy.transform(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_losses = []\n",
    "validation_accuracy = []\n",
    "validation_index = [i for i in range(1,12001)]\n",
    "for i in range(12000):\n",
    "    losses = []\n",
    "    index = [i for i in range(1,21)]\n",
    "    for j in range(0, 500, 25):\n",
    "        batch_x = x_train[j:j+25]\n",
    "        batch_y = y_train[j:j+25]\n",
    "        model_out = batch_x\n",
    "        # train\n",
    "        for layer in toy_model:\n",
    "            model_out = layer.forward(model_out)\n",
    "\n",
    "        true_labels = label_binarizer_toy.transform(batch_y)\n",
    "        l = loss_function(true_labels, model_out)\n",
    "        losses.append(l)\n",
    "\n",
    "        model_back_toy = true_labels\n",
    "        for layer in reversed(toy_model):\n",
    "            model_back_toy = layer.backward(model_back_toy)\n",
    "\n",
    "    #validation\n",
    "    validation_out = x_validation\n",
    "    for layer in toy_model:\n",
    "        validation_out = layer.forward(validation_out)\n",
    "    validation_loss = loss_function(y_validation, validation_out)\n",
    "    validation_losses.append(validation_loss)\n",
    "    validation_predictions = predict_labels(validation_out)\n",
    "    accuracy = measure_accuracy(y_validation, validation_predictions)\n",
    "    validation_accuracy.append(accuracy*100)\n",
    "\n",
    "# print(validation_losses)\n",
    "plt.plot(validation_index, validation_losses)\n",
    "plt.show()\n",
    "plt.plot(validation_index, validation_accuracy)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out = x_test\n",
    "for layer in toy_model:\n",
    "    test_out = layer.forward(test_out)\n",
    "test_prediction = predict_labels(test_out)\n",
    "accuracy = measure_accuracy(y_test, test_prediction)\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}