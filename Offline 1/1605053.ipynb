{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# information gain function\n",
    "def info_gain(df: DataFrame):\n",
    "    all_features = list(df.columns)\n",
    "    y_feature = all_features.pop(len(all_features) - 1)\n",
    "    data = df.copy()\n",
    "    y = data.pop(y_feature)\n",
    "    x = data\n",
    "    importances = mutual_info_classif(x, y)\n",
    "    info_gain_map = {}\n",
    "    for feature, gain in zip(all_features, importances):\n",
    "        info_gain_map[feature] = gain\n",
    "\n",
    "    print(json.dumps(info_gain_map, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# One-Hot encoding\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    dummies = dummies.iloc[:, :-1]\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pre-processor 1\n",
    "def read_data():\n",
    "    telco_data = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
    "                             converters={\n",
    "                                 'gender': lambda x: int(x == 'Female'),\n",
    "                                 'Partner': lambda x: int(x == 'Yes'),\n",
    "                                 'Dependents': lambda x: int(x == 'Yes'),\n",
    "                                 'PhoneService': lambda x: int(x =='Yes'),\n",
    "                                 'PaperlessBilling': lambda x: int(x =='Yes'),\n",
    "                                 'Churn': lambda x: int(x =='Yes'),\n",
    "                             })\n",
    "\n",
    "    return telco_data\n",
    "\n",
    "def process_data(telco_data):\n",
    "    telco_data.drop('customerID', axis=1, inplace=True)\n",
    "    telco_data = telco_data.astype({\n",
    "        'tenure': int,\n",
    "        \"MonthlyCharges\": float,\n",
    "        \"TotalCharges\": float\n",
    "    }, errors=\"ignore\")\n",
    "\n",
    "    total_charges_median = (telco_data['TotalCharges'].loc[telco_data['TotalCharges'] != ' ']).median()\n",
    "    telco_data['TotalCharges'].replace([' '], total_charges_median, regex=True, inplace=True)\n",
    "\n",
    "    columns_to_encode = ['MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "                         'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                         'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "    for column in columns_to_encode:\n",
    "        telco_data = encode_and_bind(telco_data, column)\n",
    "\n",
    "    # Move final column for better visualization\n",
    "    telco_data.insert(len(telco_data.columns)-1, 'Churn', telco_data.pop('Churn'))\n",
    "\n",
    "    all_columns = list(telco_data.columns)\n",
    "    telco_data[all_columns] = MinMaxScaler().fit_transform(telco_data[all_columns])\n",
    "\n",
    "    return telco_data\n",
    "\n",
    "def preprocess():\n",
    "    telco_data = read_data()\n",
    "    telco_data = process_data(telco_data)\n",
    "    telco_data.to_csv('telco.csv')\n",
    "    return telco_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(y_predicted, y_actual, size):\n",
    "    error = 0.5 * np.sum((y_actual - y_predicted) ** 2) / size\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def accuracy(y_predicted, y_actual):\n",
    "    return  np.sum(y_actual == y_predicted) / y_actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function for determining label of hypothesis\n",
    "def predict(hypothesis):\n",
    "    labels = np.array([1.0 if it > 0 else -1.0 for it in hypothesis])\n",
    "    labels = labels.reshape((labels.shape[0], 1))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "def train(x, y, early_terminate_threshold=0.0, learning_rate=0.0001, no_of_iterations=5000):\n",
    "    no_of_data, no_of_features = x.shape\n",
    "    w = np.random.rand(no_of_features, 1)\n",
    "    error = 0\n",
    "    for i in range(no_of_iterations):\n",
    "        z = np.dot(x, w)\n",
    "        h = np.tanh(z)\n",
    "        error = loss(h, y, no_of_data)\n",
    "        if error < early_terminate_threshold:\n",
    "            break\n",
    "        gradient = np.dot(x.T, (y - h) * (1 - h ** 2))\n",
    "        w += learning_rate * gradient\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resample function for adaboost\n",
    "def resample(x, y, w):\n",
    "    indices = np.random.choice(x.shape[0], x.shape[0], replace=True, p=w )\n",
    "    x_data = x[indices]\n",
    "    y_data = y[indices]\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "def adaboost(example_x, example_y, k):\n",
    "    no_of_data = example_x.shape[0]\n",
    "    w = np.array([1/no_of_data] * no_of_data)\n",
    "    h = []\n",
    "    z = []\n",
    "    for i in range(k):\n",
    "        x_data, y_data = resample(example_x, example_y, w)\n",
    "        w_learn = train(x_data, y_data, early_terminate_threshold=0.5)\n",
    "        h_k = np.tanh(np.dot(example_x, w_learn))\n",
    "        h_k = predict(h_k)\n",
    "        error = 0\n",
    "        for j in range(no_of_data):\n",
    "            if h_k[j] != example_y[j]:\n",
    "                error += w[j]\n",
    "\n",
    "        if error > 0.5:\n",
    "            continue\n",
    "\n",
    "        for j in range(no_of_data):\n",
    "            if h_k[j] == example_y[j]:\n",
    "                w[j] = w[j] * (error / (1-error))\n",
    "\n",
    "        w /= np.sum(w)\n",
    "        h.append(w_learn)\n",
    "        z.append(math.log((1-error)/error, 2))\n",
    "\n",
    "    return h, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = preprocess()\n",
    "data.insert(0, 'Ones', 1.0)\n",
    "data = data.to_numpy()\n",
    "rows, columns = data.shape\n",
    "\n",
    "data_x = data[:, :-1]\n",
    "data_y = data[:, -1]\n",
    "\n",
    "data_y = np.array([1.0 if it > 0 else -1.0 for it in data_y])\n",
    "data_y = data_y.reshape((data_y.shape[0], 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy 0.7012065294535131.\n"
     ]
    }
   ],
   "source": [
    "# logistic regression test\n",
    "w_logi = train(x_train, y_train, early_terminate_threshold=0.5)\n",
    "h_logi = np.tanh(np.dot(x_test, w_logi))\n",
    "h_logi = predict(h_logi)\n",
    "print('Logistic regression accuracy {}.'.format(accuracy(h_logi, y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy 0.7963094393186657.\n"
     ]
    }
   ],
   "source": [
    "# adaboost test\n",
    "h_ada, z_ada = adaboost(x_train, y_train, 10)\n",
    "\n",
    "hypo = np.zeros(y_test.shape)\n",
    "for h, z in zip(h_ada, z_ada):\n",
    "    l = np.tanh(np.dot(x_test, h))\n",
    "    h_out = predict(l)\n",
    "    hypo += z * l\n",
    "\n",
    "hypo /= sum(z_ada)\n",
    "\n",
    "h_out = predict(hypo)\n",
    "print('Logistic regression accuracy {}.'.format(accuracy(h_out, y_test)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}