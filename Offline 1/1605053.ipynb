{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import math\n",
    "import random\n",
    "\n",
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.display import display\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(a=2)\n",
    "np.random.seed(5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# information gain function\n",
    "def info_gain(df: DataFrame):\n",
    "    all_features = list(df.columns)\n",
    "    y_feature = all_features.pop(len(all_features) - 1)\n",
    "    data = df.copy()\n",
    "    y = data.pop(y_feature)\n",
    "    x = data\n",
    "    importances = mutual_info_classif(x, y)\n",
    "    info_gain_map = {\n",
    "        feature: gain for feature, gain in zip(all_features, importances)\n",
    "    }\n",
    "\n",
    "    info_gain_map = {k: v for k, v in sorted(info_gain_map.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return list(info_gain_map.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# One-Hot encoding\n",
    "def encode_and_bind(original_dataframe, feature_to_encode):\n",
    "    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n",
    "    dummies = dummies.iloc[:, :-1]\n",
    "    res = pd.concat([original_dataframe, dummies], axis=1)\n",
    "    res = res.drop([feature_to_encode], axis=1)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pre-processor 1\n",
    "def read_telco_data():\n",
    "    return pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv',\n",
    "                             converters={\n",
    "                                 'gender': lambda x: int(x == 'Female'),\n",
    "                                 'Partner': lambda x: int(x == 'Yes'),\n",
    "                                 'Dependents': lambda x: int(x == 'Yes'),\n",
    "                                 'PhoneService': lambda x: int(x =='Yes'),\n",
    "                                 'PaperlessBilling': lambda x: int(x =='Yes'),\n",
    "                                 'Churn': lambda x: int(x =='Yes'),\n",
    "                             })\n",
    "\n",
    "def process_telco_data(telco_data):\n",
    "    telco_data.drop('customerID', axis=1, inplace=True)\n",
    "    telco_data = telco_data.astype({\n",
    "        'tenure': int,\n",
    "        \"MonthlyCharges\": float,\n",
    "        \"TotalCharges\": float\n",
    "    }, errors=\"ignore\")\n",
    "\n",
    "    total_charges_median = (telco_data['TotalCharges'].loc[telco_data['TotalCharges'] != ' ']).median()\n",
    "    telco_data['TotalCharges'].replace([' '], total_charges_median, regex=True, inplace=True)\n",
    "\n",
    "    columns_to_encode = ['MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "                         'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "                         'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "    for column in columns_to_encode:\n",
    "        telco_data = encode_and_bind(telco_data, column)\n",
    "\n",
    "    # Move final column for better visualization\n",
    "    telco_data.insert(len(telco_data.columns)-1, 'Churn', telco_data.pop('Churn'))\n",
    "\n",
    "    all_columns = list(telco_data.columns)\n",
    "    telco_data[all_columns] = MinMaxScaler().fit_transform(telco_data[all_columns])\n",
    "\n",
    "    return telco_data\n",
    "\n",
    "def preprocess_telco_data():\n",
    "    telco_data = read_telco_data()\n",
    "    telco_data = process_telco_data(telco_data)\n",
    "    telco_data.to_csv('telco.csv')\n",
    "    return telco_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "outputs": [],
   "source": [
    "# pre processor 2\n",
    "def read_adult_data(file_name):\n",
    "    column_names = ['C'+str(i) for i in range(15)]\n",
    "    return pd.read_csv(file_name,\n",
    "                         names=column_names,\n",
    "                         header=None,\n",
    "                         sep=' *, * ',\n",
    "                         engine=\"python\",\n",
    "                         converters={\n",
    "                            'C9': lambda x: float(x == 'Male'),\n",
    "                            'C14': lambda x: float(x == '>50K')\n",
    "                         })\n",
    "\n",
    "def process_adult_data(adult_data, test=False):\n",
    "    all_columns = list(adult_data.columns)\n",
    "    missing_value_columns = [\n",
    "        column\n",
    "        for column in all_columns\n",
    "        if '?' in adult_data[column].values.tolist()\n",
    "    ]\n",
    "\n",
    "    for column in missing_value_columns:\n",
    "        adult_data[column].replace(['?'], adult_data[column].mode(), inplace=True)\n",
    "\n",
    "    columns_to_encode = ['C1', 'C3', 'C5', 'C6', 'C7', 'C8', 'C13']\n",
    "\n",
    "    for column in columns_to_encode:\n",
    "        adult_data = encode_and_bind(adult_data, column)\n",
    "\n",
    "    if test:\n",
    "        column_to_add = [0.0] * adult_data.shape[0]\n",
    "        adult_data['C13_Holand-Netherlands'] = column_to_add\n",
    "\n",
    "    adult_data.insert(len(adult_data.columns)-1, 'C14', adult_data.pop('C14'))\n",
    "\n",
    "    all_columns = list(adult_data.columns)\n",
    "    adult_data[all_columns] = MinMaxScaler().fit_transform(adult_data[all_columns])\n",
    "\n",
    "    return adult_data\n",
    "\n",
    "def preprocess_adult_data():\n",
    "    adult_data = read_adult_data('adult.csv')\n",
    "    adult_data = process_adult_data(adult_data)\n",
    "    adult_data.to_csv('adult-data.csv')\n",
    "\n",
    "    adult_test = read_adult_data('adult.test.csv')\n",
    "    adult_test = process_adult_data(adult_test, test=True)\n",
    "    adult_test.to_csv('adult-test.csv')\n",
    "\n",
    "    return adult_data, adult_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(y_predicted, y_actual, size):\n",
    "    return 0.5 * np.sum((y_actual - y_predicted) ** 2) / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def accuracy(y_predicted, y_actual):\n",
    "    return np.sum(y_actual == y_predicted) / y_actual.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prediction function for determining label of hypothesis\n",
    "def predict(hypothesis):\n",
    "    labels = np.array([1.0 if it > 0 else -1.0 for it in hypothesis])\n",
    "    labels = labels.reshape((labels.shape[0], 1))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "def train(x, y, early_terminate_threshold=0.0, learning_rate=0.0001, no_of_iterations=1000):\n",
    "    no_of_data, no_of_features = x.shape\n",
    "    w = np.random.rand(no_of_features, 1)\n",
    "    for _ in range(no_of_iterations):\n",
    "        z = np.dot(x, w)\n",
    "        h = np.tanh(z)\n",
    "        # y_pred = predict(h)\n",
    "        # error = 1 - accuracy(y_pred, y)\n",
    "        error = loss(h, y, no_of_data)\n",
    "        if error < early_terminate_threshold:\n",
    "            break\n",
    "        gradient = np.dot(x.T, (y - h) * (1 - h ** 2))\n",
    "        w += learning_rate * gradient\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resample function for adaboost\n",
    "def resample(x, y, w):\n",
    "    indices = np.random.choice(x.shape[0], x.shape[0], replace=True, p=w )\n",
    "    x_data = x[indices]\n",
    "    y_data = y[indices]\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "def adaboost(example_x, example_y, k):\n",
    "    no_of_data = example_x.shape[0]\n",
    "    w = np.array([1/no_of_data] * no_of_data)\n",
    "    h = []\n",
    "    z = []\n",
    "    for _ in range(k):\n",
    "        x_data, y_data = resample(example_x, example_y, w)\n",
    "        w_learn = train(x_data, y_data, early_terminate_threshold=0.5)\n",
    "        h_k = np.tanh(np.dot(example_x, w_learn))\n",
    "        h_k = predict(h_k)\n",
    "        error = sum(w[j] for j in range(no_of_data) if h_k[j] != example_y[j])\n",
    "        if error > 0.5:\n",
    "            continue\n",
    "\n",
    "        for j in range(no_of_data):\n",
    "            if h_k[j] == example_y[j]:\n",
    "                w[j] = w[j] * (error / (1-error))\n",
    "\n",
    "        w /= np.sum(w)\n",
    "        h.append(w_learn)\n",
    "        z.append(math.log((1-error)/error, 2))\n",
    "\n",
    "    return h, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = preprocess_telco_data()\n",
    "# final_column = data.columns[-1]\n",
    "# train_dataset, test_dataset = train_test_split(data, test_size=0.2, random_state=10)\n",
    "#\n",
    "# columns = info_gain(train_dataset)\n",
    "# feature_cutoff = 70\n",
    "# columns_to_use = columns[0:feature_cutoff]\n",
    "# columns_to_use.append(final_column)\n",
    "# reduced_training, reduced_test = train_dataset[columns_to_use], test_dataset[columns_to_use]\n",
    "#\n",
    "# reduced_training.insert(0, 'Ones', 1.0)\n",
    "# reduced_training = reduced_training.to_numpy()\n",
    "#\n",
    "# reduced_test.insert(0, 'Ones', 1.0)\n",
    "# reduced_test = reduced_test.to_numpy()\n",
    "#\n",
    "# x_train = reduced_training[:, :-1]\n",
    "# y_train = reduced_training[:, -1]\n",
    "# y_train = np.array([1.0 if it > 0 else -1.0 for it in y_train])\n",
    "# y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "#\n",
    "# x_test = reduced_test[:, :-1]\n",
    "# y_test = reduced_test[:, -1]\n",
    "# y_test = np.array([1.0 if it > 0 else -1.0 for it in y_test])\n",
    "# y_test = y_test.reshape((y_test.shape[0], 1))\n",
    "\n",
    "data.insert(0, 'Ones', 1.0)\n",
    "data = data.to_numpy()\n",
    "rows, columns = data.shape\n",
    "\n",
    "data_x = data[:, :-1]\n",
    "data_y = data[:, -1]\n",
    "\n",
    "data_y = np.array([1.0 if it > 0 else -1.0 for it in data_y])\n",
    "data_y = data_y.reshape((data_y.shape[0], 1))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [],
   "source": [
    "# training_set, test_set = preprocess_adult_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "outputs": [],
   "source": [
    "# training_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "outputs": [],
   "source": [
    "# test_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "# training_set.insert(0, 'Ones', 1.0)\n",
    "# training_set = training_set.to_numpy()\n",
    "#\n",
    "# test_set.insert(0, 'Ones', 1.0)\n",
    "# test_set = test_set.to_numpy()\n",
    "# # rows, columns = training_set.shape\n",
    "#\n",
    "# x_train = training_set[:, :-1]\n",
    "# y_train = training_set[:, -1]\n",
    "#\n",
    "# x_test = test_set[:, :-1]\n",
    "# y_test = test_set[:, -1]\n",
    "#\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "#\n",
    "# y_train = np.array([1.0 if it > 0 else -1.0 for it in y_train])\n",
    "# y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "#\n",
    "# y_test = np.array([1.0 if it > 0 else -1.0 for it in y_test])\n",
    "# y_test = y_test.reshape((y_test.shape[0], 1))\n",
    "#\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, test_size=0.2, random_state=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy 0.6763662171753017.\n"
     ]
    }
   ],
   "source": [
    "# logistic regression test\n",
    "w_logi = train(x_train, y_train, early_terminate_threshold=0.5)\n",
    "h_logi = np.tanh(np.dot(x_test, w_logi))\n",
    "h_logi = predict(h_logi)\n",
    "print('Logistic regression accuracy {}.'.format(accuracy(h_logi, y_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost accuracy 0.7750177430801988.\n"
     ]
    }
   ],
   "source": [
    "# adaboost test\n",
    "h_ada, z_ada = adaboost(x_train, y_train, 5)\n",
    "\n",
    "hypo = np.zeros(y_test.shape)\n",
    "for _h, _z in zip(h_ada, z_ada):\n",
    "    l = np.tanh(np.dot(x_test, _h))\n",
    "    # h_out = predict(l)\n",
    "    hypo += _z * l\n",
    "\n",
    "hypo /= sum(z_ada)\n",
    "\n",
    "h_out = predict(hypo)\n",
    "print('Adaboost accuracy {}.'.format(accuracy(h_out, y_test)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}